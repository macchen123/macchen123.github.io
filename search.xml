<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>机器学习笔记11</title>
    <url>/2020/08/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011/</url>
    <content><![CDATA[<h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><a id="more"></a>
<h4 id="一、什么是多项式回归"><a href="#一、什么是多项式回归" class="headerlink" title="一、什么是多项式回归"></a>一、什么是多项式回归</h4><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200815221650806.png" alt="image-20200815221650806"></p>
<h5 id="以上图来说明-该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的-ax-2-，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归"><a href="#以上图来说明-该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的-ax-2-，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归" class="headerlink" title="以上图来说明:该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的$ax^2$，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归"></a>以上图来说明:该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的$ax^2$，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归</h5><h5 id="本质上相当于对原数据集进行升维来更好的拟合高维数据"><a href="#本质上相当于对原数据集进行升维来更好的拟合高维数据" class="headerlink" title="本质上相当于对原数据集进行升维来更好的拟合高维数据"></a>本质上相当于对原数据集进行升维来更好的拟合高维数据</h5></li>
</ul>
<h4 id="二，关于sklearn中的PolynomialFeatures"><a href="#二，关于sklearn中的PolynomialFeatures" class="headerlink" title="二，关于sklearn中的PolynomialFeatures"></a>二，关于sklearn中的PolynomialFeatures</h4><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816004625821.png" alt="image-20200816004625821"></p>
<h5 id="设degree-i，则它会生成所有小于等于i的多项式特征，构成新的特征集"><a href="#设degree-i，则它会生成所有小于等于i的多项式特征，构成新的特征集" class="headerlink" title="设degree=i，则它会生成所有小于等于i的多项式特征，构成新的特征集"></a>设degree=i，则它会生成所有小于等于i的多项式特征，构成新的特征集</h5></li>
</ul>
<h4 id="三、Pipeline"><a href="#三、Pipeline" class="headerlink" title="三、Pipeline"></a>三、Pipeline</h4><ul>
<li><h5 id="拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步"><a href="#拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步" class="headerlink" title="拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步"></a>拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步</h5></li>
<li><h5 id="具体看jupyter中的代码"><a href="#具体看jupyter中的代码" class="headerlink" title="具体看jupyter中的代码"></a>具体看jupyter中的代码</h5></li>
</ul>
<h4 id="四、过拟合和欠拟合，以及分离数据集"><a href="#四、过拟合和欠拟合，以及分离数据集" class="headerlink" title="四、过拟合和欠拟合，以及分离数据集"></a>四、过拟合和欠拟合，以及分离数据集</h4><h4 id="1-泛化能力"><a href="#1-泛化能力" class="headerlink" title="1.泛化能力"></a>1.泛化能力</h4><ul>
<li><h5 id="泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱"><a href="#泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱" class="headerlink" title="泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱"></a>泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱</h5></li>
</ul>
<h4 id="2-过拟合"><a href="#2-过拟合" class="headerlink" title="2.过拟合"></a>2.过拟合</h4><ul>
<li><h5 id="就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测-区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。"><a href="#就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测-区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。" class="headerlink" title="就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。"></a>就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。</h5></li>
</ul>
<h4 id="3-欠拟合"><a href="#3-欠拟合" class="headerlink" title="3.欠拟合"></a>3.欠拟合</h4><ul>
<li><h5 id="测试样本的特性没有学到-模型不能完整的表述数据关系-，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差"><a href="#测试样本的特性没有学到-模型不能完整的表述数据关系-，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差" class="headerlink" title="测试样本的特性没有学到(模型不能完整的表述数据关系)，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差"></a>测试样本的特性没有学到(模型不能完整的表述数据关系)，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差</h5></li>
</ul>
<h4 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h4><ul>
<li><h5 id="总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行"><a href="#总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行" class="headerlink" title="总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行"></a>总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行</h5></li>
<li><p><strong>那么如何衡量模型的泛化能力强弱呢？那就是采用分离数据集的方法来衡量。这就是分离数据集更深层的意义</strong></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816210229364.png" alt="image-20200816210229364"></p>
</li>
</ul>
<h4 id="五、学习曲线（可视化查看模型的欠拟合、过拟合）"><a href="#五、学习曲线（可视化查看模型的欠拟合、过拟合）" class="headerlink" title="五、学习曲线（可视化查看模型的欠拟合、过拟合）"></a>五、学习曲线（可视化查看模型的欠拟合、过拟合）</h4><ul>
<li><h5 id="定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力"><a href="#定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力" class="headerlink" title="定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力"></a>定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力</h5></li>
<li><h5 id="学习曲线的解释："><a href="#学习曲线的解释：" class="headerlink" title="学习曲线的解释："></a>学习曲线的解释：</h5><ul>
<li><h5 id="随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；"><a href="#随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；" class="headerlink" title="随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；"></a>随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；</h5></li>
<li><h5 id="随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失"><a href="#随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失" class="headerlink" title="随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失"></a>随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失</h5></li>
</ul>
</li>
<li><h5 id="欠拟合，最佳拟合，过拟合的对比"><a href="#欠拟合，最佳拟合，过拟合的对比" class="headerlink" title="欠拟合，最佳拟合，过拟合的对比"></a>欠拟合，最佳拟合，过拟合的对比</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816214627319.png" alt="image-20200816214627319"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816214657728.png" alt="image-20200816214657728"></p>
</li>
</ul>
<h4 id="六、验证数据集与交叉验证"><a href="#六、验证数据集与交叉验证" class="headerlink" title="六、验证数据集与交叉验证"></a>六、验证数据集与交叉验证</h4><h5 id="1、验证数据集："><a href="#1、验证数据集：" class="headerlink" title="1、验证数据集："></a>1、验证数据集：</h5><ul>
<li><h5 id="由于train-test-split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集"><a href="#由于train-test-split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集" class="headerlink" title="由于train_test_split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集"></a>由于train_test_split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823165520156.png" alt="image-20200823165520156"></p>
<h5 id="由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集"><a href="#由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集" class="headerlink" title="由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集"></a>由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集</h5></li>
</ul>
</li>
<li><h5 id="但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。"><a href="#但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。" class="headerlink" title="但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。"></a>但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。</h5></li>
</ul>
<h5 id="2、交叉验证："><a href="#2、交叉验证：" class="headerlink" title="2、交叉验证："></a>2、交叉验证：</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823170347284.png" alt="image-20200823170347284"></p>
<ul>
<li><h5 id="把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。"><a href="#把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。" class="headerlink" title="把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。"></a>把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。</h5></li>
<li><h5 id="使用交叉验证进行调参优化得到的score可能会比train-test-split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据"><a href="#使用交叉验证进行调参优化得到的score可能会比train-test-split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据" class="headerlink" title="使用交叉验证进行调参优化得到的score可能会比train_test_split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据"></a>使用交叉验证进行调参优化得到的score可能会比train_test_split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据</h5></li>
</ul>
</li>
<li><h5 id="交叉验证总结："><a href="#交叉验证总结：" class="headerlink" title="交叉验证总结："></a>交叉验证总结：</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823182409604.png" alt="image-20200823182409604"></p>
<h5 id="但是这样训练出的模型是可信赖的"><a href="#但是这样训练出的模型是可信赖的" class="headerlink" title="但是这样训练出的模型是可信赖的"></a>但是这样训练出的模型是可信赖的</h5></li>
<li><h5 id="假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据"><a href="#假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据" class="headerlink" title="假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据"></a>假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823182632636.png" alt="image-20200823182632636"></p>
</li>
</ul>
</li>
</ul>
<h4 id="七、偏方差权衡（Bias-Variance-Trade-off）"><a href="#七、偏方差权衡（Bias-Variance-Trade-off）" class="headerlink" title="七、偏方差权衡（Bias Variance Trade off）"></a>七、偏方差权衡（Bias Variance Trade off）</h4><h5 id="1、偏差与方差："><a href="#1、偏差与方差：" class="headerlink" title="1、偏差与方差："></a>1、偏差与方差：</h5><h5 id="可以参考："><a href="#可以参考：" class="headerlink" title="可以参考："></a>可以参考：</h5><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">作者：「已注销」</span><br><span class="line">链接：https://www.zhihu.com/question/20448464/answer/765401873</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure>
<ul>
<li><h5 id="偏差："><a href="#偏差：" class="headerlink" title="偏差："></a>偏差：</h5><ul>
<li><h5 id="描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。"><a href="#描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。" class="headerlink" title="描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。"></a>描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。</h5></li>
</ul>
</li>
<li><h5 id="方差："><a href="#方差：" class="headerlink" title="方差："></a>方差：</h5><ul>
<li><h5 id="描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。"><a href="#描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。" class="headerlink" title="描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。"></a>描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。</h5></li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823184540816.png" alt="image-20200823184540816"></p>
<h5 id="2、模型误差："><a href="#2、模型误差：" class="headerlink" title="2、模型误差："></a>2、模型误差：</h5><ul>
<li><h5 id="模型误差-偏差（Bias）-方差（Variance）-不可避免的误差（如噪声）"><a href="#模型误差-偏差（Bias）-方差（Variance）-不可避免的误差（如噪声）" class="headerlink" title="模型误差 = 偏差（Bias）+ 方差（Variance）+ 不可避免的误差（如噪声）"></a>模型误差 = 偏差（Bias）+ 方差（Variance）+ 不可避免的误差（如噪声）</h5></li>
</ul>
<h5 id="3、产生偏差和方差的主要原因"><a href="#3、产生偏差和方差的主要原因" class="headerlink" title="3、产生偏差和方差的主要原因"></a>3、产生偏差和方差的主要原因</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823185218727.png" alt="image-20200823185218727"></p>
</li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823185258191.png" alt="image-20200823185258191"></p>
</li>
</ul>
<h5 id="4、模型与偏差和方差："><a href="#4、模型与偏差和方差：" class="headerlink" title="4、模型与偏差和方差："></a>4、模型与偏差和方差：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823221542723.png" alt="image-20200823221542723"></p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">一、为什么knn的k越小，模型越复杂（复杂指的就是过拟合）？：</span><br><span class="line">假设有一个男女性别的数据集，k=4,。在训练数据集中的，有一个新样本s（s是男），通过knn得出 男：女=3:1，意思是有75%的概率认为s是男，25%的概率认为s是女。但是为了追求更高的准确度（极端一点），让k=1，这样s的属性就会由一个离它最近的样本决定（假设该训练数据集离s最近的一个样本是男），这样我们就有100%的概率认为s为男。然而在测试数据集中，若有一个新样本j（j是男），但是离j最最近的样本是女，所以这个模型预测就出现了错误。这就是为了追求测试数据集的准确度而不具有泛化性（普遍性）导致了过拟合</span><br><span class="line"></span><br><span class="line">二、为什么knn是高方差算法？：</span><br><span class="line">可以参考：https://www.cnblogs.com/solong1989/p/9603818.html</span><br><span class="line"></span><br><span class="line">三、过拟合和方差：</span><br><span class="line">https://blog.csdn.net/liweibin1994/article/details/76859743</span><br><span class="line">假设对训练数据集过拟合，那么测试数据集（或验证数据集）上的样本点有的预测准确，有的预测不准确，这样造成预测结果之前离散程度很大，所以过拟合就是高方差</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823191041828.png" alt="image-20200823191041828"></p>
<h5 id="5、偏差与方差的关系："><a href="#5、偏差与方差的关系：" class="headerlink" title="5、偏差与方差的关系："></a>5、偏差与方差的关系：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823191117875.png" alt="image-20200823191117875"></p>
<h5 id="6、机器学习-算法层面-的主要挑战"><a href="#6、机器学习-算法层面-的主要挑战" class="headerlink" title="6、机器学习(算法层面)的主要挑战"></a>6、机器学习(算法层面)的主要挑战</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823232503791.png" alt="image-20200823232503791"></p>
<h4 id="八、模型正则化（Regularization）"><a href="#八、模型正则化（Regularization）" class="headerlink" title="八、模型正则化（Regularization）"></a>八、模型正则化（Regularization）</h4><ul>
<li><h5 id="模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。"><a href="#模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。" class="headerlink" title="模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。"></a>模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826102708414.png" alt="image-20200826102708414"></p>
<h5 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h5><ul>
<li><h5 id="要想使损失函数尽可能小，就必须让-theta-尽可能小"><a href="#要想使损失函数尽可能小，就必须让-theta-尽可能小" class="headerlink" title="要想使损失函数尽可能小，就必须让$\theta$尽可能小"></a>要想使损失函数尽可能小，就必须让$\theta$尽可能小</h5></li>
<li><h5 id="损失函数后面的正则化项求和是从1到n的，这意味着不需要将-theta-0-加入，因为-theta-0-是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）"><a href="#损失函数后面的正则化项求和是从1到n的，这意味着不需要将-theta-0-加入，因为-theta-0-是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）" class="headerlink" title="损失函数后面的正则化项求和是从1到n的，这意味着不需要将$\theta_0$加入，因为$\theta_0$是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）"></a>损失函数后面的正则化项求和是从1到n的，这意味着不需要将$\theta_0$加入，因为$\theta_0$是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）</h5></li>
<li><h5 id="第二项的1-2是为了求导好约分，加不加都行"><a href="#第二项的1-2是为了求导好约分，加不加都行" class="headerlink" title="第二项的1/2是为了求导好约分，加不加都行"></a>第二项的1/2是为了求导好约分，加不加都行</h5></li>
<li><h5 id="alpha-是一个超参数，它指的是-theta-的优化程度占整个损失函数的多少，很显然，-alpha-越大越好，但是在实际中我们要找一个能平衡经验误差项-MSE-和正则化项的-alpha-。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。"><a href="#alpha-是一个超参数，它指的是-theta-的优化程度占整个损失函数的多少，很显然，-alpha-越大越好，但是在实际中我们要找一个能平衡经验误差项-MSE-和正则化项的-alpha-。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。" class="headerlink" title="$\alpha$是一个超参数，它指的是$\theta$的优化程度占整个损失函数的多少，很显然，$\alpha$越大越好，但是在实际中我们要找一个能平衡经验误差项(MSE)和正则化项的$\alpha$。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。"></a>$\alpha$是一个超参数，它指的是$\theta$的优化程度占整个损失函数的多少，很显然，$\alpha$越大越好，但是在实际中我们要找一个能平衡经验误差项(MSE)和正则化项的$\alpha$。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。</h5></li>
</ul>
</li>
</ul>
<h4 id="九、岭回归（Ridge-Regression）"><a href="#九、岭回归（Ridge-Regression）" class="headerlink" title="九、岭回归（Ridge Regression）"></a>九、岭回归（Ridge Regression）</h4><ul>
<li><h5 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826211643626.png" alt="image-20200826211643626"></p>
</li>
</ul>
<h4 id="十、LASSO回归（LASSO-Regression）"><a href="#十、LASSO回归（LASSO-Regression）" class="headerlink" title="十、LASSO回归（LASSO Regression）"></a>十、LASSO回归（LASSO Regression）</h4><ul>
<li><h5 id="比较Ridge和LASSO：为什么随着-alpha-的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线"><a href="#比较Ridge和LASSO：为什么随着-alpha-的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线" class="headerlink" title="比较Ridge和LASSO：为什么随着$\alpha$的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线"></a>比较Ridge和LASSO：为什么随着$\alpha$的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826212135574.png" alt="image-20200826212135574"></p>
<h5 id="下面我们从梯度下降的角度来解释："><a href="#下面我们从梯度下降的角度来解释：" class="headerlink" title="下面我们从梯度下降的角度来解释："></a>下面我们从梯度下降的角度来解释：</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826213339759.png" alt="image-20200826213339759"></p>
</li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826213439382.png" alt="image-20200826213439382"></p>
<h5 id="因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些-theta-为零。这也说明了为什么叫岭回归，因为它就像下山一样。"><a href="#因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些-theta-为零。这也说明了为什么叫岭回归，因为它就像下山一样。" class="headerlink" title="因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些$\theta$为零。这也说明了为什么叫岭回归，因为它就像下山一样。"></a>因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些$\theta$为零。这也说明了为什么叫岭回归，因为它就像下山一样。</h5></li>
</ul>
</li>
<li><h5 id="LASSO回归总结"><a href="#LASSO回归总结" class="headerlink" title="LASSO回归总结"></a>LASSO回归总结</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826214412564.png" alt="image-20200826214412564"></p>
<h5 id="但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。"><a href="#但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。" class="headerlink" title="但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。"></a>但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。</h5></li>
</ul>
<h4 id="十一、L1，L2正则"><a href="#十一、L1，L2正则" class="headerlink" title="十一、L1，L2正则"></a>十一、L1，L2正则</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826214730165.png" alt="image-20200826214730165"></p>
<ul>
<li><h5 id="其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。"><a href="#其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。" class="headerlink" title="其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。"></a>其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。</h5></li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215536491.png" alt="image-20200826215536491"></p>
<ul>
<li><h5 id="基于上面的思想，我们把明可夫斯基距离进行一下如下变换："><a href="#基于上面的思想，我们把明可夫斯基距离进行一下如下变换：" class="headerlink" title="基于上面的思想，我们把明可夫斯基距离进行一下如下变换："></a>基于上面的思想，我们把明可夫斯基距离进行一下如下变换：</h5></li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215703508.png" alt="image-20200826215703508"></p>
<ul>
<li><h5 id="关于图右边开不开根号都不会影响到结果"><a href="#关于图右边开不开根号都不会影响到结果" class="headerlink" title="关于图右边开不开根号都不会影响到结果"></a>关于图右边开不开根号都不会影响到结果</h5></li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215846131.png" alt="image-20200826215846131"></p>
<ul>
<li><h5 id="L0正则项的原理是使-theta-的非零项的数量尽量少，但是必须使用穷举法来得出-theta-为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。"><a href="#L0正则项的原理是使-theta-的非零项的数量尽量少，但是必须使用穷举法来得出-theta-为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。" class="headerlink" title="L0正则项的原理是使$\theta$的非零项的数量尽量少，但是必须使用穷举法来得出$\theta$为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。"></a>L0正则项的原理是使$\theta$的非零项的数量尽量少，但是必须使用穷举法来得出$\theta$为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。</h5></li>
</ul>
<h4 id="十二、弹性网"><a href="#十二、弹性网" class="headerlink" title="十二、弹性网"></a>十二、弹性网</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826220552522.png" alt="image-20200826220552522"></p>
<ul>
<li><h5 id="机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。"><a href="#机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。" class="headerlink" title="机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。"></a>机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。</h5></li>
<li><h5 id="弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。"><a href="#弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。" class="headerlink" title="弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。"></a>弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。</h5></li>
<li><h5 id="通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。"><a href="#通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。" class="headerlink" title="通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。"></a>通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。</h5></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>多项式回归</tag>
        <tag>机器学习</tag>
        <tag>Pipeline</tag>
        <tag>过拟合与欠拟合</tag>
        <tag>学习曲线</tag>
        <tag>交叉验证</tag>
        <tag>偏差与方差</tag>
        <tag>模型正则化</tag>
        <tag>岭回归、LASSO回归</tag>
        <tag>弹性网</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记12</title>
    <url>/2020/08/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="5873cd23e104876b9daca4c2a0a94bc2680cb0ef3ba3de67826ab4b0a27c7fcb">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>OvR（或OvA）与OvO(或MvM)</tag>
        <tag>Sigmoid函数</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记13</title>
    <url>/2020/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013/</url>
    <content><![CDATA[<h3 id="评价分类结果"><a href="#评价分类结果" class="headerlink" title="评价分类结果"></a>评价分类结果</h3><a id="more"></a>
]]></content>
  </entry>
  <entry>
    <title>机器学习笔记1</title>
    <url>/2020/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<h1 id="机器学习笔记"><a href="#机器学习笔记" class="headerlink" title="机器学习笔记"></a>机器学习笔记</h1><a id="more"></a>
<h3 id="一：什么是机器学习"><a href="#一：什么是机器学习" class="headerlink" title="一：什么是机器学习"></a>一：什么是机器学习</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706165617797.png" alt="image-20200706165617797"></p>
<h4 id="首先先了解人类如何学习（经验学习）"><a href="#首先先了解人类如何学习（经验学习）" class="headerlink" title="首先先了解人类如何学习（经验学习）"></a>首先先了解人类如何学习（经验学习）</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706170401290.png" alt="image-20200706170401290"></p>
<h4 id="机器学习和人类学习是非常相似的"><a href="#机器学习和人类学习是非常相似的" class="headerlink" title="机器学习和人类学习是非常相似的"></a>机器学习和人类学习是非常相似的</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706170712331.png" alt="image-20200706170712331"></p>
<h3 id="二：机器学习应用"><a href="#二：机器学习应用" class="headerlink" title="二：机器学习应用"></a>二：机器学习应用</h3><h4 id="1-当前应用"><a href="#1-当前应用" class="headerlink" title="1.当前应用"></a>1.当前应用</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706171220946.png" alt="image-20200706171220946"></p>
<h4 id="2-未来应用"><a href="#2-未来应用" class="headerlink" title="2.未来应用"></a>2.未来应用</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706171619513.png" alt="image-20200706171619513"></p>
<h3 id="三：机器学习的定位"><a href="#三：机器学习的定位" class="headerlink" title="三：机器学习的定位"></a>三：机器学习的定位</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706172204133.png" alt="image-20200706172204133"></p>
<h4 id="不要孤立的看待机器学习，它和很多领域都有密切的联系"><a href="#不要孤立的看待机器学习，它和很多领域都有密切的联系" class="headerlink" title="不要孤立的看待机器学习，它和很多领域都有密切的联系"></a>不要孤立的看待机器学习，它和很多领域都有密切的联系</h4><h3 id="四：机器学习的框架"><a href="#四：机器学习的框架" class="headerlink" title="四：机器学习的框架"></a>四：机器学习的框架</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706172446522.png" alt="image-20200706172446522"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706172809977.png" alt="image-20200706172809977"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
        <tag>机器学习基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记3</title>
    <url>/2020/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/</url>
    <content><![CDATA[<h3 id="KNN-K近邻算法（K-Nearest-Neighbors）"><a href="#KNN-K近邻算法（K-Nearest-Neighbors）" class="headerlink" title="KNN -K近邻算法（K-Nearest Neighbors）"></a>KNN -K近邻算法（K-Nearest Neighbors）</h3><a id="more"></a>
<h4 id="一：KNN—非常适合初学者入门的算法"><a href="#一：KNN—非常适合初学者入门的算法" class="headerlink" title="一：KNN—非常适合初学者入门的算法"></a>一：KNN—非常适合初学者入门的算法</h4><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707205845716.png" alt="image-20200707205845716"></li>
</ul>
<h5 id="1-什么是KNN"><a href="#1-什么是KNN" class="headerlink" title="1.什么是KNN"></a>1.什么是KNN</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707210428247.png" alt="image-20200707210428247"></p>
</li>
<li><ul>
<li>上图解释：红色是良性肿瘤，蓝色是恶性肿瘤，绿色是一个新的不确定的肿瘤。这里设K=3（后面会讲如何取K值），表示选取离绿色最近的三个肿瘤，然后这3个之间进行投票（以自己的标签投票），最终蓝色：红色==3:0.因此我们有很大的概率认为绿色也是恶性肿瘤。</li>
<li>因此，KNN表示在K个样本中，哪个样本越多，就表示目标样本有很大概率是同一类别。</li>
</ul>
</li>
</ul>
<h5 id="2-KNN的特性"><a href="#2-KNN的特性" class="headerlink" title="2.KNN的特性"></a>2.KNN的特性</h5><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707234150755.png" alt="image-20200707234150755"></li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707234227091.png" alt="image-20200707234227091"></li>
<li><ul>
<li>scikit_learn正是按这套流程进行的算法封装</li>
</ul>
</li>
</ul>
<h5 id="3-判断机器学习算法的性能"><a href="#3-判断机器学习算法的性能" class="headerlink" title="3.判断机器学习算法的性能"></a>3.判断机器学习算法的性能</h5><ul>
<li>考虑到真实环境下，将所有的原始数据都当成训练数据来训练模型是不恰当的，因此我们要进行训练和测试数据集切分(train test split)</li>
<li><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711013945748.png" alt="image-20200710224630969"></li>
</ul>
</li>
<li><p>注意：这里补充一下seed随机种子的直观理解：其实，设置seed（）里的数字就相当于设置了一个盛有随机数的“聚宝盆”，一个数字代表一个“聚宝盆”，当我们在seed（）的括号里设置相同的seed，“聚宝盆”就是一样的，那当然每次拿出的随机数就会相同（不要觉得就是从里面随机取数字，只要设置的seed相同取出地随机数就一样）。如果不设置seed，则每次会生成不同的随机数。（注：seed括号里的数值基本可以随便设置哦）<br>————————————————<br>版权声明：本文为CSDN博主「白糖炒栗子~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/weixin_41571493/article/details/80549833" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41571493/article/details/80549833</a></p>
<ul>
<li>np.random.seed(0)<br>np.random.rand(4,3)<br>Out[362]:<br>array([[0.5488135 , 0.71518937, 0.60276338],<pre><code>   [0.54488318, 0.4236548 , 0.64589411],
   [0.43758721, 0.891773  , 0.96366276],
   [0.38344152, 0.79172504, 0.52889492]])
</code></pre>np.random.seed(0)<br>np.random.rand(4,3)<br>Out[364]:<br>array([[0.5488135 , 0.71518937, 0.60276338],<pre><code>   [0.54488318, 0.4236548 , 0.64589411],
   [0.43758721, 0.891773  , 0.96366276],
   [0.38344152, 0.79172504, 0.52889492]])
</code></pre></li>
</ul>
</li>
<li><p>分类准确度：accuracy：详见jupyter</p>
</li>
<li><p>超参数</p>
<ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200710224630969.png" alt="image-20200711013945748"></p>
</li>
<li><p>如何寻找好的超参数</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711014228711.png" alt="image-20200711014228711"></p>
<ul>
<li><p>KNN中其实除了K还有一个超参数，那就是距离的权重（它是距离的倒数）</p>
<ul>
<li><p>设红和蓝与绿色的距离分别为1,3,4</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711021206076.png" alt="image-20200711021206076"></p>
<ul>
<li><p>若有三个类别，而k也等于3，那么就有可能出现平票的情况</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711021519742.png" alt="image-20200711021519742"></p>
</li>
<li><p>补充：明可夫斯基距离</p>
<ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711023118851.png" alt="image-20200711023118851"><ul>
<li>当p=1时是曼哈顿距离</li>
<li>当p=2时是欧拉距离</li>
<li>当p&gt;2时是其他距离</li>
</ul>
</li>
<li>更多的距离定义（暂不做深入研究，只是了解）<ul>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200723233646364.png" alt="image-20200723233646364"></p>
<ul>
<li><h5 id="我们可以使用sklearn的网格搜索来进行超参数搜索，具体看jupyter上的‘06-Validation-and-Cross-Validation’中的回顾网格搜索"><a href="#我们可以使用sklearn的网格搜索来进行超参数搜索，具体看jupyter上的‘06-Validation-and-Cross-Validation’中的回顾网格搜索" class="headerlink" title="我们可以使用sklearn的网格搜索来进行超参数搜索，具体看jupyter上的‘06-Validation-and-Cross-Validation’中的回顾网格搜索"></a>我们可以使用sklearn的网格搜索来进行超参数搜索，具体看jupyter上的‘06-Validation-and-Cross-Validation’中的回顾网格搜索</h5></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
        <tag>KNN</tag>
        <tag>超参数与网格搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记6</title>
    <url>/2020/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06/</url>
    <content><![CDATA[<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><a id="more"></a>
<h4 id="一、线性回归简述"><a href="#一、线性回归简述" class="headerlink" title="一、线性回归简述"></a>一、线性回归简述</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726225427820.png" alt="image-20200726225427820"></p>
<ul>
<li><h5 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726225808532.png" alt="image-20200726225808532"></p>
</li>
<li><h5 id="注意：分类算法的横纵坐标都是样本特征；而回归算法的横纵坐标分别为样本特征和样本标签"><a href="#注意：分类算法的横纵坐标都是样本特征；而回归算法的横纵坐标分别为样本特征和样本标签" class="headerlink" title="注意：分类算法的横纵坐标都是样本特征；而回归算法的横纵坐标分别为样本特征和样本标签"></a>注意：分类算法的横纵坐标都是样本特征；而回归算法的横纵坐标分别为样本特征和样本标签</h5></li>
<li><h5 id="样本特征只有一个称为简单线性回归"><a href="#样本特征只有一个称为简单线性回归" class="headerlink" title="样本特征只有一个称为简单线性回归"></a>样本特征只有一个称为简单线性回归</h5></li>
</ul>
</li>
</ul>
<h3 id="二、简单线性回归"><a href="#二、简单线性回归" class="headerlink" title="二、简单线性回归"></a>二、简单线性回归</h3><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726230627487.png" alt="image-20200726230627487"></p>
</li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726230701216.png" alt="image-20200726230701216"></p>
<h4 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h4><ul>
<li><h6 id="因为我们要考虑所有样本的差距，所以不能有正有负（可能会抵消为0），因此每个样本的差距值应该-gt-0。"><a href="#因为我们要考虑所有样本的差距，所以不能有正有负（可能会抵消为0），因此每个样本的差距值应该-gt-0。" class="headerlink" title="因为我们要考虑所有样本的差距，所以不能有正有负（可能会抵消为0），因此每个样本的差距值应该&gt;=0。"></a>因为我们要考虑所有样本的差距，所以不能有正有负（可能会抵消为0），因此每个样本的差距值应该&gt;=0。</h6></li>
<li><h6 id="由于我们希望真实值和预测值差距尽量小，所以需要求极值，而绝对值函数不是连续处处可导，所以我们改差值的平方函数。"><a href="#由于我们希望真实值和预测值差距尽量小，所以需要求极值，而绝对值函数不是连续处处可导，所以我们改差值的平方函数。" class="headerlink" title="由于我们希望真实值和预测值差距尽量小，所以需要求极值，而绝对值函数不是连续处处可导，所以我们改差值的平方函数。"></a>由于我们希望真实值和预测值差距尽量小，所以需要求极值，而绝对值函数不是连续处处可导，所以我们改差值的平方函数。</h6></li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726231427640.png" alt="image-20200726231427640"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726231641441.png" alt="image-20200726231641441"></p>
<h4 id="说明：-1"><a href="#说明：-1" class="headerlink" title="说明："></a>说明：</h4><ul>
<li>损失函数：度量模型没有拟合到的样本的程度</li>
<li><p>效用函数：度量模型拟合到的样本的程度</p>
</li>
<li><p>近乎所有的参数学习算法都是这样来求解最优参数（线性回归，多项式回归，逻辑回归，SVM，神经网络等等）</p>
</li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726232703309.png" alt="image-20200726232703309"></p>
<h4 id="2-最小二乘法："><a href="#2-最小二乘法：" class="headerlink" title="2.最小二乘法："></a>2.最小二乘法：</h4><h5 id="推导过程（求解a和b）"><a href="#推导过程（求解a和b）" class="headerlink" title="推导过程（求解a和b）"></a>推导过程（求解a和b）</h5><p>首先求解参数b</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726234248085.png" alt="image-20200726234248085"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726234339896.png" alt="image-20200726234339896"></p>
<p>然后求解参数a</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200727000111783.png" alt="image-20200727000111783"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200727000132271.png" alt="image-20200727000132271"></p>
<h3 id="说明：其实到这里已经将参数a求了出来，但是为了编程实现方便，我们要进一步简化这个式子，请看下图，注意-bar-x和-bar-y-都是常数，所以可以提到求和符号前面"><a href="#说明：其实到这里已经将参数a求了出来，但是为了编程实现方便，我们要进一步简化这个式子，请看下图，注意-bar-x和-bar-y-都是常数，所以可以提到求和符号前面" class="headerlink" title="说明：其实到这里已经将参数a求了出来，但是为了编程实现方便，我们要进一步简化这个式子，请看下图，注意$ \bar x和\bar y $都是常数，所以可以提到求和符号前面"></a>说明：其实到这里已经将参数a求了出来，但是为了编程实现方便，我们要进一步简化这个式子，请看下图，注意$ \bar x和\bar y $都是常数，所以可以提到求和符号前面</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200727001049264.png" alt="image-20200727001049264"></p>
<h3 id="三，向量化运算"><a href="#三，向量化运算" class="headerlink" title="三，向量化运算"></a>三，向量化运算</h3><h4 id="在代码编写过程中使用for循环会使运行效率大大降低，所以我们要把循环去掉。"><a href="#在代码编写过程中使用for循环会使运行效率大大降低，所以我们要把循环去掉。" class="headerlink" title="在代码编写过程中使用for循环会使运行效率大大降低，所以我们要把循环去掉。"></a>在代码编写过程中使用for循环会使运行效率大大降低，所以我们要把循环去掉。</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200729233852136.png" alt="image-20200729233852136"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200729233924794.png" alt="image-20200729233924794"></p>
<h4 id="所以分子为-vec-vec-x-bar-x-cdot-vec-vec-y-bar-y"><a href="#所以分子为-vec-vec-x-bar-x-cdot-vec-vec-y-bar-y" class="headerlink" title="所以分子为(\vec{\vec{x}-\bar x})\cdot(\vec{\vec{y}-\bar y} )"></a>所以分子为<script type="math/tex">(\vec{\vec{x}-\bar x})\cdot(\vec{\vec{y}-\bar y} )</script></h4><h4 id="分母为-vec-vec-x-bar-x-cdot-vec-vec-x-bar-x"><a href="#分母为-vec-vec-x-bar-x-cdot-vec-vec-x-bar-x" class="headerlink" title="分母为(\vec{\vec{x}-\bar x})\cdot(\vec{\vec{x}-\bar x})"></a>分母为<script type="math/tex">(\vec{\vec{x}-\bar x})\cdot(\vec{\vec{x}-\bar x})</script></h4><h4 id="注意，这种思想非常重要，请以后常用"><a href="#注意，这种思想非常重要，请以后常用" class="headerlink" title="注意，这种思想非常重要，请以后常用"></a>注意，这种思想非常重要，请以后常用</h4>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
        <tag>简单线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记4</title>
    <url>/2020/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/</url>
    <content><![CDATA[<h3 id="数据归一化与标准化"><a href="#数据归一化与标准化" class="headerlink" title="数据归一化与标准化"></a>数据归一化与标准化</h3><a id="more"></a>
<h4 id="一，为什么要进行数据归一化或归一化"><a href="#一，为什么要进行数据归一化或归一化" class="headerlink" title="一，为什么要进行数据归一化或归一化"></a>一，为什么要进行数据归一化或归一化</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200723234524737.png" alt="image-20200723234524737"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200723234716174.png" alt="image-20200723234716174"></p>
<ul>
<li><h4 id="这张图表示样本之间的距离又会被肿瘤大小来主导"><a href="#这张图表示样本之间的距离又会被肿瘤大小来主导" class="headerlink" title="这张图表示样本之间的距离又会被肿瘤大小来主导"></a>这张图表示样本之间的距离又会被肿瘤大小来主导</h4></li>
<li><h4 id="因此我们需要统一进行数据归一化"><a href="#因此我们需要统一进行数据归一化" class="headerlink" title="因此我们需要统一进行数据归一化"></a>因此我们需要统一进行数据归一化</h4></li>
</ul>
<h3 id="二，什么是数据归一化或标准化"><a href="#二，什么是数据归一化或标准化" class="headerlink" title="二，什么是数据归一化或标准化"></a>二，什么是数据归一化或标准化</h3><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200723235207881.png" alt="image-20200723235207881"></p>
</li>
<li><h4 id="最值归一化（normalization）适用于分布有明显边界的情况。比如像工资就不能用最值归一化（工资无上限）"><a href="#最值归一化（normalization）适用于分布有明显边界的情况。比如像工资就不能用最值归一化（工资无上限）" class="headerlink" title="最值归一化（normalization）适用于分布有明显边界的情况。比如像工资就不能用最值归一化（工资无上限）"></a>最值归一化（normalization）适用于分布有明显边界的情况。比如像工资就不能用最值归一化（工资无上限）</h4></li>
</ul>
<h4 id="第二种是标准化"><a href="#第二种是标准化" class="headerlink" title="第二种是标准化"></a>第二种是标准化</h4><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724000028651.png" alt="image-20200724000028651"></p>
</li>
<li><h5 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h5><ul>
<li><h4 id="此处老师讲错，应该为标准化，而不是均值方差归一化"><a href="#此处老师讲错，应该为标准化，而不是均值方差归一化" class="headerlink" title="此处老师讲错，应该为标准化，而不是均值方差归一化"></a>此处老师讲错，应该为标准化，而不是均值方差归一化</h4></li>
<li><h4 id="s表示标准差。除非数据集有明确的边界，（初学情况）一般情况下都使用标准化"><a href="#s表示标准差。除非数据集有明确的边界，（初学情况）一般情况下都使用标准化" class="headerlink" title="s表示标准差。除非数据集有明确的边界，（初学情况）一般情况下都使用标准化"></a>s表示标准差。除非数据集有明确的边界，（初学情况）一般情况下都使用标准化</h4></li>
<li><h4 id="其实就是利用标准化处理将正态分布转为了标准正态分布"><a href="#其实就是利用标准化处理将正态分布转为了标准正态分布" class="headerlink" title="其实就是利用标准化处理将正态分布转为了标准正态分布"></a>其实就是利用标准化处理将正态分布转为了标准正态分布</h4></li>
<li><h4 id="深入了解请看https-www-jiqizhixin-com-articles-19070701-https-www-zhihu-com-question-56891433"><a href="#深入了解请看https-www-jiqizhixin-com-articles-19070701-https-www-zhihu-com-question-56891433" class="headerlink" title="深入了解请看https://www.jiqizhixin.com/articles/19070701 https://www.zhihu.com/question/56891433"></a>深入了解请看<a href="https://www.jiqizhixin.com/articles/19070701" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/19070701</a> <a href="https://www.zhihu.com/question/56891433" target="_blank" rel="noopener">https://www.zhihu.com/question/56891433</a></h4></li>
<li><h4 id="在线演示标准正态分布https-www-shuxuele-com-data-standard-normal-distribution-table-html"><a href="#在线演示标准正态分布https-www-shuxuele-com-data-standard-normal-distribution-table-html" class="headerlink" title="在线演示标准正态分布https://www.shuxuele.com/data/standard-normal-distribution-table.html"></a>在线演示标准正态分布<a href="https://www.shuxuele.com/data/standard-normal-distribution-table.html" target="_blank" rel="noopener">https://www.shuxuele.com/data/standard-normal-distribution-table.html</a></h4></li>
</ul>
</li>
</ul>
<h3 id="三，这两种的区别"><a href="#三，这两种的区别" class="headerlink" title="三，这两种的区别"></a>三，这两种的区别</h3><h5 id="最值归一化"><a href="#最值归一化" class="headerlink" title="最值归一化"></a>最值归一化</h5><ul>
<li>数据集必须得有边界，否则会出现偏差很悬殊的值</li>
</ul>
<h5 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724020154141.png" alt="image-20200724020154141"></p>
</li>
<li><h5 id="平均值是曲线的中心。这是曲线的最高点，因为大多数点都在平均值附近；"><a href="#平均值是曲线的中心。这是曲线的最高点，因为大多数点都在平均值附近；" class="headerlink" title="平均值是曲线的中心。这是曲线的最高点，因为大多数点都在平均值附近；"></a>平均值是曲线的中心。这是曲线的最高点，因为大多数点都在平均值附近；</h5></li>
<li><h5 id="标准化更符合统计学假设（自然界大多数符合正态分布）：对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。"><a href="#标准化更符合统计学假设（自然界大多数符合正态分布）：对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。" class="headerlink" title="标准化更符合统计学假设（自然界大多数符合正态分布）：对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。"></a>标准化更符合统计学假设（自然界大多数符合正态分布）：对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。</h5></li>
<li><h5 id="新的数据由于对方差进行了归一化，这时候每个维度的量纲其实已经等价了，每个维度都服从均值为0、方差1的正态分布，在计算距离的时候，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。"><a href="#新的数据由于对方差进行了归一化，这时候每个维度的量纲其实已经等价了，每个维度都服从均值为0、方差1的正态分布，在计算距离的时候，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。" class="headerlink" title="新的数据由于对方差进行了归一化，这时候每个维度的量纲其实已经等价了，每个维度都服从均值为0、方差1的正态分布，在计算距离的时候，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。"></a>新的数据由于对方差进行了归一化，这时候每个维度的量纲其实已经等价了，每个维度都服从均值为0、方差1的正态分布，在计算距离的时候，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。</h5></li>
</ul>
<h3 id="四，对测试数据集如何归一化或标准化"><a href="#四，对测试数据集如何归一化或标准化" class="headerlink" title="四，对测试数据集如何归一化或标准化"></a>四，对测试数据集如何归一化或标准化</h3><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724022251989.png" alt="image-20200724022251989"></p>
</li>
<li><h4 id="结论：因为一个模型是要投入生产生活中的，所以我们要让测试数据集也要使用训练数据集的均值和标准差"><a href="#结论：因为一个模型是要投入生产生活中的，所以我们要让测试数据集也要使用训练数据集的均值和标准差" class="headerlink" title="结论：因为一个模型是要投入生产生活中的，所以我们要让测试数据集也要使用训练数据集的均值和标准差"></a>结论：因为一个模型是要投入生产生活中的，所以我们要让测试数据集也要使用训练数据集的均值和标准差</h4></li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724022712834.png" alt="image-20200724022712834"></p>
</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
        <tag>数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记2</title>
    <url>/2020/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/</url>
    <content><![CDATA[<h3 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h3><h4 id="一：机器学习的数据"><a href="#一：机器学习的数据" class="headerlink" title="一：机器学习的数据"></a>一：机器学习的数据</h4><a id="more"></a>
<h5 id="1-以鸢尾花数据来举例"><a href="#1-以鸢尾花数据来举例" class="headerlink" title="1.以鸢尾花数据来举例"></a>1.以鸢尾花数据来举例</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706180226511.png" alt="image-20200706180226511"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706180341873_1.png" alt="image-20200706180341873_1"></p>
<ul>
<li>注意：种类是用0,1,2来表示的。</li>
</ul>
<h5 id="2-从该数据集中引出的基本概念"><a href="#2-从该数据集中引出的基本概念" class="headerlink" title="2.从该数据集中引出的基本概念"></a>2.从该数据集中引出的基本概念</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706181305828.png" alt="image-20200706181305828"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706221107817_1.png" alt="image-20200706221107817_1"></p>
<p>注意:</p>
<ul>
<li>在机器学习中，大写字母表示矩阵，小写字母表示向量</li>
<li>标记y是机器学习真正要学习（预测）的</li>
<li>一般来说，特征向量都是列向量，所以如果表示整个特征集，需要将每个特征向量转置为行向量（看上图理解）</li>
</ul>
<h5 id="3-为了可视化方便，取鸢尾花的前两个特征（长度，宽度）绘图"><a href="#3-为了可视化方便，取鸢尾花的前两个特征（长度，宽度）绘图" class="headerlink" title="3.为了可视化方便，取鸢尾花的前两个特征（长度，宽度）绘图"></a>3.为了可视化方便，取鸢尾花的前两个特征（长度，宽度）绘图</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706221859929.png" alt="image-20200706221859929"></p>
<p>解释：</p>
<ul>
<li>红蓝代表两种鸢尾花，一个点落入哪块儿区域就说明是哪种类型的鸢尾花。</li>
<li>玩具数据集的特征空间切分可能是直线，但是现实中的数据集一般不可能用直线正好切分</li>
<li>有时若在高维空间不好理解，可以类比为低维度空间理解，然后再迁移到高维空间 </li>
</ul>
<h5 id="4-特征不一定是具有语义的，比如下图（像素点指每一个小矩形）"><a href="#4-特征不一定是具有语义的，比如下图（像素点指每一个小矩形）" class="headerlink" title="4.特征不一定是具有语义的，比如下图（像素点指每一个小矩形）"></a>4.特征不一定是具有语义的，比如下图（像素点指每一个小矩形）</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706223051356.png" alt="image-20200706223051356"></p>
<h4 id="二：机器学习（监督学习）的基本任务"><a href="#二：机器学习（监督学习）的基本任务" class="headerlink" title="二：机器学习（监督学习）的基本任务"></a>二：机器学习（监督学习）的基本任务</h4><h5 id="1-机器学习（监督学习）的基本任务主要分为：分类和回归"><a href="#1-机器学习（监督学习）的基本任务主要分为：分类和回归" class="headerlink" title="1.机器学习（监督学习）的基本任务主要分为：分类和回归"></a>1.机器学习（监督学习）的基本任务主要分为：分类和回归</h5><h5 id="2-分类任务："><a href="#2-分类任务：" class="headerlink" title="2.分类任务："></a>2.分类任务：</h5><ul>
<li><p>二分类：如垃圾邮件判断</p>
</li>
<li><p>多分类：</p>
<ul>
<li>如数字识别，图像识别</li>
<li>其实很多复杂问题可以转换为多分类问题，比如下围棋，无人驾驶</li>
<li>一些算法只支持二分类，但可以转换为多分类</li>
<li>但是多分类可以转换为二分类</li>
<li>有一些算法天然支持多分类任务</li>
</ul>
</li>
<li><p>多标签分类：属于cv方向</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706224809022.png" alt="image-20200706224809022"></p>
</li>
</ul>
<h5 id="3-回归任务：结果是一个连续值，而不是一个类别"><a href="#3-回归任务：结果是一个连续值，而不是一个类别" class="headerlink" title="3.回归任务：结果是一个连续值，而不是一个类别"></a>3.回归任务：结果是一个连续值，而不是一个类别</h5><ul>
<li>如房屋价格，市场分析，学生成绩，股票价格</li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706225329441.png" alt="image-20200706225329441"></p>
<ul>
<li>一些情况下，回归任务可以转换为分类任务<ul>
<li>如要预测学生成绩，可以预测具体成绩（回归），也可以预测成绩评级（分类）</li>
</ul>
</li>
</ul>
<h4 id="三：机器学习的分类"><a href="#三：机器学习的分类" class="headerlink" title="三：机器学习的分类"></a>三：机器学习的分类</h4><ul>
<li><p>监督学习：给机器的训练数据有标签（y）</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706230605230.png" alt="image-20200706230605230"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706230631948.png" alt="image-20200706230631948"></p>
</li>
<li><p>非监督学习：给机器的训练数据没有任何标签</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706230857321.png" alt="image-20200706230857321"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706231214211.png" alt="image-20200706231214211"></p>
<ul>
<li>特征压缩指在尽量小的损失下，将高维度数据转换为低维度数据</li>
<li>降维处理的意义：方便可视化，因为人类无法理解四维以上的空间</li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706231654647.png" alt="image-20200706231654647"></p>
</li>
<li><p>半监督学习：一部分数据有标签，另一部分数据没有，更符合现实情况</p>
<ul>
<li>通常都是先使用无监督学习手段对数据做处理，之后用监督学习手段做模型的训练与预测</li>
</ul>
</li>
<li><p>强化学习</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706233156709_1.png" alt="image-20200706233156709_1"></p>
<ul>
<li>Agent（指算法）会根据环境（environment）来做出一些行为（action），然后Agent会收到一些反馈，有时是奖赏（reward），有时是惩罚。根据这些反馈Agent会改进自己的行为（action）模式，在一轮轮迭代中逐渐增强自己的智能</li>
</ul>
</li>
</ul>
<h4 id="四：机器学习的其他分类"><a href="#四：机器学习的其他分类" class="headerlink" title="四：机器学习的其他分类"></a>四：机器学习的其他分类</h4><ul>
<li><p>批量学习（离线学习）（Batch learning）</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706234715003_1.png" alt="image-20200706234715003_1"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706234926469.png" alt="image-20200706234926469"></p>
<ul>
<li>如股票预测，因为其每时每刻都在变化</li>
<li>而垃圾邮件分类就可以定时重新批量学习，因为变化频率低 </li>
</ul>
</li>
<li><p>在线学习（Online Learning）</p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707011631601.png" alt="image-20200707011631601"></p>
<ul>
<li>输入样例输入模型以后会输出预测值，同时输入样例里的真实值也会输出 ，最终真实值，预测值，它们之间的差异等构成新的学习资料传给算法进行算法修正。</li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707012338369.png" alt="image-20200707012338369"></li>
</ul>
</li>
<li><p>批量学习（离线学习）</p>
</li>
<li><p>参数学习（parametric learning）</p>
<ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707012614143.png" alt="image-20200707012614143"><ul>
<li>我们假设该模型的的函数是线性回归，然后想办法找到最优的a和b这两个参数</li>
<li>一旦学到了参数，就不再需要原有的数据集</li>
</ul>
</li>
</ul>
</li>
<li><p>非参数学习（nonparametric learning）</p>
<ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707013028462.png" alt="image-20200707013028462"></li>
<li>详细可参考<a href="https://www.cnblogs.com/wjunneng/p/9126906.html" target="_blank" rel="noopener">https://www.cnblogs.com/wjunneng/p/9126906.html</a></li>
</ul>
</li>
</ul>
<h4 id="五：和机器学习相关的哲学思考"><a href="#五：和机器学习相关的哲学思考" class="headerlink" title="五：和机器学习相关的哲学思考"></a>五：和机器学习相关的哲学思考</h4><h5 id="1-问题引出："><a href="#1-问题引出：" class="headerlink" title="1.问题引出："></a>1.问题引出：</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707013704925_1.png" alt="image-20200707013704925_1"></p>
</li>
<li><ul>
<li>纵坐标表示算法准确度，横坐标表示数据规模（数据量），可以看到随着数据量的增大，4种算法的准确度都处于上升状态，最终的差距越来越小</li>
</ul>
</li>
</ul>
<h5 id="2-争论点："><a href="#2-争论点：" class="headerlink" title="2.争论点："></a>2.争论点：</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707014212423.png" alt="image-20200707014212423"></p>
</li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707014340413.png" alt="image-20200707014340413"></p>
</li>
</ul>
<h5 id="3-总结："><a href="#3-总结：" class="headerlink" title="3.总结："></a>3.总结：</h5><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707014718816_1.png" alt="image-20200707014718816_1"></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
        <tag>机器学习基础概念</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记5</title>
    <url>/2020/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05/</url>
    <content><![CDATA[<h3 id="更多有关KNN的思考"><a href="#更多有关KNN的思考" class="headerlink" title="更多有关KNN的思考"></a>更多有关KNN的思考</h3><a id="more"></a>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724025657646.png" alt="image-20200724025657646"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724025727331.png" alt="image-20200724025727331"></p>
<h4 id="解释：可以把离绿球最近的三个蓝球的均值作为绿球的值，也可以将距离权重考虑进去求加权平均数。总之，这些都是回归问题。（sklearn其实已经封装好了KNN解决回归问题的库：KNeighborsRegressor）"><a href="#解释：可以把离绿球最近的三个蓝球的均值作为绿球的值，也可以将距离权重考虑进去求加权平均数。总之，这些都是回归问题。（sklearn其实已经封装好了KNN解决回归问题的库：KNeighborsRegressor）" class="headerlink" title="解释：可以把离绿球最近的三个蓝球的均值作为绿球的值，也可以将距离权重考虑进去求加权平均数。总之，这些都是回归问题。（sklearn其实已经封装好了KNN解决回归问题的库：KNeighborsRegressor）"></a>解释：可以把离绿球最近的三个蓝球的均值作为绿球的值，也可以将距离权重考虑进去求加权平均数。总之，这些都是回归问题。（sklearn其实已经封装好了KNN解决回归问题的库：KNeighborsRegressor）</h4><h3 id="它的缺点"><a href="#它的缺点" class="headerlink" title="它的缺点"></a>它的缺点</h3><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724030254962.png" alt="image-20200724030254962"></p>
</li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724030519829.png" alt="image-20200724030519829"></p>
<ul>
<li><h4 id="解释：缺点2：拿上图红蓝绿球来举例，假如k-3，而其中两个是红色，一个是蓝色。按照KNN就会认定绿球为红色类，但事实上绿球的标签是蓝色类"><a href="#解释：缺点2：拿上图红蓝绿球来举例，假如k-3，而其中两个是红色，一个是蓝色。按照KNN就会认定绿球为红色类，但事实上绿球的标签是蓝色类" class="headerlink" title="解释：缺点2：拿上图红蓝绿球来举例，假如k=3，而其中两个是红色，一个是蓝色。按照KNN就会认定绿球为红色类，但事实上绿球的标签是蓝色类"></a>解释：缺点2：拿上图红蓝绿球来举例，假如k=3，而其中两个是红色，一个是蓝色。按照KNN就会认定绿球为红色类，但事实上绿球的标签是蓝色类</h4></li>
</ul>
</li>
<li><h4 id="缺点4："><a href="#缺点4：" class="headerlink" title="缺点4："></a>缺点4：<img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724031158354.png" alt="image-20200724031158354"></h4></li>
</ul>
<h3 id="机器学习流程回顾"><a href="#机器学习流程回顾" class="headerlink" title="机器学习流程回顾"></a>机器学习流程回顾</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724031341729.png" alt="image-20200724031341729"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记7</title>
    <url>/2020/07/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07/</url>
    <content><![CDATA[<h3 id="回归算法的评价"><a href="#回归算法的评价" class="headerlink" title="回归算法的评价"></a>回归算法的评价</h3><a id="more"></a>
<h4 id="一、从简单线性回归引出横梁标准"><a href="#一、从简单线性回归引出横梁标准" class="headerlink" title="一、从简单线性回归引出横梁标准"></a>一、从简单线性回归引出横梁标准</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730220946800.png" alt="image-20200730220946800"></p>
<h5 id="衡量标准就是测试集中的真实值与测试集中的预测值的差的平方，但是该衡量标准与样本数量m有关。为了避免不同样本量的数据集带来的模型优劣争议，我们要将这个式子除以m，得到下图"><a href="#衡量标准就是测试集中的真实值与测试集中的预测值的差的平方，但是该衡量标准与样本数量m有关。为了避免不同样本量的数据集带来的模型优劣争议，我们要将这个式子除以m，得到下图" class="headerlink" title="衡量标准就是测试集中的真实值与测试集中的预测值的差的平方，但是该衡量标准与样本数量m有关。为了避免不同样本量的数据集带来的模型优劣争议，我们要将这个式子除以m，得到下图"></a>衡量标准就是测试集中的真实值与测试集中的预测值的差的平方，但是该衡量标准与样本数量m有关。为了避免不同样本量的数据集带来的模型优劣争议，我们要将这个式子除以m，得到下图</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730221356928.png" alt="image-20200730221356928"></p>
<h5 id="但是MSE会使得该数据集的量纲（单位）为原来的平方（比如米变成平方米），所以我们可以给MSE开根号使得它和原数据集的量纲相同，这就是RMSE"><a href="#但是MSE会使得该数据集的量纲（单位）为原来的平方（比如米变成平方米），所以我们可以给MSE开根号使得它和原数据集的量纲相同，这就是RMSE" class="headerlink" title="但是MSE会使得该数据集的量纲（单位）为原来的平方（比如米变成平方米），所以我们可以给MSE开根号使得它和原数据集的量纲相同，这就是RMSE"></a>但是MSE会使得该数据集的量纲（单位）为原来的平方（比如米变成平方米），所以我们可以给MSE开根号使得它和原数据集的量纲相同，这就是RMSE</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730221818237.png" alt="image-20200730221818237"></p>
<h5 id="同样我们还可以使用以下的评价算法，注意线性回归模型找最优参数时不能用绝对值（不是处处可导），但是评价时就可以用，它们是互不影响的。"><a href="#同样我们还可以使用以下的评价算法，注意线性回归模型找最优参数时不能用绝对值（不是处处可导），但是评价时就可以用，它们是互不影响的。" class="headerlink" title="同样我们还可以使用以下的评价算法，注意线性回归模型找最优参数时不能用绝对值（不是处处可导），但是评价时就可以用，它们是互不影响的。"></a>同样我们还可以使用以下的评价算法，注意线性回归模型找最优参数时不能用绝对值（不是处处可导），但是评价时就可以用，它们是互不影响的。</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730222124641.png" alt="image-20200730222124641"></p>
<h3 id="二、RMSE-VS-MAE"><a href="#二、RMSE-VS-MAE" class="headerlink" title="二、RMSE VS  MAE"></a>二、RMSE VS  MAE</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730230527231.png" alt="image-20200730230527231"></p>
<h4 id="RMSE中的平方操作相当于放大了max-y-true-y-predict-，所以开根号以后依然比MAE大，因此我们要想让RMSE或MSE更有意义，就要让差值尽量小。"><a href="#RMSE中的平方操作相当于放大了max-y-true-y-predict-，所以开根号以后依然比MAE大，因此我们要想让RMSE或MSE更有意义，就要让差值尽量小。" class="headerlink" title="RMSE中的平方操作相当于放大了max(y_{true}-y_{predict})，所以开根号以后依然比MAE大，因此我们要想让RMSE或MSE更有意义，就要让差值尽量小。"></a>RMSE中的平方操作相当于放大了<script type="math/tex">max(y_{true}-y_{predict})</script>，所以开根号以后依然比MAE大，因此我们要想让RMSE或MSE更有意义，就要让差值尽量小。</h4><h3 id="三、最好的衡量线性回归法的指标"><a href="#三、最好的衡量线性回归法的指标" class="headerlink" title="三、最好的衡量线性回归法的指标"></a>三、最好的衡量线性回归法的指标</h3><h4 id="从分类算法的评价算法我们可以得出它可以通过0到1中的一个数来衡量模型的好坏。但由于RMSE和MSE只能得出平均差值，所以导致训练处理的模型不具有通用性（比如房价和学生成绩）"><a href="#从分类算法的评价算法我们可以得出它可以通过0到1中的一个数来衡量模型的好坏。但由于RMSE和MSE只能得出平均差值，所以导致训练处理的模型不具有通用性（比如房价和学生成绩）" class="headerlink" title="从分类算法的评价算法我们可以得出它可以通过0到1中的一个数来衡量模型的好坏。但由于RMSE和MSE只能得出平均差值，所以导致训练处理的模型不具有通用性（比如房价和学生成绩）"></a>从分类算法的评价算法我们可以得出它可以通过0到1中的一个数来衡量模型的好坏。但由于RMSE和MSE只能得出平均差值，所以导致训练处理的模型不具有通用性（比如房价和学生成绩）</h4><h4 id="因此，我们要使用以下算法来评价模型"><a href="#因此，我们要使用以下算法来评价模型" class="headerlink" title="因此，我们要使用以下算法来评价模型"></a>因此，我们要使用以下算法来评价模型</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730232425399.png" alt="image-20200730232425399"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730232723872.png" alt="image-20200730232723872"></p>
<h4 id="R2是衡量了我们模型没有产生错误的相应指标"><a href="#R2是衡量了我们模型没有产生错误的相应指标" class="headerlink" title="R2是衡量了我们模型没有产生错误的相应指标"></a>R2是衡量了我们模型没有产生错误的相应指标</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730233257406.png" alt="image-20200730233257406"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730233453803.png" alt="image-20200730233453803"></p>
<h4 id="注意，Var指方差"><a href="#注意，Var指方差" class="headerlink" title="注意，Var指方差"></a>注意，Var指方差</h4>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
        <tag>回归算法评价</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记9</title>
    <url>/2020/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09/</url>
    <content><![CDATA[<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><a id="more"></a>
<h4 id="一，什么是梯度下降法"><a href="#一，什么是梯度下降法" class="headerlink" title="一，什么是梯度下降法"></a>一，什么是梯度下降法</h4><h4 id="1、综述"><a href="#1、综述" class="headerlink" title="1、综述"></a>1、综述</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200807161901759.png" alt="image-20200807161901759"></p>
<h4 id="2、什么是梯度下降法"><a href="#2、什么是梯度下降法" class="headerlink" title="2、什么是梯度下降法"></a>2、什么是梯度下降法</h4><h4 id="2-1、什么是方向导数"><a href="#2-1、什么是方向导数" class="headerlink" title="2.1、什么是方向导数"></a>2.1、什么是方向导数</h4><ul>
<li><h5 id="2-1-1、什么是全微分"><a href="#2-1-1、什么是全微分" class="headerlink" title="2.1.1、什么是全微分"></a>2.1.1、什么是全微分</h5><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822224646922.png" alt="image-20200822224646922"></li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822224817278.png" alt="image-20200822224817278"></li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822224842975.png" alt="image-20200822224842975"></li>
</ul>
</li>
<li><h5 id="2-1-2、方向余弦与单位方向向量"><a href="#2-1-2、方向余弦与单位方向向量" class="headerlink" title="2.1.2、方向余弦与单位方向向量"></a>2.1.2、方向余弦与单位方向向量</h5><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230436729.png" alt="image-20200822230436729"></li>
</ul>
</li>
<li><h5 id="2-1-3、方向导数"><a href="#2-1-3、方向导数" class="headerlink" title="2.1.3、方向导数"></a>2.1.3、方向导数</h5><ul>
<li><h5 id="偏导数衡量了点在x轴和y轴移动时函数的变化，那么如果在其它方向移动呢？是否在任意方向上都有一个导数呢？答案是肯定的，那就是方向导数。"><a href="#偏导数衡量了点在x轴和y轴移动时函数的变化，那么如果在其它方向移动呢？是否在任意方向上都有一个导数呢？答案是肯定的，那就是方向导数。" class="headerlink" title="偏导数衡量了点在x轴和y轴移动时函数的变化，那么如果在其它方向移动呢？是否在任意方向上都有一个导数呢？答案是肯定的，那就是方向导数。"></a>偏导数衡量了点在x轴和y轴移动时函数的变化，那么如果在其它方向移动呢？是否在任意方向上都有一个导数呢？答案是肯定的，那就是方向导数。</h5></li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230631927.png" alt="image-20200822230631927"></p>
</li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230656288.png" alt="image-20200822230656288"></li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230718652.png" alt="image-20200822230718652"></li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230805318.png" alt="image-20200822230805318"></li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230823590.png" alt="image-20200822230823590"></li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230845941.png" alt="image-20200822230845941"></li>
</ul>
</li>
</ul>
<h4 id="2-2、什么是梯度"><a href="#2-2、什么是梯度" class="headerlink" title="2.2、什么是梯度"></a>2.2、什么是梯度</h4><ul>
<li><h5 id="上图（方向导数的最值）的向量a被称为梯度，记为grad-f-x0-y0-，它是综合了函数f所有偏导数的向量"><a href="#上图（方向导数的最值）的向量a被称为梯度，记为grad-f-x0-y0-，它是综合了函数f所有偏导数的向量" class="headerlink" title="上图（方向导数的最值）的向量a被称为梯度，记为grad f(x0,y0)，它是综合了函数f所有偏导数的向量"></a>上图（方向导数的最值）的向量a被称为梯度，记为grad f(x0,y0)，它是综合了函数f所有偏导数的向量</h5><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822231644755.png" alt="image-20200822231644755"></li>
</ul>
</li>
<li><h5 id="还可以表示为："><a href="#还可以表示为：" class="headerlink" title="还可以表示为："></a>还可以表示为：</h5><ul>
<li><h5 id=""><a href="#" class="headerlink" title=""></a><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808012812302.png" alt="image-20200808012812302"></h5></li>
</ul>
</li>
</ul>
<h4 id="2-3、梯度的意义"><a href="#2-3、梯度的意义" class="headerlink" title="2.3、梯度的意义"></a>2.3、梯度的意义</h4><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822233159506.png" alt="image-20200822233159506"></li>
</ul>
<h4 id="2-4、梯度下降"><a href="#2-4、梯度下降" class="headerlink" title="2.4、梯度下降"></a>2.4、梯度下降</h4><ul>
<li><h4 id="在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解效用函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。"><a href="#在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解效用函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。" class="headerlink" title="在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解效用函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。"></a>在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解效用函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。</h4></li>
<li><h4 id="简单地说，梯度下降就是沿着沿梯度下降的方向求解极小值时的自变量-也就是-theta-。"><a href="#简单地说，梯度下降就是沿着沿梯度下降的方向求解极小值时的自变量-也就是-theta-。" class="headerlink" title="简单地说，梯度下降就是沿着沿梯度下降的方向求解极小值时的自变量(也就是\theta)。"></a>简单地说，梯度下降就是沿着沿梯度下降的方向求解极小值时的自变量(也就是<script type="math/tex">\theta</script>)。</h4></li>
<li><h4 id="原理："><a href="#原理：" class="headerlink" title="原理："></a>原理：</h4><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808015214860.png" alt="image-20200808015214860"></p>
</li>
<li><h4 id="问题是这样做实在太慢，迭代过程及其耗时，所以人们在此基础上设计出更加快速的处理办法—增加学习率"><a href="#问题是这样做实在太慢，迭代过程及其耗时，所以人们在此基础上设计出更加快速的处理办法—增加学习率" class="headerlink" title="问题是这样做实在太慢，迭代过程及其耗时，所以人们在此基础上设计出更加快速的处理办法—增加学习率"></a>问题是这样做实在太慢，迭代过程及其耗时，所以人们在此基础上设计出更加快速的处理办法—增加学习率</h4></li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808015518727.png" alt="image-20200808015518727"></p>
</li>
<li><h4 id="下图用η表示学习率"><a href="#下图用η表示学习率" class="headerlink" title="下图用η表示学习率"></a>下图用η表示学习率</h4><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808020121659.png" alt="image-20200808020121659"></li>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808020208050.png" alt="image-20200808020208050"></li>
</ul>
</li>
</ul>
</li>
<li><h4 id="局部最优解和全局最优解"><a href="#局部最优解和全局最优解" class="headerlink" title="局部最优解和全局最优解"></a>局部最优解和全局最优解</h4><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808020340354.png" alt="image-20200808020340354"></li>
</ul>
</li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808020443300.png" alt="image-20200808020443300"></p>
</li>
</ul>
<h4 id="二、通过线性回归来学习：批量梯度下降"><a href="#二、通过线性回归来学习：批量梯度下降" class="headerlink" title="二、通过线性回归来学习：批量梯度下降"></a>二、通过线性回归来学习：批量梯度下降</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809002443221.png" alt="image-20200809002443221"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809002831126.png" alt="image-20200809002831126"></p>
<h4 id="说明"><a href="#说明" class="headerlink" title="说明:"></a>说明:</h4><ul>
<li><h4 id="首先这里面theta是未知向量，x和y是已知向量；其次为了简化，在Xb矩阵里面加一列1，进而简化损失函数"><a href="#首先这里面theta是未知向量，x和y是已知向量；其次为了简化，在Xb矩阵里面加一列1，进而简化损失函数" class="headerlink" title="首先这里面theta是未知向量，x和y是已知向量；其次为了简化，在Xb矩阵里面加一列1，进而简化损失函数"></a>首先这里面theta是未知向量，x和y是已知向量；其次为了简化，在Xb矩阵里面加一列1，进而简化损失函数</h4></li>
<li><h4 id="我们希望最终求出的梯度与样本数量m无关，所以请看下图"><a href="#我们希望最终求出的梯度与样本数量m无关，所以请看下图" class="headerlink" title="我们希望最终求出的梯度与样本数量m无关，所以请看下图"></a>我们希望最终求出的梯度与样本数量m无关，所以请看下图</h4></li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809003822687.png" alt="image-20200809003822687"></p>
<h4 id="三，线性回归中梯度下降的向量化表示"><a href="#三，线性回归中梯度下降的向量化表示" class="headerlink" title="三，线性回归中梯度下降的向量化表示"></a>三，线性回归中梯度下降的向量化表示</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809013423862.png" alt="image-20200809013423862"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809013459381.png" alt="image-20200809013459381"></p>
<h4 id="四、真实环境下使用梯度下降法的注意事项"><a href="#四、真实环境下使用梯度下降法的注意事项" class="headerlink" title="四、真实环境下使用梯度下降法的注意事项"></a>四、真实环境下使用梯度下降法的注意事项</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809015016880.png" alt="image-20200809015016880"></p>
<h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><ul>
<li><h4 id="梯度下降法对于多样本数据集比正规方程解更省时间"><a href="#梯度下降法对于多样本数据集比正规方程解更省时间" class="headerlink" title="梯度下降法对于多样本数据集比正规方程解更省时间"></a>梯度下降法对于多样本数据集比正规方程解更省时间</h4></li>
<li><h4 id="关于使用sklearn中的归一化可以参看https-blog-csdn-net-u011734144-article-details-84066784和Jupyter"><a href="#关于使用sklearn中的归一化可以参看https-blog-csdn-net-u011734144-article-details-84066784和Jupyter" class="headerlink" title="关于使用sklearn中的归一化可以参看https://blog.csdn.net/u011734144/article/details/84066784和Jupyter"></a>关于使用sklearn中的归一化可以参看<a href="https://blog.csdn.net/u011734144/article/details/84066784和Jupyter" target="_blank" rel="noopener">https://blog.csdn.net/u011734144/article/details/84066784和Jupyter</a></h4></li>
</ul>
<h4 id="五、随机梯度下降法"><a href="#五、随机梯度下降法" class="headerlink" title="五、随机梯度下降法"></a>五、随机梯度下降法</h4><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809220204342.png" alt="image-20200809220204342"></p>
<ul>
<li><h4 id="因为批量梯度下降法会随着样本数量的增加导致运行效率低下，所以我们考虑每次迭代时只随机选取某一行数据进行梯度计算"><a href="#因为批量梯度下降法会随着样本数量的增加导致运行效率低下，所以我们考虑每次迭代时只随机选取某一行数据进行梯度计算" class="headerlink" title="因为批量梯度下降法会随着样本数量的增加导致运行效率低下，所以我们考虑每次迭代时只随机选取某一行数据进行梯度计算"></a>因为批量梯度下降法会随着样本数量的增加导致运行效率低下，所以我们考虑每次迭代时只随机选取某一行数据进行梯度计算</h4></li>
</ul>
</li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809220604635.png" alt="image-20200809220604635"></p>
<ul>
<li><h4 id="由于每次迭代是随机选取的一行数据，所以损失函数的值未必是一定朝着梯度下降的方向进行搜索，因此，学习率就不能是一个固定值（固定值可能导致已经搜索到了最优参数，但是迭代未完成，这样会导致远离最优参数），而是要逐渐减小的"><a href="#由于每次迭代是随机选取的一行数据，所以损失函数的值未必是一定朝着梯度下降的方向进行搜索，因此，学习率就不能是一个固定值（固定值可能导致已经搜索到了最优参数，但是迭代未完成，这样会导致远离最优参数），而是要逐渐减小的" class="headerlink" title="由于每次迭代是随机选取的一行数据，所以损失函数的值未必是一定朝着梯度下降的方向进行搜索，因此，学习率就不能是一个固定值（固定值可能导致已经搜索到了最优参数，但是迭代未完成，这样会导致远离最优参数），而是要逐渐减小的"></a>由于每次迭代是随机选取的一行数据，所以损失函数的值未必是一定朝着梯度下降的方向进行搜索，因此，学习率就不能是一个固定值（固定值可能导致已经搜索到了最优参数，但是迭代未完成，这样会导致远离最优参数），而是要逐渐减小的</h4></li>
<li><h4 id="因此，学习率为上图所示，其中a和b是随机梯度下降法的超参数"><a href="#因此，学习率为上图所示，其中a和b是随机梯度下降法的超参数" class="headerlink" title="因此，学习率为上图所示，其中a和b是随机梯度下降法的超参数"></a>因此，学习率为上图所示，其中a和b是随机梯度下降法的超参数</h4></li>
<li><h4 id="其实，该学习率是采用了模拟退火的思想"><a href="#其实，该学习率是采用了模拟退火的思想" class="headerlink" title="其实，该学习率是采用了模拟退火的思想"></a>其实，该学习率是采用了模拟退火的思想</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809230249384.png" alt="image-20200809230249384"></p>
</li>
<li><h4 id="在随机梯度下降中，迭代次数指的是对提供的样本要迭代几遍，设样本数为m，迭代次数为n-iters，则真正的迭代次数为m-n-iters"><a href="#在随机梯度下降中，迭代次数指的是对提供的样本要迭代几遍，设样本数为m，迭代次数为n-iters，则真正的迭代次数为m-n-iters" class="headerlink" title="在随机梯度下降中，迭代次数指的是对提供的样本要迭代几遍，设样本数为m，迭代次数为n_iters，则真正的迭代次数为m*n_iters"></a>在随机梯度下降中，迭代次数指的是对提供的样本要迭代几遍，设样本数为m，迭代次数为n_iters，则真正的迭代次数为m*n_iters</h4></li>
</ul>
</li>
</ul>
<h4 id="六、关于梯度的调试"><a href="#六、关于梯度的调试" class="headerlink" title="六、关于梯度的调试"></a>六、关于梯度的调试</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810003902485.png" alt="image-20200810003902485"></p>
<h4 id="我们为了验证我们计算的梯度是否正确可以采用如上方法，在目标点（红点）的正方向和负方向分别取两个点，这两个点距离红点的距离近乎为0，然后将这两个蓝点连线，则它的斜率近似等于红点切线处的斜率"><a href="#我们为了验证我们计算的梯度是否正确可以采用如上方法，在目标点（红点）的正方向和负方向分别取两个点，这两个点距离红点的距离近乎为0，然后将这两个蓝点连线，则它的斜率近似等于红点切线处的斜率" class="headerlink" title="我们为了验证我们计算的梯度是否正确可以采用如上方法，在目标点（红点）的正方向和负方向分别取两个点，这两个点距离红点的距离近乎为0，然后将这两个蓝点连线，则它的斜率近似等于红点切线处的斜率"></a>我们为了验证我们计算的梯度是否正确可以采用如上方法，在目标点（红点）的正方向和负方向分别取两个点，这两个点距离红点的距离近乎为0，然后将这两个蓝点连线，则它的斜率近似等于红点切线处的斜率</h4><h4 id="下面我们推广到高维数据集上："><a href="#下面我们推广到高维数据集上：" class="headerlink" title="下面我们推广到高维数据集上："></a>下面我们推广到高维数据集上：</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810004502460.png" alt="image-20200810004502460"></p>
<h4 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810011144727.png" alt="image-20200810011144727"></p>
<h4 id="小批量梯度下降法结合了批量梯度下降法的稳定（一定沿着梯度下降方向计算）以及随机梯度下降法的速度快的优点，每次迭代只计算k行样本"><a href="#小批量梯度下降法结合了批量梯度下降法的稳定（一定沿着梯度下降方向计算）以及随机梯度下降法的速度快的优点，每次迭代只计算k行样本" class="headerlink" title="小批量梯度下降法结合了批量梯度下降法的稳定（一定沿着梯度下降方向计算）以及随机梯度下降法的速度快的优点，每次迭代只计算k行样本"></a>小批量梯度下降法结合了批量梯度下降法的稳定（一定沿着梯度下降方向计算）以及随机梯度下降法的速度快的优点，每次迭代只计算k行样本</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810011434738.png" alt="image-20200810011434738"></p>
<h4 id="关于梯度上升法："><a href="#关于梯度上升法：" class="headerlink" title="关于梯度上升法："></a>关于梯度上升法：</h4><ul>
<li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810011643033.png" alt="image-20200810011643033"></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记8</title>
    <url>/2020/07/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08/</url>
    <content><![CDATA[<h3 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h3><a id="more"></a>
<h4 id="一、什么是多元线性回归"><a href="#一、什么是多元线性回归" class="headerlink" title="一、什么是多元线性回归"></a>一、什么是多元线性回归</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730235347025.png" alt="image-20200730235347025"></p>
<h4 id="二、多元线性回归的正规方程解"><a href="#二、多元线性回归的正规方程解" class="headerlink" title="二、多元线性回归的正规方程解"></a>二、多元线性回归的正规方程解</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731030050773.png" alt="image-20200731030050773"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731030338464.png" alt="image-20200731030338464"></p>
<h4 id="因为X是一个特征矩阵，所以X-i-本身就是行向量，因此-theta必须由行向量转置为列向量"><a href="#因为X是一个特征矩阵，所以X-i-本身就是行向量，因此-theta必须由行向量转置为列向量" class="headerlink" title="因为X是一个特征矩阵，所以X^{(i)}本身就是行向量，因此\theta必须由行向量转置为列向量"></a>因为X是一个特征矩阵，所以<script type="math/tex">X^{(i)}</script>本身就是行向量，因此<script type="math/tex">\theta</script>必须由行向量转置为列向量</h4><h4 id="接下来进一步推广到整个特征矩阵"><a href="#接下来进一步推广到整个特征矩阵" class="headerlink" title="接下来进一步推广到整个特征矩阵"></a>接下来进一步推广到整个特征矩阵</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731031136440.png" alt="image-20200731031136440"></p>
<h4 id="因为X-b是m-n的矩阵，-theta-是n行列向量，所以-hat-y-是一个m行的列向量"><a href="#因为X-b是m-n的矩阵，-theta-是n行列向量，所以-hat-y-是一个m行的列向量" class="headerlink" title="因为X_b是m*n的矩阵，\theta 是n行列向量，所以\hat{y}是一个m行的列向量"></a>因为<script type="math/tex">X_b</script>是m*n的矩阵，<script type="math/tex">\theta</script> 是n行列向量，所以<script type="math/tex">\hat{y}</script>是一个m行的列向量</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731032608917.png" alt="image-20200731032608917"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731032641791.png" alt="image-20200731032641791"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731032712657.png" alt="image-20200731032712657"></p>
<h4 id="因为-theta只是X前面的系数，与量纲无关，所以不需要归一化处理"><a href="#因为-theta只是X前面的系数，与量纲无关，所以不需要归一化处理" class="headerlink" title="因为\theta只是X前面的系数，与量纲无关，所以不需要归一化处理"></a>因为<script type="math/tex">\theta</script>只是X前面的系数，与量纲无关，所以不需要归一化处理</h4><h4 id="三、线性回归总结"><a href="#三、线性回归总结" class="headerlink" title="三、线性回归总结"></a>三、线性回归总结</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200805000611219.png" alt="image-20200805000611219"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200805000625908.png" alt="image-20200805000625908"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200805000642638.png" alt="image-20200805000642638"></p>
<h4 id="关于可解释性，我们拿波士顿房价预测来说，训练得到的参数的正负表示该特征与房价呈正相关还是负相关；绝对值大小表示对房价的影响程度，越大则对房价影响越大，具体看Jupyter。"><a href="#关于可解释性，我们拿波士顿房价预测来说，训练得到的参数的正负表示该特征与房价呈正相关还是负相关；绝对值大小表示对房价的影响程度，越大则对房价影响越大，具体看Jupyter。" class="headerlink" title="关于可解释性，我们拿波士顿房价预测来说，训练得到的参数的正负表示该特征与房价呈正相关还是负相关；绝对值大小表示对房价的影响程度，越大则对房价影响越大，具体看Jupyter。"></a>关于可解释性，我们拿波士顿房价预测来说，训练得到的参数的正负表示该特征与房价呈正相关还是负相关；绝对值大小表示对房价的影响程度，越大则对房价影响越大，具体看Jupyter。</h4>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
        <tag>多元线性回归</tag>
        <tag>模型可解释性</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习笔记10</title>
    <url>/2020/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010/</url>
    <content><![CDATA[<h4 id="PCA-主成分分析（Principal-Component-Analysis）"><a href="#PCA-主成分分析（Principal-Component-Analysis）" class="headerlink" title="PCA-主成分分析（Principal Component Analysis）"></a>PCA-主成分分析（Principal Component Analysis）</h4><a id="more"></a>
<h4 id="一、PCA综述"><a href="#一、PCA综述" class="headerlink" title="一、PCA综述"></a>一、PCA综述</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810220126595.png" alt="image-20200810220126595"></p>
<h4 id="二、详解PCA"><a href="#二、详解PCA" class="headerlink" title="二、详解PCA"></a>二、详解PCA</h4><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810223917827.png" alt="image-20200810223917827"></p>
<ul>
<li><h4 id="我们首先想到的降维方法是舍弃特征1或舍弃特征2，这样样本点就会映射到单一坐标轴上，达到了降维的效果。如下图："><a href="#我们首先想到的降维方法是舍弃特征1或舍弃特征2，这样样本点就会映射到单一坐标轴上，达到了降维的效果。如下图：" class="headerlink" title="我们首先想到的降维方法是舍弃特征1或舍弃特征2，这样样本点就会映射到单一坐标轴上，达到了降维的效果。如下图："></a>我们首先想到的降维方法是舍弃特征1或舍弃特征2，这样样本点就会映射到单一坐标轴上，达到了降维的效果。如下图：</h4></li>
<li><h5 id="左图为舍弃特征1；右图为舍弃特征2："><a href="#左图为舍弃特征1；右图为舍弃特征2：" class="headerlink" title="左图为舍弃特征1；右图为舍弃特征2："></a>左图为舍弃特征1；右图为舍弃特征2：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810224220339.png" alt="image-20200810224220339"></p>
</li>
<li><h4 id="显然右图的降维效果比左图好，因为样本点直接距离较大，拥有更高的可区分度"><a href="#显然右图的降维效果比左图好，因为样本点直接距离较大，拥有更高的可区分度" class="headerlink" title="显然右图的降维效果比左图好，因为样本点直接距离较大，拥有更高的可区分度"></a>显然右图的降维效果比左图好，因为样本点直接距离较大，拥有更高的可区分度</h4></li>
</ul>
</li>
<li><h4 id="但是，这是最好的方案吗？显然不是。我们想象一下-能够找到下图这样的一条斜线w-将数据降维到w上-映射到w上-之后-能最好的保留原来的分布特征-且这些点分布在了一个轴上-斜线w-后点和点之间的距离也比之前的两种方案更加的大-此时的区分度也更加明显"><a href="#但是，这是最好的方案吗？显然不是。我们想象一下-能够找到下图这样的一条斜线w-将数据降维到w上-映射到w上-之后-能最好的保留原来的分布特征-且这些点分布在了一个轴上-斜线w-后点和点之间的距离也比之前的两种方案更加的大-此时的区分度也更加明显" class="headerlink" title="但是，这是最好的方案吗？显然不是。我们想象一下,能够找到下图这样的一条斜线w,将数据降维到w上(映射到w上)之后,能最好的保留原来的分布特征,且这些点分布在了一个轴上(斜线w)后点和点之间的距离也比之前的两种方案更加的大,此时的区分度也更加明显"></a>但是，这是最好的方案吗？显然不是。我们想象一下,能够找到下图这样的一条斜线w,将数据降维到w上(映射到w上)之后,能最好的保留原来的分布特征,且这些点分布在了一个轴上(斜线w)后点和点之间的距离也比之前的两种方案更加的大,此时的区分度也更加明显</h4><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810224703164.png" alt="image-20200810224703164"></p>
</li>
<li><h4 id="接下来我们要思考以下两个问题了："><a href="#接下来我们要思考以下两个问题了：" class="headerlink" title="接下来我们要思考以下两个问题了："></a>接下来我们要思考以下两个问题了：</h4><ul>
<li><h5 id="如何找到让这个样本降维后间距最大的轴"><a href="#如何找到让这个样本降维后间距最大的轴" class="headerlink" title="如何找到让这个样本降维后间距最大的轴?"></a>如何找到让这个样本降维后间距最大的轴?</h5></li>
<li><h5 id="如何定义样本间距"><a href="#如何定义样本间距" class="headerlink" title="如何定义样本间距?"></a>如何定义样本间距?</h5></li>
</ul>
</li>
<li><h4 id="在统计学中-有一个直接的指标可以表示样本间的间距-离散程度-那就是方差-Variance"><a href="#在统计学中-有一个直接的指标可以表示样本间的间距-离散程度-那就是方差-Variance" class="headerlink" title="在统计学中,有一个直接的指标可以表示样本间的间距(离散程度),那就是方差(Variance)"></a>在统计学中,有一个直接的指标可以表示样本间的间距(离散程度),那就是方差(Variance)</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810225109737.png" alt="image-20200810225109737"></p>
</li>
<li><h4 id="上述的问题就变为了：找到一个轴-使得样本空间的所有点映射到这个轴之后-方差最大"><a href="#上述的问题就变为了：找到一个轴-使得样本空间的所有点映射到这个轴之后-方差最大" class="headerlink" title="上述的问题就变为了：找到一个轴,使得样本空间的所有点映射到这个轴之后,方差最大"></a>上述的问题就变为了：找到一个轴,使得样本空间的所有点映射到这个轴之后,方差最大</h4></li>
<li><h4 id="接下来就是求解这个轴："><a href="#接下来就是求解这个轴：" class="headerlink" title="接下来就是求解这个轴："></a>接下来就是求解这个轴：</h4><ul>
<li><h5 id="首先，将样例的均值归为0（demean），即将所有样本减去这批样本的均值-所有的样本点都会移动，因此可以看作样本点不动，移动坐标轴-，之后就变为了下图所示"><a href="#首先，将样例的均值归为0（demean），即将所有样本减去这批样本的均值-所有的样本点都会移动，因此可以看作样本点不动，移动坐标轴-，之后就变为了下图所示" class="headerlink" title="首先，将样例的均值归为0（demean），即将所有样本减去这批样本的均值(所有的样本点都会移动，因此可以看作样本点不动，移动坐标轴)，之后就变为了下图所示"></a>首先，将样例的均值归为0（demean），即将所有样本减去这批样本的均值(所有的样本点都会移动，因此可以看作样本点不动，移动坐标轴)，之后就变为了下图所示</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810225647955.png" alt="image-20200810225647955"></p>
<h5 id="此时的样本的均值就为0，方差就变成了"><a href="#此时的样本的均值就为0，方差就变成了" class="headerlink" title="此时的样本的均值就为0，方差就变成了"></a>此时的样本的均值就为0，方差就变成了</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810230237650.png" alt="image-20200810230237650"></p>
</li>
<li><h5 id="我们想要求w轴的方向-w1-w2-使得-Var-Xproject-最大，Xproject-是映射到w轴之后的X的坐标："><a href="#我们想要求w轴的方向-w1-w2-使得-Var-Xproject-最大，Xproject-是映射到w轴之后的X的坐标：" class="headerlink" title="我们想要求w轴的方向(w1,w2),使得 Var(Xproject) 最大，Xproject 是映射到w轴之后的X的坐标："></a>我们想要求w轴的方向(w1,w2),使得 <em>Var(X<strong>project</strong>)</em> 最大，<em>X</em>project 是映射到w轴之后的X的坐标：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810230906480.png" alt="image-20200810230906480"></p>
<h5 id="注意：因为X是矩阵，X-i-是X中的一个向量，所以在公式中不能写成括号的平方，而是模的平方"><a href="#注意：因为X是矩阵，X-i-是X中的一个向量，所以在公式中不能写成括号的平方，而是模的平方" class="headerlink" title="注意：因为X是矩阵，X(i)是X中的一个向量，所以在公式中不能写成括号的平方，而是模的平方"></a>注意：因为X是矩阵，X(i)是X中的一个向量，所以在公式中不能写成括号的平方，而是模的平方</h5><h5 id="接下来求解-X-i-project-，即如何用-X-i-表示（下图点积可以参考《机器学习中的数学》13页-1-4-4）"><a href="#接下来求解-X-i-project-，即如何用-X-i-表示（下图点积可以参考《机器学习中的数学》13页-1-4-4）" class="headerlink" title="接下来求解$X^{(i)}_{project}$，即如何用$X^{(i)}$表示（下图点积可以参考《机器学习中的数学》13页 1.4.4）"></a>接下来求解$X^{(i)}_{project}$，即如何用$X^{(i)}$表示（下图点积可以参考《机器学习中的数学》13页 1.4.4）</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810233526467.png" alt="image-20200810233526467"></p>
<h5 id="注意：因为我们要将样本点映射到w方向上，所以将w定义为单位向量，其实最终求得是样本点向量在w方向上的分量，即蓝色线条，它对应的就是-vec-X-i-project"><a href="#注意：因为我们要将样本点映射到w方向上，所以将w定义为单位向量，其实最终求得是样本点向量在w方向上的分量，即蓝色线条，它对应的就是-vec-X-i-project" class="headerlink" title="注意：因为我们要将样本点映射到w方向上，所以将w定义为单位向量，其实最终求得是样本点向量在w方向上的分量，即蓝色线条，它对应的就是$\vec{X^{(i)}_{project}}$"></a>注意：因为我们要将样本点映射到w方向上，所以将w定义为单位向量，其实最终求得是样本点向量在w方向上的分量，即蓝色线条，它对应的就是$\vec{X^{(i)}_{project}}$</h5></li>
</ul>
</li>
<li><h4 id="最终："><a href="#最终：" class="headerlink" title="最终："></a>最终：</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810234231684.png" alt="image-20200810234231684"></p>
</li>
</ul>
</li>
</ul>
<h4 id="三、使用梯度上升法解决PCA问题"><a href="#三、使用梯度上升法解决PCA问题" class="headerlink" title="三、使用梯度上升法解决PCA问题"></a>三、使用梯度上升法解决PCA问题</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810235951325.png" alt="image-20200810235951325"></p>
<h4 id="接下来进行向量化，让其运行效率更高"><a href="#接下来进行向量化，让其运行效率更高" class="headerlink" title="接下来进行向量化，让其运行效率更高"></a>接下来进行向量化，让其运行效率更高</h4><ul>
<li><h5 id="在向量化时要想清楚谁是矩阵，谁是向量，向量是行还是列向量"><a href="#在向量化时要想清楚谁是矩阵，谁是向量，向量是行还是列向量" class="headerlink" title="在向量化时要想清楚谁是矩阵，谁是向量，向量是行还是列向量"></a>在向量化时要想清楚谁是矩阵，谁是向量，向量是行还是列向量</h5></li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200811000106484.png" alt="image-20200811000106484"></p>
<h4 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200811000436388.png" alt="image-20200811000436388"></p>
<h4 id="四、求数据的前n个主成分（即新坐标系的每一个轴对应的方向）"><a href="#四、求数据的前n个主成分（即新坐标系的每一个轴对应的方向）" class="headerlink" title="四、求数据的前n个主成分（即新坐标系的每一个轴对应的方向）"></a>四、求数据的前n个主成分（即新坐标系的每一个轴对应的方向）</h4><ul>
<li><h5 id="PCA的本质是将数据从一个坐标系转移到另一个坐标系"><a href="#PCA的本质是将数据从一个坐标系转移到另一个坐标系" class="headerlink" title="PCA的本质是将数据从一个坐标系转移到另一个坐标系"></a>PCA的本质是将数据从一个坐标系转移到另一个坐标系</h5></li>
<li><h5 id="下面是求解过程："><a href="#下面是求解过程：" class="headerlink" title="下面是求解过程："></a>下面是求解过程：</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200814010029327.png" alt="image-20200814010029327"></p>
<h5 id="先将-X-i-project-乘以单位向量-vec-w-变成-vec-X-i-project-，这个就是数据X在第一主成分上的分量，我们用X减去这一分量，即-vec-X-i-vec-X-i-project-，这是向量的减法，最终得出的是图中绿色的向量。"><a href="#先将-X-i-project-乘以单位向量-vec-w-变成-vec-X-i-project-，这个就是数据X在第一主成分上的分量，我们用X减去这一分量，即-vec-X-i-vec-X-i-project-，这是向量的减法，最终得出的是图中绿色的向量。" class="headerlink" title="先将$|{X^{(i)}_{project}}|$乘以单位向量$\vec{w}$变成$\vec{X^{(i)}_{project}}$，这个就是数据X在第一主成分上的分量，我们用X减去这一分量，即$\vec{X^{(i)}} - \vec{X^{(i)}_{project}}$，这是向量的减法，最终得出的是图中绿色的向量。"></a>先将$|{X^{(i)}_{project}}|$乘以单位向量$\vec{w}$变成$\vec{X^{(i)}_{project}}$，这个就是数据X在第一主成分上的分量，我们用X减去这一分量，即$\vec{X^{(i)}} - \vec{X^{(i)}_{project}}$，这是向量的减法，最终得出的是图中绿色的向量。</h5></li>
<li><h5 id="然后在新的数据上求第一主成分（即原数据X的第二主成分），以此类推。"><a href="#然后在新的数据上求第一主成分（即原数据X的第二主成分），以此类推。" class="headerlink" title="然后在新的数据上求第一主成分（即原数据X的第二主成分），以此类推。"></a>然后在新的数据上求第一主成分（即原数据X的第二主成分），以此类推。</h5></li>
</ul>
</li>
</ul>
<h4 id="五、高维数据向低维数据映射"><a href="#五、高维数据向低维数据映射" class="headerlink" title="五、高维数据向低维数据映射"></a>五、高维数据向低维数据映射</h4><h5 id="PCA的本质是将数据从一个坐标系转换到另一个坐标系上，即原来是n维坐标系，转换后也是n维坐标系。但是我们要将高维向低维映射，所以要从转换后的n维坐标系中取前k个坐标轴形成一个k维的坐标系。然后将X中的每一行样本都乘以W中每一个维度的坐标轴。这样就从n维降到了k维："><a href="#PCA的本质是将数据从一个坐标系转换到另一个坐标系上，即原来是n维坐标系，转换后也是n维坐标系。但是我们要将高维向低维映射，所以要从转换后的n维坐标系中取前k个坐标轴形成一个k维的坐标系。然后将X中的每一行样本都乘以W中每一个维度的坐标轴。这样就从n维降到了k维：" class="headerlink" title="PCA的本质是将数据从一个坐标系转换到另一个坐标系上，即原来是n维坐标系，转换后也是n维坐标系。但是我们要将高维向低维映射，所以要从转换后的n维坐标系中取前k个坐标轴形成一个k维的坐标系。然后将X中的每一行样本都乘以W中每一个维度的坐标轴。这样就从n维降到了k维："></a>PCA的本质是将数据从一个坐标系转换到另一个坐标系上，即原来是n维坐标系，转换后也是n维坐标系。但是我们要将高维向低维映射，所以要从转换后的n维坐标系中取前k个坐标轴形成一个k维的坐标系。然后将X中的每一行样本都乘以W中每一个维度的坐标轴。这样就从n维降到了k维：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200814224611023.png" alt="image-20200814224611023"></p>
<h5 id="也可以从k维恢复到n维，但是恢复回来的-X-m-不等于原来的X："><a href="#也可以从k维恢复到n维，但是恢复回来的-X-m-不等于原来的X：" class="headerlink" title="也可以从k维恢复到n维，但是恢复回来的$X_m$不等于原来的X："></a>也可以从k维恢复到n维，但是恢复回来的$X_m$不等于原来的X：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200814224911159.png" alt="image-20200814224911159"></p>
<h4 id="六、PCA在人脸识别中的应用"><a href="#六、PCA在人脸识别中的应用" class="headerlink" title="六、PCA在人脸识别中的应用"></a>六、PCA在人脸识别中的应用</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200815183722277.png" alt="image-20200815183722277"></p>
<h5 id="对于-W-k-来说，本身每一行代表一个方向。那么换一种思路：如果每一行看作一个样本，那么第一行表示最能反映原数据X的样本特征，以此类推。"><a href="#对于-W-k-来说，本身每一行代表一个方向。那么换一种思路：如果每一行看作一个样本，那么第一行表示最能反映原数据X的样本特征，以此类推。" class="headerlink" title="对于$W_k$来说，本身每一行代表一个方向。那么换一种思路：如果每一行看作一个样本，那么第一行表示最能反映原数据X的样本特征，以此类推。"></a>对于$W_k$来说，本身每一行代表一个方向。那么换一种思路：如果每一行看作一个样本，那么第一行表示最能反映原数据X的样本特征，以此类推。</h5><h5 id="在人脸识别中-X的每一行对应一个人脸数据，W的每一行也是一个人脸数据，这个就被称为特征脸，所以每一个特征脸其实对应一个主成分"><a href="#在人脸识别中-X的每一行对应一个人脸数据，W的每一行也是一个人脸数据，这个就被称为特征脸，所以每一个特征脸其实对应一个主成分" class="headerlink" title="在人脸识别中,X的每一行对应一个人脸数据，W的每一行也是一个人脸数据，这个就被称为特征脸，所以每一个特征脸其实对应一个主成分"></a>在人脸识别中,X的每一行对应一个人脸数据，W的每一行也是一个人脸数据，这个就被称为特征脸，所以每一个特征脸其实对应一个主成分</h5>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>PCA</tag>
        <tag>非监督学习</tag>
        <tag>数据降维</tag>
      </tags>
  </entry>
</search>
