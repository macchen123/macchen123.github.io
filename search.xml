<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习笔记14</title>
      <link href="/2020/09/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014/"/>
      <url>/2020/09/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B014/</url>
      
        <content type="html"><![CDATA[<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><a id="more"></a><ul><li><h4 id="机器学习：SVM（基础理解）"><a href="#机器学习：SVM（基础理解）" class="headerlink" title="机器学习：SVM（基础理解）"></a><a href="https://www.cnblogs.com/volcao/p/9411514.html" target="_blank" rel="noopener">机器学习：SVM（基础理解）</a></h4></li><li><h4 id="机器学习：SVM（目标函数推导：Hard-Margin-SVM、Soft-Margin-SVM）"><a href="#机器学习：SVM（目标函数推导：Hard-Margin-SVM、Soft-Margin-SVM）" class="headerlink" title="机器学习：SVM（目标函数推导：Hard Margin SVM、Soft Margin SVM）"></a><a href="https://www.cnblogs.com/volcao/p/9413528.html" target="_blank" rel="noopener">机器学习：SVM（目标函数推导：Hard Margin SVM、Soft Margin SVM）</a></h4></li><li><h4 id="机器学习：SVM（scikit-learn-中的-SVM：LinearSVC）"><a href="#机器学习：SVM（scikit-learn-中的-SVM：LinearSVC）" class="headerlink" title="机器学习：SVM（scikit-learn 中的 SVM：LinearSVC）"></a><a href="https://www.cnblogs.com/volcao/p/9464009.html" target="_blank" rel="noopener">机器学习：SVM（scikit-learn 中的 SVM：LinearSVC）</a></h4></li><li><h4 id="机器学习：SVM（非线性数据分类：SVM中使用多项式特征和核函数SVC）"><a href="#机器学习：SVM（非线性数据分类：SVM中使用多项式特征和核函数SVC）" class="headerlink" title="机器学习：SVM（非线性数据分类：SVM中使用多项式特征和核函数SVC）"></a><a href="https://www.cnblogs.com/volcao/p/9464991.html" target="_blank" rel="noopener">机器学习：SVM（非线性数据分类：SVM中使用多项式特征和核函数SVC）</a></h4></li><li><h4 id="机器学习：SVM（核函数、高斯核函数RBF）"><a href="#机器学习：SVM（核函数、高斯核函数RBF）" class="headerlink" title="机器学习：SVM（核函数、高斯核函数RBF）"></a><a href="https://www.cnblogs.com/volcao/p/9465214.html" target="_blank" rel="noopener">机器学习：SVM（核函数、高斯核函数RBF）</a></h4></li><li><h4 id="机器学习：SVM（scikit-learn-中的-RBF、RBF-中的超参数-γ）"><a href="#机器学习：SVM（scikit-learn-中的-RBF、RBF-中的超参数-γ）" class="headerlink" title="机器学习：SVM（scikit-learn 中的 RBF、RBF 中的超参数 γ）"></a><a href="https://www.cnblogs.com/volcao/p/9470209.html" target="_blank" rel="noopener">机器学习：SVM（scikit-learn 中的 RBF、RBF 中的超参数 γ）</a></h4></li><li><h4 id="机器学习：SVM（SVM-思想解决回归问题）"><a href="#机器学习：SVM（SVM-思想解决回归问题）" class="headerlink" title="机器学习：SVM（SVM 思想解决回归问题）"></a><a href="https://www.cnblogs.com/volcao/p/9471800.html" target="_blank" rel="noopener">机器学习：SVM（SVM 思想解决回归问题）</a></h4></li></ul><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><ul><li><h4 id="机器学习：决策树（基本思想、信息熵、构建决策树的问题及思想）"><a href="#机器学习：决策树（基本思想、信息熵、构建决策树的问题及思想）" class="headerlink" title="机器学习：决策树（基本思想、信息熵、构建决策树的问题及思想）"></a><a href="https://www.cnblogs.com/volcao/p/9474366.html" target="_blank" rel="noopener">机器学习：决策树（基本思想、信息熵、构建决策树的问题及思想）</a></h4></li><li><h4 id="机器学习：决策树（使用信息熵寻找最优划分）"><a href="#机器学习：决策树（使用信息熵寻找最优划分）" class="headerlink" title="机器学习：决策树（使用信息熵寻找最优划分）"></a><a href="https://www.cnblogs.com/volcao/p/9477776.html" target="_blank" rel="noopener">机器学习：决策树（使用信息熵寻找最优划分）</a></h4></li><li><h4 id="机器学习：决策树（使用基尼系数划分节点数据集）"><a href="#机器学习：决策树（使用基尼系数划分节点数据集）" class="headerlink" title="机器学习：决策树（使用基尼系数划分节点数据集）"></a><a href="https://www.cnblogs.com/volcao/p/9478314.html" target="_blank" rel="noopener">机器学习：决策树（使用基尼系数划分节点数据集）</a></h4></li><li><h4 id="机器学习：决策树（CART-、决策树中的超参数）"><a href="#机器学习：决策树（CART-、决策树中的超参数）" class="headerlink" title="机器学习：决策树（CART 、决策树中的超参数）"></a><a href="https://www.cnblogs.com/volcao/p/9480431.html" target="_blank" rel="noopener">机器学习：决策树（CART 、决策树中的超参数）</a></h4></li><li><h4 id="机器学习：决策树（决策树解决回归问题、决策树算法的局限性）"><a href="#机器学习：决策树（决策树解决回归问题、决策树算法的局限性）" class="headerlink" title="机器学习：决策树（决策树解决回归问题、决策树算法的局限性）"></a><a href="https://www.cnblogs.com/volcao/p/9481634.html" target="_blank" rel="noopener">机器学习：决策树（决策树解决回归问题、决策树算法的局限性）</a></h4></li></ul><h3 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h3><ul><li><h4 id="机器学习：集成学习（集成学习思想、scikit-learn-中的集成分类器）"><a href="#机器学习：集成学习（集成学习思想、scikit-learn-中的集成分类器）" class="headerlink" title="机器学习：集成学习（集成学习思想、scikit-learn 中的集成分类器）"></a><a href="https://www.cnblogs.com/volcao/p/9482268.html" target="_blank" rel="noopener">机器学习：集成学习（集成学习思想、scikit-learn 中的集成分类器）</a></h4></li><li><h4 id="机器学习：集成学习（Soft-Voting-Classifier）"><a href="#机器学习：集成学习（Soft-Voting-Classifier）" class="headerlink" title="机器学习：集成学习（Soft Voting Classifier）"></a><a href="https://www.cnblogs.com/volcao/p/9483026.html" target="_blank" rel="noopener">机器学习：集成学习（Soft Voting Classifier）</a></h4></li><li><h4 id="机器学习：集成学习（Bagging、Pasting）"><a href="#机器学习：集成学习（Bagging、Pasting）" class="headerlink" title="机器学习：集成学习（Bagging、Pasting）"></a><a href="https://www.cnblogs.com/volcao/p/9486417.html" target="_blank" rel="noopener">机器学习：集成学习（Bagging、Pasting）</a></h4></li><li><h4 id="机器学习：集成学习（OOB-和-关于-Bagging-的更多讨论）"><a href="#机器学习：集成学习（OOB-和-关于-Bagging-的更多讨论）" class="headerlink" title="机器学习：集成学习（OOB 和 关于 Bagging 的更多讨论）"></a><a href="https://www.cnblogs.com/volcao/p/9488113.html" target="_blank" rel="noopener">机器学习：集成学习（OOB 和 关于 Bagging 的更多讨论）</a></h4></li><li><h4 id="机器学习：集成学习（随机森林、集成学习参数）"><a href="#机器学习：集成学习（随机森林、集成学习参数）" class="headerlink" title="机器学习：集成学习（随机森林、集成学习参数）"></a><a href="https://www.cnblogs.com/volcao/p/9488771.html" target="_blank" rel="noopener">机器学习：集成学习（随机森林、集成学习参数）</a></h4></li><li><h4 id="机器学习：集成学习（Ada-Boosting-和-Gradient-Boosting）"><a href="#机器学习：集成学习（Ada-Boosting-和-Gradient-Boosting）" class="headerlink" title="机器学习：集成学习（Ada Boosting 和 Gradient Boosting）"></a><a href="https://www.cnblogs.com/volcao/p/9490651.html" target="_blank" rel="noopener">机器学习：集成学习（Ada Boosting 和 Gradient Boosting）</a></h4></li><li><h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a><a href="http://mantchs.com/2019/07/09/ML/GBDT/" target="_blank" rel="noopener">GBDT</a></h4></li><li><h4 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a><a href="http://mantchs.com/2019/07/10/ML/XGBoost/" target="_blank" rel="noopener">XGBoost</a></h4></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
            <tag> 决策树 </tag>
            
            <tag> 随机森林 </tag>
            
            <tag> 集成学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>acl论文记录</title>
      <link href="/2020/09/16/acl%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/"/>
      <url>/2020/09/16/acl%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h3 id="对话系统"><a href="#对话系统" class="headerlink" title="对话系统"></a>对话系统</h3><a id="more"></a><h4 id="一、Dynamic-Fusion-Network-for-Multi-Domain-End-to-end-Task-Oriented-Dialog"><a href="#一、Dynamic-Fusion-Network-for-Multi-Domain-End-to-end-Task-Oriented-Dialog" class="headerlink" title="一、Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog"></a>一、Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog</h4><h5 id="动态聚合网络，用于多领域端到端任务型对话系统"><a href="#动态聚合网络，用于多领域端到端任务型对话系统" class="headerlink" title="动态聚合网络，用于多领域端到端任务型对话系统"></a>动态聚合网络，用于多领域端到端任务型对话系统</h5><ul><li>Abstract：<br>Recent studies have shown remarkable suc- cess in end-to-end<br>task-oriented dialog system. However, most neural models rely on large<br>training data, which are only available for a certain number of task domains,<br>such as nav- igation and scheduling. This makes it difficult to scalable for a<br>new domain with limited la- beled data. However, there has been relatively<br>little research on how to effectively use data from all domains to improve the<br>performance of each domain and also unseen domains. To this end, we<br>investigate methods that can make explicit use of domain knowledge and intro-<br>duce a shared-private network to learn shared and specific knowledge. In<br>addition, we pro- pose a novel Dynamic Fusion Network (DF- Net) which<br>automatically exploit the relevance between the target domain and each domain.<br>Results show that our model outperforms exist- ing methods on multi-domain<br>dialogue, giving the state-of-the-art in the literature. Besides, with little<br>training data, we show its transfer- ability by outperforming prior best model<br>by 13.9% on average.</li><li>摘要：<br>最近的研究表明，在端到端的面向任务的对话系统中取得了巨大的成功。但是，大多数神经模型都依赖于大量训练数据，这些训练数据仅可用于一定数量的任务领域，例如导航和调度。<br>这使得对具有有限标记数据的新领域进行扩展变得很困难。但是，关于如何有效地使用所有领域的数据来改善每个领域以及未知领域的性能表现的研究相对较少。<br>为此，我们研究了可以准确使用领域知识的方法，并引入了共享－私有网络来学习共享和特定知识。<br>此外，我们提出了一种新颖的动态聚合网络（DFNet），该网络可自动利用目标领域与每个领域之间的相关性。结果表明，我们的模型优于现有的多领域对话方法，提供了文献中的最新技术。<br>此外，在训练数据很少的情况下，我们的性能比以前的最佳模型平均高13.9％，从而显示出其可移植性。</li><li>Conclusion：<br>In this paper, we propose to use a shared-private<br>model to investigate explicit modeling domain<br>knowledge for multi-domain dialog. In addition, a<br>dynamic fusion layer is proposed to dynamically<br>capture the correlation between a target domain and<br>all source domains. Experiments on two datasets<br>show the effectiveness of the proposed models. Be-<br>sides, our model can quickly adapt to a new domain<br>with little annotated data.</li><li>结论：<br>本文中，我们建议使用共享-私有模型研究多领域对话显式建模域知识。另外，提出了动态聚合以动态捕获目标域和所有源域之间的相关性。<br>在两个数据集上的实验表明了所提出模型的有效性。此外，我们的模型可以在几乎没有注释数据的情况下快速适应新领域。</li><li>summary：<ul><li>现有的神经模型大多数都依赖大量的训练数据，且只能用在一部分任务领域。</li><li>本文引入了shared-private network（共享－私有网络）来学习共享和特定的知识。</li><li>提出了新的动态聚合网络（DFNET),它可以自动探索目标域与各个领域之间的相关性。并且该模型同时可以应用在几乎没有标签数据的新领域中。</li></ul></li></ul><h4 id="二、Learning-Dialog-Policies-from-Weak-Demonstrations"><a href="#二、Learning-Dialog-Policies-from-Weak-Demonstrations" class="headerlink" title="二、Learning Dialog Policies from Weak Demonstrations"></a>二、Learning Dialog Policies from Weak Demonstrations</h4><h5 id="从弱演示中学习对话策略"><a href="#从弱演示中学习对话策略" class="headerlink" title="从弱演示中学习对话策略"></a>从弱演示中学习对话策略</h5><ul><li><p>Abstract<br>Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user’s requests. We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators. We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment. Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on outof-domain data.</p></li><li><p>深度强化学习是一种很有前途的训练对话管理器的方法，但是目前的方法难以适应多领域对话系统的大规模状态空间和动作空间。基于深度问答学习(DQfD)，一种在困难的雅达利游戏中得分很高的算法，我们利用对话数据来指导代理来成功响应用户的请求。我们对所需数据的假设越来越少，使用标记的、简化标记的、甚至是未标记的数据来训练专家演示。我们引入了强化微调学习，这是DQfD的扩展，使我们能够克服数据集和环境之间的领域差距。在一个具有挑战性的多领域对话系统框架中进行的实验验证了我们的方法，并且即使在领域外的数据上进行训练也获得了很高的成功率。</p></li><li><p>Conclusion</p><p>In this paper, we have shown that weak demonstrations can be leveraged to learn an accurate dialog manager with Deep Q-Learning from Demonstrations in a challenging multi-domain environment. We established that expert demonstrators can be trained on labeled, reduced-labeled, and unlabeled data and still guide the RL agent by means of their respective auxiliary losses. Evaluation has shown that all experts exceeded the performance of reinforcement and supervised learning baselines, and in some cases even approached the results of a hand-crafted rule-based dialog manager. Furthermore, we introduced Reinforced Finetune Learning (RoFL) a DAgger-inspired extension to DQfD which allows a pre-trained expert to adapt to an RL environment on-the-fly, bridging the domain-gap. Our experiments show that RoFL training is beneficial across different sources of demonstration data, boosting both the rate of convergence and final system performance. It even enables an expert trained on unannotated out-ofdomain data to guide an RL dialog manager in a challenging environment. In future, we want to continue to investigate the possibility of using even weaker demonstrations. Since our No Label Expert is trained on unannotated data, it would be interesting to leverage large and noisy conversational datasets drawn from message boards ormovie subtitles, and to see how RoFL training fares with such a significant domain gap between the data and the RL environment.</p></li><li><p>在本文中，我们展示了在一个具有挑战性的多领域环境中，可以利用弱演示来学习使用深度Q-Learning的精确对话管理器。我们建立了专家演示可以对标记数据、少标记数据和未标记数据进行训练，并且仍然可以通过各自的辅助损失来指导RL agent。评估表明，所有的专家都超过了增强和监督学习基线的性能，在某些情况下甚至接近了手工制作的基于规则的对话管理器的结果。此外，我们引入了增强的Finetune学习(RoFL)，这是DQfD的一种受短剑启发的扩展，它允许预先训练的专家可以动态适应RL环境，弥补了领域的差距。我们的实验表明，RoFL训练对于跨不同来源的演示数据是有益的，它提高收敛速度和最终的系统性能。它甚至可以让一个在未标注的域外数据方面受过训练的专家在一个充满挑战的环境中指导一个RL对话管理器。在将来，我们希望继续研究使用更弱的演示的可能性。由于我们的无标签专家是在未经注释的数据上训练的，所以利用从留言板或电影字幕中提取的大量嘈杂的对话数据集，并看看RoFL训练在数据和RL环境之间如此显著的领域差距下如何取得成功，这将是很有趣的。</p></li><li><p>Summary</p><ul><li>目前的方法难以适应多领域对话系统的大规模状态空间和动作空间</li><li>本文提出在复杂的多领域环境下，利用DQfD算法在弱演示中可以学习到一个精确的对话管理器。</li><li>本文建立的专家演示可以在数据标记受限的情况下达到非常好的效果，在某些情况甚至接近了基于规则的对话管理器的结果。</li><li>本文还引入了RoFL，这是DQfD的一种扩展，它允许预先训练的专家可以动态适应RL环境，弥补了领域的差距。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 顶会论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> acl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记13</title>
      <link href="/2020/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013/"/>
      <url>/2020/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B013/</url>
      
        <content type="html"><![CDATA[<h3 id="评价分类结果"><a href="#评价分类结果" class="headerlink" title="评价分类结果"></a>评价分类结果</h3><a id="more"></a><h4 id="一、分类准确度的缺陷和混淆矩阵"><a href="#一、分类准确度的缺陷和混淆矩阵" class="headerlink" title="一、分类准确度的缺陷和混淆矩阵"></a>一、分类准确度的缺陷和混淆矩阵</h4><h5 id="1、分类准确度的缺陷："><a href="#1、分类准确度的缺陷：" class="headerlink" title="1、分类准确度的缺陷："></a>1、分类准确度的缺陷：</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200905225600926.png" alt="image-20200905225600926"></p><h5 id="假设这个癌症预测系统的预测准确度（分类准确度）是99-9-，若癌症产生概率只有0-01-，我们如果人为预测所有人都健康，这样准确度会达到99-99-。这比该预测系统的准确度更高，显然这个预测系统（模型）是失败的。因此，对于这类极度偏斜（Skewed-Data）的数据，只使用分类准确度是不行的。"><a href="#假设这个癌症预测系统的预测准确度（分类准确度）是99-9-，若癌症产生概率只有0-01-，我们如果人为预测所有人都健康，这样准确度会达到99-99-。这比该预测系统的准确度更高，显然这个预测系统（模型）是失败的。因此，对于这类极度偏斜（Skewed-Data）的数据，只使用分类准确度是不行的。" class="headerlink" title="假设这个癌症预测系统的预测准确度（分类准确度）是99.9%，若癌症产生概率只有0.01%，我们如果人为预测所有人都健康，这样准确度会达到99.99%。这比该预测系统的准确度更高，显然这个预测系统（模型）是失败的。因此，对于这类极度偏斜（Skewed Data）的数据，只使用分类准确度是不行的。"></a>假设这个癌症预测系统的预测准确度（分类准确度）是99.9%，若癌症产生概率只有0.01%，我们如果人为预测所有人都健康，这样准确度会达到99.99%。这比该预测系统的准确度更高，显然这个预测系统（模型）是失败的。因此，对于这类极度偏斜（Skewed Data）的数据，只使用分类准确度是不行的。</h5></li></ul><h5 id="2、混淆矩阵："><a href="#2、混淆矩阵：" class="headerlink" title="2、混淆矩阵："></a>2、混淆矩阵：</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200905232354868.png" alt="image-20200905232354868"></p><h5 id="T表示True，F表示False，N表示Negative，P表示Positive。"><a href="#T表示True，F表示False，N表示Negative，P表示Positive。" class="headerlink" title="T表示True，F表示False，N表示Negative，P表示Positive。"></a>T表示True，F表示False，N表示Negative，P表示Positive。</h5><h5 id="具体可看https-blog-csdn-net-Orange-Spotty-Cat-article-details-80520839"><a href="#具体可看https-blog-csdn-net-Orange-Spotty-Cat-article-details-80520839" class="headerlink" title="具体可看https://blog.csdn.net/Orange_Spotty_Cat/article/details/80520839"></a>具体可看<a href="https://blog.csdn.net/Orange_Spotty_Cat/article/details/80520839" target="_blank" rel="noopener">https://blog.csdn.net/Orange_Spotty_Cat/article/details/80520839</a></h5><ul><li><h5 id="真实值是positive，模型认为是positive的数量（True-Positive-TP）"><a href="#真实值是positive，模型认为是positive的数量（True-Positive-TP）" class="headerlink" title="真实值是positive，模型认为是positive的数量（True Positive=TP）"></a>真实值是positive，模型认为是positive的数量（True Positive=TP）</h5></li><li><h5 id="真实值是positive，模型认为是negative的数量（False-Negative-FN）"><a href="#真实值是positive，模型认为是negative的数量（False-Negative-FN）" class="headerlink" title="真实值是positive，模型认为是negative的数量（False Negative=FN）"></a>真实值是positive，模型认为是negative的数量（False Negative=FN）</h5></li><li><h5 id="真实值是negative，模型认为是positive的数量（False-Positive-FP）"><a href="#真实值是negative，模型认为是positive的数量（False-Positive-FP）" class="headerlink" title="真实值是negative，模型认为是positive的数量（False Positive=FP）"></a>真实值是negative，模型认为是positive的数量（False Positive=FP）</h5></li><li><h5 id="真实值是negative，模型认为是negative的数量（True-Negative-TN）"><a href="#真实值是negative，模型认为是negative的数量（True-Negative-TN）" class="headerlink" title="真实值是negative，模型认为是negative的数量（True Negative=TN）"></a>真实值是negative，模型认为是negative的数量（True Negative=TN）</h5></li></ul><h5 id="举例说明：假如有10000个人，其中有（9978-12）人没患病，有（2-8）人患病"><a href="#举例说明：假如有10000个人，其中有（9978-12）人没患病，有（2-8）人患病" class="headerlink" title="举例说明：假如有10000个人，其中有（9978+12）人没患病，有（2+8）人患病"></a>举例说明：假如有10000个人，其中有（9978+12）人没患病，有（2+8）人患病</h5><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200905233743570.png" alt="image-20200905233743570"></li></ul></li><li><h5 id="通过上例说明精准率与召回率："><a href="#通过上例说明精准率与召回率：" class="headerlink" title="通过上例说明精准率与召回率："></a>通过上例说明精准率与召回率：</h5><ul><li><h5 id="精准率："><a href="#精准率：" class="headerlink" title="精准率："></a>精准率：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200905233949626.png" alt="image-20200905233949626"></p><h5 id="在分类问题中，我们一般关注的重点（预测的结果）是“1”而不是“0”，针对本例就是指患病这种情况。所以精准率指在所有预测为“1”的结果中，预测正确的比重是多少。在本例中指在所有预测为患病的人中，预测正确的比重是多少。"><a href="#在分类问题中，我们一般关注的重点（预测的结果）是“1”而不是“0”，针对本例就是指患病这种情况。所以精准率指在所有预测为“1”的结果中，预测正确的比重是多少。在本例中指在所有预测为患病的人中，预测正确的比重是多少。" class="headerlink" title="在分类问题中，我们一般关注的重点（预测的结果）是“1”而不是“0”，针对本例就是指患病这种情况。所以精准率指在所有预测为“1”的结果中，预测正确的比重是多少。在本例中指在所有预测为患病的人中，预测正确的比重是多少。"></a>在分类问题中，我们一般关注的重点（预测的结果）是“1”而不是“0”，针对本例就是指患病这种情况。所以精准率指在所有预测为“1”的结果中，预测正确的比重是多少。在本例中指在所有预测为患病的人中，预测正确的比重是多少。</h5></li><li><h5 id="召回率："><a href="#召回率：" class="headerlink" title="召回率："></a>召回率：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200905234614050.png" alt="image-20200905234614050"></p><h5 id="召回率指在所有真实为“1”的结果中，预测正确的比重是多少。在本例中指在所有确实患病的人中，预测正确的比重是多少。"><a href="#召回率指在所有真实为“1”的结果中，预测正确的比重是多少。在本例中指在所有确实患病的人中，预测正确的比重是多少。" class="headerlink" title="召回率指在所有真实为“1”的结果中，预测正确的比重是多少。在本例中指在所有确实患病的人中，预测正确的比重是多少。"></a>召回率指在所有真实为“1”的结果中，预测正确的比重是多少。在本例中指在所有确实患病的人中，预测正确的比重是多少。</h5></li></ul></li><li><h5 id="再回到最开始的例子中，我们预测在10000个人中，有9990个人健康，10个人患病，这样会得到下面的混淆矩阵："><a href="#再回到最开始的例子中，我们预测在10000个人中，有9990个人健康，10个人患病，这样会得到下面的混淆矩阵：" class="headerlink" title="再回到最开始的例子中，我们预测在10000个人中，有9990个人健康，10个人患病，这样会得到下面的混淆矩阵："></a>再回到最开始的例子中，我们预测在10000个人中，有9990个人健康，10个人患病，这样会得到下面的混淆矩阵：</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200905235343865.png" alt="image-20200905235343865"></p><h5 id="虽然分类准确度达到了99-9-，但是精准率与召回率却都是0，很显然这个模型是不好的。"><a href="#虽然分类准确度达到了99-9-，但是精准率与召回率却都是0，很显然这个模型是不好的。" class="headerlink" title="虽然分类准确度达到了99.9%，但是精准率与召回率却都是0，很显然这个模型是不好的。"></a>虽然分类准确度达到了99.9%，但是精准率与召回率却都是0，很显然这个模型是不好的。</h5></li></ul></li></ul><h4 id="二、F1-Score"><a href="#二、F1-Score" class="headerlink" title="二、F1 Score"></a>二、F1 Score</h4><ul><li><h5 id="关于精准率和召回率的解读要视实际情况来定："><a href="#关于精准率和召回率的解读要视实际情况来定：" class="headerlink" title="关于精准率和召回率的解读要视实际情况来定："></a>关于精准率和召回率的解读要视实际情况来定：</h5><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907203853160.png" alt="image-20200907203853160"></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907203913917.png" alt="image-20200907203913917"></li></ul></li><li><h5 id="是否有一个指标可以将二者都兼顾呢——F1-Score"><a href="#是否有一个指标可以将二者都兼顾呢——F1-Score" class="headerlink" title="是否有一个指标可以将二者都兼顾呢——F1 Score"></a>是否有一个指标可以将二者都兼顾呢——F1 Score</h5><ul><li><h5 id="F1-Score是precision和recall的调和平均值。它的特性是只有二者都比较大才能使得F1-Score比较大；其中一个很大（很小），另一个相反，都不会使得F1-Score很大。"><a href="#F1-Score是precision和recall的调和平均值。它的特性是只有二者都比较大才能使得F1-Score比较大；其中一个很大（很小），另一个相反，都不会使得F1-Score很大。" class="headerlink" title="F1 Score是precision和recall的调和平均值。它的特性是只有二者都比较大才能使得F1 Score比较大；其中一个很大（很小），另一个相反，都不会使得F1 Score很大。"></a>F1 Score是precision和recall的调和平均值。它的特性是只有二者都比较大才能使得F1 Score比较大；其中一个很大（很小），另一个相反，都不会使得F1 Score很大。</h5></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907204616387.png" alt="image-20200907204616387"></p></li></ul></li></ul><h4 id="三、Precision-Recall的平衡"><a href="#三、Precision-Recall的平衡" class="headerlink" title="三、Precision-Recall的平衡"></a>三、Precision-Recall的平衡</h4><ul><li><h5 id="在逻辑回归中"><a href="#在逻辑回归中" class="headerlink" title="在逻辑回归中"></a>在逻辑回归中</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907222636806.png" alt="image-20200907222636806"></p></li><li><h5 id="为啥决策边界要以0为界限呢？其实我们可以根据需求来调节决策边界的界限，进而改变分类结果。"><a href="#为啥决策边界要以0为界限呢？其实我们可以根据需求来调节决策边界的界限，进而改变分类结果。" class="headerlink" title="为啥决策边界要以0为界限呢？其实我们可以根据需求来调节决策边界的界限，进而改变分类结果。"></a>为啥决策边界要以0为界限呢？其实我们可以根据需求来调节决策边界的界限，进而改变分类结果。</h5></li></ul></li><li><h5 id="因此我们可以让决策边界等于threshold（阈值），让它改变分类结果，进而影响Precision与Recall的权重。"><a href="#因此我们可以让决策边界等于threshold（阈值），让它改变分类结果，进而影响Precision与Recall的权重。" class="headerlink" title="因此我们可以让决策边界等于threshold（阈值），让它改变分类结果，进而影响Precision与Recall的权重。"></a>因此我们可以让决策边界等于threshold（阈值），让它改变分类结果，进而影响Precision与Recall的权重。</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907223244946.png" alt="image-20200907223244946"></p><ul><li><h5 id="threshold-0时："><a href="#threshold-0时：" class="headerlink" title="threshold=0时："></a>threshold=0时：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907223426718.png" alt="image-20200907223426718"></p><h5 id="星星表示实际为1的样本，圆圈表示实际为0的样本。左边是预测为0的样本，右边是预测为1的样本。"><a href="#星星表示实际为1的样本，圆圈表示实际为0的样本。左边是预测为0的样本，右边是预测为1的样本。" class="headerlink" title="星星表示实际为1的样本，圆圈表示实际为0的样本。左边是预测为0的样本，右边是预测为1的样本。"></a>星星表示实际为1的样本，圆圈表示实际为0的样本。左边是预测为0的样本，右边是预测为1的样本。</h5></li><li><h5 id="threshold-gt-0时："><a href="#threshold-gt-0时：" class="headerlink" title="threshold&gt;0时："></a>threshold&gt;0时：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907223654382.png" alt="image-20200907223654382"></p></li><li><h5 id="threshold-lt-0时："><a href="#threshold-lt-0时：" class="headerlink" title="threshold&lt;0时："></a>threshold&lt;0时：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907223734566.png" alt="image-20200907223734566"></p></li></ul></li></ul></li><li><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li><h5 id="精准率与召回率互相牵制，一个升高，另一个就会降低"><a href="#精准率与召回率互相牵制，一个升高，另一个就会降低" class="headerlink" title="精准率与召回率互相牵制，一个升高，另一个就会降低"></a>精准率与召回率互相牵制，一个升高，另一个就会降低</h5></li><li><h5 id="阈值越大，精准率越高，召回率越低；阈值越小，精准率越低，召回率越高"><a href="#阈值越大，精准率越高，召回率越低；阈值越小，精准率越低，召回率越高" class="headerlink" title="阈值越大，精准率越高，召回率越低；阈值越小，精准率越低，召回率越高"></a>阈值越大，精准率越高，召回率越低；阈值越小，精准率越低，召回率越高</h5></li></ul></li></ul><h4 id="四、P-R曲线"><a href="#四、P-R曲线" class="headerlink" title="四、P-R曲线"></a>四、P-R曲线</h4><ul><li><h5 id="P-R曲线顾名思义横坐标是精准率，纵坐标是召回率"><a href="#P-R曲线顾名思义横坐标是精准率，纵坐标是召回率" class="headerlink" title="P-R曲线顾名思义横坐标是精准率，纵坐标是召回率"></a>P-R曲线顾名思义横坐标是精准率，纵坐标是召回率</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907233249933.png" alt="image-20200907233249933"></p><ul><li><h5 id="途中曲线开始急剧下降的点，可能就是精准率和召回率平衡位置的点"><a href="#途中曲线开始急剧下降的点，可能就是精准率和召回率平衡位置的点" class="headerlink" title="途中曲线开始急剧下降的点，可能就是精准率和召回率平衡位置的点"></a>途中曲线开始急剧下降的点，可能就是精准率和召回率平衡位置的点</h5></li></ul></li><li><h5 id="不同的模型对应的不同的-Precision-Recall-曲线"><a href="#不同的模型对应的不同的-Precision-Recall-曲线" class="headerlink" title="不同的模型对应的不同的 Precision - Recall 曲线"></a>不同的模型对应的不同的 Precision - Recall 曲线</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200907233440669.png" alt="image-20200907233440669"></p><ul><li><h5 id="外层曲线对应的模型更优（外层曲线的P值和R值都比里层曲线的大）；或者称与坐标轴一起包围的面积越大者越优"><a href="#外层曲线对应的模型更优（外层曲线的P值和R值都比里层曲线的大）；或者称与坐标轴一起包围的面积越大者越优" class="headerlink" title="外层曲线对应的模型更优（外层曲线的P值和R值都比里层曲线的大）；或者称与坐标轴一起包围的面积越大者越优"></a>外层曲线对应的模型更优（外层曲线的P值和R值都比里层曲线的大）；或者称与坐标轴一起包围的面积越大者越优</h5></li><li><h5 id="P-R-曲线也可以作为选择算法、模型、超参数的指标；但一般不适用此曲线，而是使用-ROC-曲线"><a href="#P-R-曲线也可以作为选择算法、模型、超参数的指标；但一般不适用此曲线，而是使用-ROC-曲线" class="headerlink" title="P - R 曲线也可以作为选择算法、模型、超参数的指标；但一般不适用此曲线，而是使用 ROC 曲线"></a>P - R 曲线也可以作为选择算法、模型、超参数的指标；但一般不适用此曲线，而是使用 ROC 曲线</h5></li></ul></li></ul><h4 id="五、ROC曲线"><a href="#五、ROC曲线" class="headerlink" title="五、ROC曲线"></a>五、ROC曲线</h4><ul><li><h5 id="ROC曲线描述TPR和FPR之间的关系"><a href="#ROC曲线描述TPR和FPR之间的关系" class="headerlink" title="ROC曲线描述TPR和FPR之间的关系"></a>ROC曲线描述TPR和FPR之间的关系</h5><ul><li><h5 id="TPR："><a href="#TPR：" class="headerlink" title="TPR："></a>TPR：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200913172134092.png" alt="image-20200913172134092"></p></li><li><h5 id="FPR"><a href="#FPR" class="headerlink" title="FPR:"></a>FPR:</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200913172216856.png" alt="image-20200913172216856"></p></li><li><h5 id="TPR与FPR的关系"><a href="#TPR与FPR的关系" class="headerlink" title="TPR与FPR的关系"></a>TPR与FPR的关系</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200913180416522.png" alt="image-20200913180416522"></p><ul><li><h5 id="随着阈值-threshold-的增大，FPR-和-TPR-都逐渐减小；"><a href="#随着阈值-threshold-的增大，FPR-和-TPR-都逐渐减小；" class="headerlink" title="随着阈值 threshold 的增大，FPR 和 TPR 都逐渐减小；"></a>随着阈值 threshold 的增大，FPR 和 TPR 都逐渐减小；</h5></li><li><h5 id="FPR-和-TPR-称正相关关系，FPR-越高，TPR-相应的也越高；"><a href="#FPR-和-TPR-称正相关关系，FPR-越高，TPR-相应的也越高；" class="headerlink" title="FPR 和 TPR 称正相关关系，FPR 越高，TPR 相应的也越高；"></a>FPR 和 TPR 称正相关关系，FPR 越高，TPR 相应的也越高；</h5></li></ul></li></ul></li><li><h5 id="与P-R曲线的区别"><a href="#与P-R曲线的区别" class="headerlink" title="与P-R曲线的区别"></a>与P-R曲线的区别</h5><ul><li><h5 id="P-R曲线：应用于判定由极度有偏数据所训练的模型的优劣"><a href="#P-R曲线：应用于判定由极度有偏数据所训练的模型的优劣" class="headerlink" title="P-R曲线：应用于判定由极度有偏数据所训练的模型的优劣"></a>P-R曲线：应用于判定由极度有偏数据所训练的模型的优劣</h5></li><li><h5 id="ROC曲线：应用于比较两个模型的优劣"><a href="#ROC曲线：应用于比较两个模型的优劣" class="headerlink" title="ROC曲线：应用于比较两个模型的优劣"></a>ROC曲线：应用于比较两个模型的优劣</h5><ul><li><h5 id="可以是同样算法不同超参数得到的不同模型，也可以是不同算法得到的不同模型"><a href="#可以是同样算法不同超参数得到的不同模型，也可以是不同算法得到的不同模型" class="headerlink" title="可以是同样算法不同超参数得到的不同模型，也可以是不同算法得到的不同模型"></a>可以是同样算法不同超参数得到的不同模型，也可以是不同算法得到的不同模型</h5></li></ul></li></ul></li><li><h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h5><ul><li><h5 id="ROC曲线与图形边界围成的面积，作为衡量模型优劣的标准，面积越大，模型越优"><a href="#ROC曲线与图形边界围成的面积，作为衡量模型优劣的标准，面积越大，模型越优" class="headerlink" title="ROC曲线与图形边界围成的面积，作为衡量模型优劣的标准，面积越大，模型越优"></a>ROC曲线与图形边界围成的面积，作为衡量模型优劣的标准，面积越大，模型越优</h5></li><li><h5 id="计算ROC曲线与坐标轴围成的面积：称ROC的auc；"><a href="#计算ROC曲线与坐标轴围成的面积：称ROC的auc；" class="headerlink" title="计算ROC曲线与坐标轴围成的面积：称ROC的auc；"></a>计算ROC曲线与坐标轴围成的面积：称ROC的auc；</h5></li><li><h5 id="面积越大，模型越优："><a href="#面积越大，模型越优：" class="headerlink" title="面积越大，模型越优："></a>面积越大，模型越优：</h5></li></ul></li></ul><h4 id="六、-多分类问题中的混淆矩阵"><a href="#六、-多分类问题中的混淆矩阵" class="headerlink" title="六、 多分类问题中的混淆矩阵"></a>六、 多分类问题中的混淆矩阵</h4><h5 id="这块儿内容直接看‘08-Confusion-Matrix-in-Multiclass-Classification-ipynb’"><a href="#这块儿内容直接看‘08-Confusion-Matrix-in-Multiclass-Classification-ipynb’" class="headerlink" title="这块儿内容直接看‘08-Confusion-Matrix-in-Multiclass-Classification.ipynb’"></a>这块儿内容直接看‘08-Confusion-Matrix-in-Multiclass-Classification.ipynb’</h5>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 混淆矩阵 </tag>
            
            <tag> F1 SCORE </tag>
            
            <tag> 精准率与召回率 </tag>
            
            <tag> PR曲线与ROC曲线 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记12</title>
      <link href="/2020/08/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012/"/>
      <url>/2020/08/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012/</url>
      
        <content type="html"><![CDATA[<h3 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h3><a id="more"></a><h4 id="一、什么是逻辑回归"><a href="#一、什么是逻辑回归" class="headerlink" title="一、什么是逻辑回归"></a>一、什么是逻辑回归</h4><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200827181823137.png" alt="image-20200827181823137"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200827181942648.png" alt="image-20200827181942648"></p><ul><li><h5 id="线性回归的预测值是我们本身就需要的指标，比如房价预测中，y就是房价。但是逻辑回归的预测值是概率值，比如预测未来一周某地下雨的可能性。这里可以将逻辑回归当作回归算法，因为概率值也属于连续值。但是通常逻辑回归被当作是分类算法，比如将下雨的可能性（回归）转换为是否下雨（分类），所以需要进一步的转换，将概率p-gt-0-5的转换为1，p-lt-0-5的转换为0。"><a href="#线性回归的预测值是我们本身就需要的指标，比如房价预测中，y就是房价。但是逻辑回归的预测值是概率值，比如预测未来一周某地下雨的可能性。这里可以将逻辑回归当作回归算法，因为概率值也属于连续值。但是通常逻辑回归被当作是分类算法，比如将下雨的可能性（回归）转换为是否下雨（分类），所以需要进一步的转换，将概率p-gt-0-5的转换为1，p-lt-0-5的转换为0。" class="headerlink" title="线性回归的预测值是我们本身就需要的指标，比如房价预测中，y就是房价。但是逻辑回归的预测值是概率值，比如预测未来一周某地下雨的可能性。这里可以将逻辑回归当作回归算法，因为概率值也属于连续值。但是通常逻辑回归被当作是分类算法，比如将下雨的可能性（回归）转换为是否下雨（分类），所以需要进一步的转换，将概率p&gt;0.5的转换为1，p&lt;0.5的转换为0。"></a>线性回归的预测值是我们本身就需要的指标，比如房价预测中，y就是房价。但是逻辑回归的预测值是概率值，比如预测未来一周某地下雨的可能性。这里可以将逻辑回归当作回归算法，因为概率值也属于连续值。但是通常逻辑回归被当作是分类算法，比如将下雨的可能性（回归）转换为是否下雨（分类），所以需要进一步的转换，将概率p&gt;0.5的转换为1，p&lt;0.5的转换为0。</h5></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200827183353872.png" alt="image-20200827183353872"></p><h5 id="为了解决线性回归不适合处理分类的问题，我们在线性回归的基础上提出了逻辑回归。但是线性回归得出的预测值的值域往往是-inf-inf-，所以我们需要一个-sigma-函数将线性回归的结果映射到-0-1-上，使它符合概率的值域。那么，哪个函数能完成这一工作呢？那就是Sigmoid函数了。"><a href="#为了解决线性回归不适合处理分类的问题，我们在线性回归的基础上提出了逻辑回归。但是线性回归得出的预测值的值域往往是-inf-inf-，所以我们需要一个-sigma-函数将线性回归的结果映射到-0-1-上，使它符合概率的值域。那么，哪个函数能完成这一工作呢？那就是Sigmoid函数了。" class="headerlink" title="为了解决线性回归不适合处理分类的问题，我们在线性回归的基础上提出了逻辑回归。但是线性回归得出的预测值的值域往往是[-inf , +inf]，所以我们需要一个$\sigma$函数将线性回归的结果映射到(0 , 1)上，使它符合概率的值域。那么，哪个函数能完成这一工作呢？那就是Sigmoid函数了。"></a>为了解决线性回归不适合处理分类的问题，我们在线性回归的基础上提出了逻辑回归。但是线性回归得出的预测值的值域往往是[-inf , +inf]，所以我们需要一个$\sigma$函数将线性回归的结果映射到(0 , 1)上，使它符合概率的值域。那么，哪个函数能完成这一工作呢？那就是Sigmoid函数了。</h5></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200827184535801.png" alt="image-20200827184535801"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200827184642354.png" alt="image-20200827184642354"></p><h5 id="因此下面就是逻辑回归的全貌："><a href="#因此下面就是逻辑回归的全貌：" class="headerlink" title="因此下面就是逻辑回归的全貌："></a>因此下面就是逻辑回归的全貌：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200827184806464.png" alt="image-20200827184806464"></p></li></ul></li></ul><h4 id="二、逻辑回归的损失函数（cost指损失函数）"><a href="#二、逻辑回归的损失函数（cost指损失函数）" class="headerlink" title="二、逻辑回归的损失函数（cost指损失函数）"></a>二、逻辑回归的损失函数（cost指损失函数）</h4><ul><li><h5 id="由于逻辑回归的标签值（真实值）是二分类，所以我们不能采用线性回归那样的损失函数，因此我们采用以下方案"><a href="#由于逻辑回归的标签值（真实值）是二分类，所以我们不能采用线性回归那样的损失函数，因此我们采用以下方案" class="headerlink" title="由于逻辑回归的标签值（真实值）是二分类，所以我们不能采用线性回归那样的损失函数，因此我们采用以下方案"></a>由于逻辑回归的标签值（真实值）是二分类，所以我们不能采用线性回归那样的损失函数，因此我们采用以下方案</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200827185300946.png" alt="image-20200827185300946"></p><h5 id="第一个图的解释："><a href="#第一个图的解释：" class="headerlink" title="第一个图的解释："></a>第一个图的解释：</h5><ul><li><h5 id="如果真实值y等于1，但是预测得出的概率p很小（小于0-5），使得预测出的分类-y-hat-等于0。所以预测出现了错误，自然导致损失函数的值非常大（预测出的p越小，则cost越大）。"><a href="#如果真实值y等于1，但是预测得出的概率p很小（小于0-5），使得预测出的分类-y-hat-等于0。所以预测出现了错误，自然导致损失函数的值非常大（预测出的p越小，则cost越大）。" class="headerlink" title="如果真实值y等于1，但是预测得出的概率p很小（小于0.5），使得预测出的分类$y^{hat}$等于0。所以预测出现了错误，自然导致损失函数的值非常大（预测出的p越小，则cost越大）。"></a>如果真实值y等于1，但是预测得出的概率p很小（小于0.5），使得预测出的分类$y^{hat}$等于0。所以预测出现了错误，自然导致损失函数的值非常大（预测出的p越小，则cost越大）。</h5></li><li><h5 id="如果真实值y等于0，但是预测得出的概率p很大（大于0-5），使得预测出的分类-y-hat-等于1。所以预测出现了错误，自然导致损失函数的值非常大（预测出的p越大，则cost越大）。"><a href="#如果真实值y等于0，但是预测得出的概率p很大（大于0-5），使得预测出的分类-y-hat-等于1。所以预测出现了错误，自然导致损失函数的值非常大（预测出的p越大，则cost越大）。" class="headerlink" title="如果真实值y等于0，但是预测得出的概率p很大（大于0.5），使得预测出的分类$y^{hat}$等于1。所以预测出现了错误，自然导致损失函数的值非常大（预测出的p越大，则cost越大）。"></a>如果真实值y等于0，但是预测得出的概率p很大（大于0.5），使得预测出的分类$y^{hat}$等于1。所以预测出现了错误，自然导致损失函数的值非常大（预测出的p越大，则cost越大）。</h5></li></ul><h5 id="那么，为什么可以采用第二个图的分段函数来实现这一功能呢？（下图的x就是p）"><a href="#那么，为什么可以采用第二个图的分段函数来实现这一功能呢？（下图的x就是p）" class="headerlink" title="那么，为什么可以采用第二个图的分段函数来实现这一功能呢？（下图的x就是p）"></a>那么，为什么可以采用第二个图的分段函数来实现这一功能呢？（下图的x就是p）</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200827190453596.png" alt="image-20200827190453596"></p><h5 id="很显然，这个分段函数的图像（横坐标是概率值，纵坐标是损失函数值，交点的横坐标是0-5）能很好的体现sigmoid函数的特性。"><a href="#很显然，这个分段函数的图像（横坐标是概率值，纵坐标是损失函数值，交点的横坐标是0-5）能很好的体现sigmoid函数的特性。" class="headerlink" title="很显然，这个分段函数的图像（横坐标是概率值，纵坐标是损失函数值，交点的横坐标是0.5）能很好的体现sigmoid函数的特性。"></a>很显然，这个分段函数的图像（横坐标是概率值，纵坐标是损失函数值，交点的横坐标是0.5）能很好的体现sigmoid函数的特性。</h5><h5 id="但是分段函数不太方便计算，所以我们用下面这个函数来表示分段函数："><a href="#但是分段函数不太方便计算，所以我们用下面这个函数来表示分段函数：" class="headerlink" title="但是分段函数不太方便计算，所以我们用下面这个函数来表示分段函数："></a>但是分段函数不太方便计算，所以我们用下面这个函数来表示分段函数：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200828002537051.png" alt="image-20200828002537051"></p><h5 id="将p的表达式代入进一步推导："><a href="#将p的表达式代入进一步推导：" class="headerlink" title="将p的表达式代入进一步推导："></a>将p的表达式代入进一步推导：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200828002717063.png" alt="image-20200828002717063"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200828002755119.png" alt="image-20200828002755119"></p></li></ul><h4 id="三、逻辑回归损失函数的梯度"><a href="#三、逻辑回归损失函数的梯度" class="headerlink" title="三、逻辑回归损失函数的梯度"></a>三、逻辑回归损失函数的梯度</h4><ul><li><h5 id="具体求解梯度过程请参考《神经网络与深度学习-邱锡鹏》的3-2-1参数学习部分的推导。"><a href="#具体求解梯度过程请参考《神经网络与深度学习-邱锡鹏》的3-2-1参数学习部分的推导。" class="headerlink" title="具体求解梯度过程请参考《神经网络与深度学习-邱锡鹏》的3.2.1参数学习部分的推导。"></a>具体求解梯度过程请参考《神经网络与深度学习-邱锡鹏》的3.2.1参数学习部分的推导。</h5><ul><li><h5 id="最终推导得出："><a href="#最终推导得出：" class="headerlink" title="最终推导得出："></a>最终推导得出：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200902015231936.png" alt="image-20200902015231936"></p><h5 id="因为是对-theta-j-求偏导，所以在-sigma-X-b-i-theta-的基础上还要有-theta-j-前面的系数-X-j-i-。"><a href="#因为是对-theta-j-求偏导，所以在-sigma-X-b-i-theta-的基础上还要有-theta-j-前面的系数-X-j-i-。" class="headerlink" title="因为是对$\theta_j$求偏导，所以在$\sigma{(X_{b}^{(i)}\theta)}$的基础上还要有$\theta_j$前面的系数$X_{j}^{(i)}$。"></a>因为是对$\theta_j$求偏导，所以在$\sigma{(X_{b}^{(i)}\theta)}$的基础上还要有$\theta_j$前面的系数$X_{j}^{(i)}$。</h5></li><li><h5 id="J-theta-的梯度为："><a href="#J-theta-的梯度为：" class="headerlink" title="$J{(\theta)}$的梯度为："></a>$J{(\theta)}$的梯度为：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200902020905850.png" alt="image-20200902020905850"></p></li></ul></li><li><h5 id="对表达式进行向量化（参考线性回归）"><a href="#对表达式进行向量化（参考线性回归）" class="headerlink" title="对表达式进行向量化（参考线性回归）"></a>对表达式进行向量化（参考线性回归）</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200902021018913.png" alt="image-20200902021018913"></p><h5 id="参考上面，逻辑回归的梯度向量化为："><a href="#参考上面，逻辑回归的梯度向量化为：" class="headerlink" title="参考上面，逻辑回归的梯度向量化为："></a>参考上面，逻辑回归的梯度向量化为：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200902021114185.png" alt="image-20200902021114185"></p></li></ul></li></ul><h4 id="四、决策边界"><a href="#四、决策边界" class="headerlink" title="四、决策边界"></a>四、决策边界</h4><ul><li><h5 id="决策边界就是将结果通过类别按区域划分"><a href="#决策边界就是将结果通过类别按区域划分" class="headerlink" title="决策边界就是将结果通过类别按区域划分"></a>决策边界就是将结果通过类别按区域划分</h5></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200903221221036.png" alt="image-20200903221221036"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200903221241679.png" alt="image-20200903221241679"></p></li><li><h5 id="逻辑回归决策边界示例"><a href="#逻辑回归决策边界示例" class="headerlink" title="逻辑回归决策边界示例"></a>逻辑回归决策边界示例</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200903221447790.png" alt="image-20200903221447790"></p></li><li><h5 id="KNN决策边界示例"><a href="#KNN决策边界示例" class="headerlink" title="KNN决策边界示例"></a>KNN决策边界示例</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200903221529302.png" alt="image-20200903221529302"></p></li><li><h5 id="因为逻辑回归是基于线性回归的，而且现实中的数据的决策边界几乎不可能是直线，所以我们可以像线性回归那样给逻辑回归添加多项式项（原理一样），进而让逻辑回归可以应对非线性数据。"><a href="#因为逻辑回归是基于线性回归的，而且现实中的数据的决策边界几乎不可能是直线，所以我们可以像线性回归那样给逻辑回归添加多项式项（原理一样），进而让逻辑回归可以应对非线性数据。" class="headerlink" title="因为逻辑回归是基于线性回归的，而且现实中的数据的决策边界几乎不可能是直线，所以我们可以像线性回归那样给逻辑回归添加多项式项（原理一样），进而让逻辑回归可以应对非线性数据。"></a>因为逻辑回归是基于线性回归的，而且现实中的数据的决策边界几乎不可能是直线，所以我们可以像线性回归那样给逻辑回归添加多项式项（原理一样），进而让逻辑回归可以应对非线性数据。</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200903224323601.png" alt="image-20200903224323601"></p></li></ul><h4 id="五、逻辑回归的正则化"><a href="#五、逻辑回归的正则化" class="headerlink" title="五、逻辑回归的正则化"></a>五、逻辑回归的正则化</h4><ul><li><h5 id="左边是线性回归的正则化，右边是逻辑回归的正则化（因为一般情况下建议必须使用正则化，所以将权重超参数放到了损失函数上）"><a href="#左边是线性回归的正则化，右边是逻辑回归的正则化（因为一般情况下建议必须使用正则化，所以将权重超参数放到了损失函数上）" class="headerlink" title="左边是线性回归的正则化，右边是逻辑回归的正则化（因为一般情况下建议必须使用正则化，所以将权重超参数放到了损失函数上）"></a>左边是线性回归的正则化，右边是逻辑回归的正则化（因为一般情况下建议必须使用正则化，所以将权重超参数放到了损失函数上）</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200903232842954.png" alt="image-20200903232842954"></p></li></ul><h4 id="六、OvR（或OvA）与OvO-或MvM"><a href="#六、OvR（或OvA）与OvO-或MvM" class="headerlink" title="六、OvR（或OvA）与OvO(或MvM)"></a>六、OvR（或OvA）与OvO(或MvM)</h4><h5 id="都是将多分类转换为二分类的方法"><a href="#都是将多分类转换为二分类的方法" class="headerlink" title="都是将多分类转换为二分类的方法"></a>都是将多分类转换为二分类的方法</h5><h5 id="1、OvR："><a href="#1、OvR：" class="headerlink" title="1、OvR："></a>1、OvR：</h5><ul><li><h5 id="每次选取一个类别（ONE），然后将其他类别混合起来当作一个新的类别-REST"><a href="#每次选取一个类别（ONE），然后将其他类别混合起来当作一个新的类别-REST" class="headerlink" title="每次选取一个类别（ONE），然后将其他类别混合起来当作一个新的类别(REST)"></a>每次选取一个类别（ONE），然后将其他类别混合起来当作一个新的类别(REST)</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200904003819162.png" alt="image-20200904003819162"></p></li></ul><h5 id="2、OvO："><a href="#2、OvO：" class="headerlink" title="2、OvO："></a>2、OvO：</h5><ul><li><h5 id="每次从多个类别中只选出两个类别进行分类"><a href="#每次从多个类别中只选出两个类别进行分类" class="headerlink" title="每次从多个类别中只选出两个类别进行分类"></a>每次从多个类别中只选出两个类别进行分类</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200904003951785.png" alt="image-20200904003951785"></p><h5 id="使用OvO准确率更高，但是更耗时间"><a href="#使用OvO准确率更高，但是更耗时间" class="headerlink" title="使用OvO准确率更高，但是更耗时间"></a>使用OvO准确率更高，但是更耗时间</h5></li></ul><h4 id="七、逻辑回归的总结与思考"><a href="#七、逻辑回归的总结与思考" class="headerlink" title="七、逻辑回归的总结与思考"></a>七、逻辑回归的总结与思考</h4><p><a href="https://www.cnblogs.com/lijingblog/p/11043849.html" target="_blank" rel="noopener">https://www.cnblogs.com/lijingblog/p/11043849.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
            <tag> OvR（或OvA）与OvO(或MvM) </tag>
            
            <tag> Sigmoid函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记11</title>
      <link href="/2020/08/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011/"/>
      <url>/2020/08/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011/</url>
      
        <content type="html"><![CDATA[<h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><a id="more"></a><h4 id="一、什么是多项式回归"><a href="#一、什么是多项式回归" class="headerlink" title="一、什么是多项式回归"></a>一、什么是多项式回归</h4><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200815221650806.png" alt="image-20200815221650806"></p><h5 id="以上图来说明-该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的-ax-2-，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归"><a href="#以上图来说明-该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的-ax-2-，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归" class="headerlink" title="以上图来说明:该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的$ax^2$，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归"></a>以上图来说明:该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的$ax^2$，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归</h5><h5 id="本质上相当于对原数据集进行升维来更好的拟合高维数据"><a href="#本质上相当于对原数据集进行升维来更好的拟合高维数据" class="headerlink" title="本质上相当于对原数据集进行升维来更好的拟合高维数据"></a>本质上相当于对原数据集进行升维来更好的拟合高维数据</h5></li></ul><h4 id="二，关于sklearn中的PolynomialFeatures"><a href="#二，关于sklearn中的PolynomialFeatures" class="headerlink" title="二，关于sklearn中的PolynomialFeatures"></a>二，关于sklearn中的PolynomialFeatures</h4><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816004625821.png" alt="image-20200816004625821"></p><h5 id="设degree-i，则它会生成所有小于等于i的多项式特征，构成新的特征集"><a href="#设degree-i，则它会生成所有小于等于i的多项式特征，构成新的特征集" class="headerlink" title="设degree=i，则它会生成所有小于等于i的多项式特征，构成新的特征集"></a>设degree=i，则它会生成所有小于等于i的多项式特征，构成新的特征集</h5></li></ul><h4 id="三、Pipeline"><a href="#三、Pipeline" class="headerlink" title="三、Pipeline"></a>三、Pipeline</h4><ul><li><h5 id="拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步"><a href="#拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步" class="headerlink" title="拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步"></a>拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步</h5></li><li><h5 id="具体看jupyter中的代码"><a href="#具体看jupyter中的代码" class="headerlink" title="具体看jupyter中的代码"></a>具体看jupyter中的代码</h5></li></ul><h4 id="四、过拟合和欠拟合，以及分离数据集"><a href="#四、过拟合和欠拟合，以及分离数据集" class="headerlink" title="四、过拟合和欠拟合，以及分离数据集"></a>四、过拟合和欠拟合，以及分离数据集</h4><h4 id="1-泛化能力"><a href="#1-泛化能力" class="headerlink" title="1.泛化能力"></a>1.泛化能力</h4><ul><li><h5 id="泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱"><a href="#泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱" class="headerlink" title="泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱"></a>泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱</h5></li></ul><h4 id="2-过拟合"><a href="#2-过拟合" class="headerlink" title="2.过拟合"></a>2.过拟合</h4><ul><li><h5 id="就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测-区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。"><a href="#就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测-区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。" class="headerlink" title="就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。"></a>就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。</h5></li></ul><h4 id="3-欠拟合"><a href="#3-欠拟合" class="headerlink" title="3.欠拟合"></a>3.欠拟合</h4><ul><li><h5 id="测试样本的特性没有学到-模型不能完整的表述数据关系-，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差"><a href="#测试样本的特性没有学到-模型不能完整的表述数据关系-，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差" class="headerlink" title="测试样本的特性没有学到(模型不能完整的表述数据关系)，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差"></a>测试样本的特性没有学到(模型不能完整的表述数据关系)，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差</h5></li></ul><h4 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h4><ul><li><h5 id="总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行"><a href="#总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行" class="headerlink" title="总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行"></a>总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行</h5></li><li><p><strong>那么如何衡量模型的泛化能力强弱呢？那就是采用分离数据集的方法来衡量。这就是分离数据集更深层的意义</strong></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816210229364.png" alt="image-20200816210229364"></p></li></ul><h4 id="五、学习曲线（可视化查看模型的欠拟合、过拟合）"><a href="#五、学习曲线（可视化查看模型的欠拟合、过拟合）" class="headerlink" title="五、学习曲线（可视化查看模型的欠拟合、过拟合）"></a>五、学习曲线（可视化查看模型的欠拟合、过拟合）</h4><ul><li><h5 id="定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力"><a href="#定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力" class="headerlink" title="定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力"></a>定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力</h5></li><li><h5 id="学习曲线的解释："><a href="#学习曲线的解释：" class="headerlink" title="学习曲线的解释："></a>学习曲线的解释：</h5><ul><li><h5 id="随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；"><a href="#随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；" class="headerlink" title="随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；"></a>随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；</h5></li><li><h5 id="随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失"><a href="#随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失" class="headerlink" title="随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失"></a>随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失</h5></li></ul></li><li><h5 id="欠拟合，最佳拟合，过拟合的对比"><a href="#欠拟合，最佳拟合，过拟合的对比" class="headerlink" title="欠拟合，最佳拟合，过拟合的对比"></a>欠拟合，最佳拟合，过拟合的对比</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816214627319.png" alt="image-20200816214627319"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816214657728.png" alt="image-20200816214657728"></p></li></ul><h4 id="六、验证数据集与交叉验证"><a href="#六、验证数据集与交叉验证" class="headerlink" title="六、验证数据集与交叉验证"></a>六、验证数据集与交叉验证</h4><h5 id="1、验证数据集："><a href="#1、验证数据集：" class="headerlink" title="1、验证数据集："></a>1、验证数据集：</h5><ul><li><h5 id="由于train-test-split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集"><a href="#由于train-test-split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集" class="headerlink" title="由于train_test_split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集"></a>由于train_test_split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823165520156.png" alt="image-20200823165520156"></p><h5 id="由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集"><a href="#由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集" class="headerlink" title="由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集"></a>由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集</h5></li></ul></li><li><h5 id="但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。"><a href="#但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。" class="headerlink" title="但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。"></a>但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。</h5></li></ul><h5 id="2、交叉验证："><a href="#2、交叉验证：" class="headerlink" title="2、交叉验证："></a>2、交叉验证：</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823170347284.png" alt="image-20200823170347284"></p><ul><li><h5 id="把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。"><a href="#把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。" class="headerlink" title="把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。"></a>把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。</h5></li><li><h5 id="使用交叉验证进行调参优化得到的score可能会比train-test-split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据"><a href="#使用交叉验证进行调参优化得到的score可能会比train-test-split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据" class="headerlink" title="使用交叉验证进行调参优化得到的score可能会比train_test_split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据"></a>使用交叉验证进行调参优化得到的score可能会比train_test_split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据</h5></li></ul></li><li><h5 id="交叉验证总结："><a href="#交叉验证总结：" class="headerlink" title="交叉验证总结："></a>交叉验证总结：</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823182409604.png" alt="image-20200823182409604"></p><h5 id="但是这样训练出的模型是可信赖的"><a href="#但是这样训练出的模型是可信赖的" class="headerlink" title="但是这样训练出的模型是可信赖的"></a>但是这样训练出的模型是可信赖的</h5></li><li><h5 id="假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据"><a href="#假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据" class="headerlink" title="假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据"></a>假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823182632636.png" alt="image-20200823182632636"></p></li></ul></li></ul><h4 id="七、偏方差权衡（Bias-Variance-Trade-off）"><a href="#七、偏方差权衡（Bias-Variance-Trade-off）" class="headerlink" title="七、偏方差权衡（Bias Variance Trade off）"></a>七、偏方差权衡（Bias Variance Trade off）</h4><h5 id="1、偏差与方差："><a href="#1、偏差与方差：" class="headerlink" title="1、偏差与方差："></a>1、偏差与方差：</h5><h5 id="可以参考："><a href="#可以参考：" class="headerlink" title="可以参考："></a>可以参考：</h5><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">作者：「已注销」</span><br><span class="line">链接：https://www.zhihu.com/question/20448464/answer/765401873</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure><ul><li><h5 id="偏差："><a href="#偏差：" class="headerlink" title="偏差："></a>偏差：</h5><ul><li><h5 id="描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。"><a href="#描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。" class="headerlink" title="描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。"></a>描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。</h5></li></ul></li><li><h5 id="方差："><a href="#方差：" class="headerlink" title="方差："></a>方差：</h5><ul><li><h5 id="描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。"><a href="#描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。" class="headerlink" title="描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。"></a>描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。</h5></li></ul></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823184540816.png" alt="image-20200823184540816"></p><h5 id="2、模型误差："><a href="#2、模型误差：" class="headerlink" title="2、模型误差："></a>2、模型误差：</h5><ul><li><h5 id="模型误差-偏差（Bias）-方差（Variance）-不可避免的误差（如噪声）"><a href="#模型误差-偏差（Bias）-方差（Variance）-不可避免的误差（如噪声）" class="headerlink" title="模型误差 = 偏差（Bias）+ 方差（Variance）+ 不可避免的误差（如噪声）"></a>模型误差 = 偏差（Bias）+ 方差（Variance）+ 不可避免的误差（如噪声）</h5></li></ul><h5 id="3、产生偏差和方差的主要原因"><a href="#3、产生偏差和方差的主要原因" class="headerlink" title="3、产生偏差和方差的主要原因"></a>3、产生偏差和方差的主要原因</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823185218727.png" alt="image-20200823185218727"></p></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823185258191.png" alt="image-20200823185258191"></p></li></ul><h5 id="4、模型与偏差和方差："><a href="#4、模型与偏差和方差：" class="headerlink" title="4、模型与偏差和方差："></a>4、模型与偏差和方差：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823221542723.png" alt="image-20200823221542723"></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">一、为什么knn的k越小，模型越复杂（复杂指的就是过拟合）？：</span><br><span class="line">假设有一个男女性别的数据集，k=4,。在训练数据集中的，有一个新样本s（s是男），通过knn得出 男：女=3:1，意思是有75%的概率认为s是男，25%的概率认为s是女。但是为了追求更高的准确度（极端一点），让k=1，这样s的属性就会由一个离它最近的样本决定（假设该训练数据集离s最近的一个样本是男），这样我们就有100%的概率认为s为男。然而在测试数据集中，若有一个新样本j（j是男），但是离j最最近的样本是女，所以这个模型预测就出现了错误。这就是为了追求测试数据集的准确度而不具有泛化性（普遍性）导致了过拟合</span><br><span class="line"></span><br><span class="line">二、为什么knn是高方差算法？：</span><br><span class="line">可以参考：https://www.cnblogs.com/solong1989/p/9603818.html</span><br><span class="line"></span><br><span class="line">三、过拟合和方差：</span><br><span class="line">https://blog.csdn.net/liweibin1994/article/details/76859743</span><br><span class="line">假设对训练数据集过拟合，那么测试数据集（或验证数据集）上的样本点有的预测准确，有的预测不准确，这样造成预测结果之前离散程度很大，所以过拟合就是高方差</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823191041828.png" alt="image-20200823191041828"></p><h5 id="5、偏差与方差的关系："><a href="#5、偏差与方差的关系：" class="headerlink" title="5、偏差与方差的关系："></a>5、偏差与方差的关系：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823191117875.png" alt="image-20200823191117875"></p><h5 id="6、机器学习-算法层面-的主要挑战"><a href="#6、机器学习-算法层面-的主要挑战" class="headerlink" title="6、机器学习(算法层面)的主要挑战"></a>6、机器学习(算法层面)的主要挑战</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823232503791.png" alt="image-20200823232503791"></p><h4 id="八、模型正则化（Regularization）"><a href="#八、模型正则化（Regularization）" class="headerlink" title="八、模型正则化（Regularization）"></a>八、模型正则化（Regularization）</h4><ul><li><h5 id="模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。"><a href="#模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。" class="headerlink" title="模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。"></a>模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826102708414.png" alt="image-20200826102708414"></p><h5 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h5><ul><li><h5 id="要想使损失函数尽可能小，就必须让-theta-尽可能小"><a href="#要想使损失函数尽可能小，就必须让-theta-尽可能小" class="headerlink" title="要想使损失函数尽可能小，就必须让$\theta$尽可能小"></a>要想使损失函数尽可能小，就必须让$\theta$尽可能小</h5></li><li><h5 id="损失函数后面的正则化项求和是从1到n的，这意味着不需要将-theta-0-加入，因为-theta-0-是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）"><a href="#损失函数后面的正则化项求和是从1到n的，这意味着不需要将-theta-0-加入，因为-theta-0-是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）" class="headerlink" title="损失函数后面的正则化项求和是从1到n的，这意味着不需要将$\theta_0$加入，因为$\theta_0$是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）"></a>损失函数后面的正则化项求和是从1到n的，这意味着不需要将$\theta_0$加入，因为$\theta_0$是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）</h5></li><li><h5 id="第二项的1-2是为了求导好约分，加不加都行"><a href="#第二项的1-2是为了求导好约分，加不加都行" class="headerlink" title="第二项的1/2是为了求导好约分，加不加都行"></a>第二项的1/2是为了求导好约分，加不加都行</h5></li><li><h5 id="alpha-是一个超参数，它指的是-theta-的优化程度占整个损失函数的多少，很显然，-alpha-越大越好，但是在实际中我们要找一个能平衡经验误差项-MSE-和正则化项的-alpha-。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。"><a href="#alpha-是一个超参数，它指的是-theta-的优化程度占整个损失函数的多少，很显然，-alpha-越大越好，但是在实际中我们要找一个能平衡经验误差项-MSE-和正则化项的-alpha-。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。" class="headerlink" title="$\alpha$是一个超参数，它指的是$\theta$的优化程度占整个损失函数的多少，很显然，$\alpha$越大越好，但是在实际中我们要找一个能平衡经验误差项(MSE)和正则化项的$\alpha$。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。"></a>$\alpha$是一个超参数，它指的是$\theta$的优化程度占整个损失函数的多少，很显然，$\alpha$越大越好，但是在实际中我们要找一个能平衡经验误差项(MSE)和正则化项的$\alpha$。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。</h5></li></ul></li></ul><h4 id="九、岭回归（Ridge-Regression）"><a href="#九、岭回归（Ridge-Regression）" class="headerlink" title="九、岭回归（Ridge Regression）"></a>九、岭回归（Ridge Regression）</h4><ul><li><h5 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826211643626.png" alt="image-20200826211643626"></p></li></ul><h4 id="十、LASSO回归（LASSO-Regression）"><a href="#十、LASSO回归（LASSO-Regression）" class="headerlink" title="十、LASSO回归（LASSO Regression）"></a>十、LASSO回归（LASSO Regression）</h4><ul><li><h5 id="比较Ridge和LASSO：为什么随着-alpha-的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线"><a href="#比较Ridge和LASSO：为什么随着-alpha-的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线" class="headerlink" title="比较Ridge和LASSO：为什么随着$\alpha$的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线"></a>比较Ridge和LASSO：为什么随着$\alpha$的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826212135574.png" alt="image-20200826212135574"></p><h5 id="下面我们从梯度下降的角度来解释："><a href="#下面我们从梯度下降的角度来解释：" class="headerlink" title="下面我们从梯度下降的角度来解释："></a>下面我们从梯度下降的角度来解释：</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826213339759.png" alt="image-20200826213339759"></p></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826213439382.png" alt="image-20200826213439382"></p><h5 id="因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些-theta-为零。这也说明了为什么叫岭回归，因为它就像下山一样。"><a href="#因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些-theta-为零。这也说明了为什么叫岭回归，因为它就像下山一样。" class="headerlink" title="因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些$\theta$为零。这也说明了为什么叫岭回归，因为它就像下山一样。"></a>因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些$\theta$为零。这也说明了为什么叫岭回归，因为它就像下山一样。</h5></li></ul></li><li><h5 id="LASSO回归总结"><a href="#LASSO回归总结" class="headerlink" title="LASSO回归总结"></a>LASSO回归总结</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826214412564.png" alt="image-20200826214412564"></p><h5 id="但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。"><a href="#但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。" class="headerlink" title="但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。"></a>但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。</h5></li></ul><h4 id="十一、L1，L2正则"><a href="#十一、L1，L2正则" class="headerlink" title="十一、L1，L2正则"></a>十一、L1，L2正则</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826214730165.png" alt="image-20200826214730165"></p><ul><li><h5 id="其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。"><a href="#其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。" class="headerlink" title="其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。"></a>其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。</h5></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215536491.png" alt="image-20200826215536491"></p><ul><li><h5 id="基于上面的思想，我们把明可夫斯基距离进行一下如下变换："><a href="#基于上面的思想，我们把明可夫斯基距离进行一下如下变换：" class="headerlink" title="基于上面的思想，我们把明可夫斯基距离进行一下如下变换："></a>基于上面的思想，我们把明可夫斯基距离进行一下如下变换：</h5></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215703508.png" alt="image-20200826215703508"></p><ul><li><h5 id="关于图右边开不开根号都不会影响到结果"><a href="#关于图右边开不开根号都不会影响到结果" class="headerlink" title="关于图右边开不开根号都不会影响到结果"></a>关于图右边开不开根号都不会影响到结果</h5></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215846131.png" alt="image-20200826215846131"></p><ul><li><h5 id="L0正则项的原理是使-theta-的非零项的数量尽量少，但是必须使用穷举法来得出-theta-为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。"><a href="#L0正则项的原理是使-theta-的非零项的数量尽量少，但是必须使用穷举法来得出-theta-为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。" class="headerlink" title="L0正则项的原理是使$\theta$的非零项的数量尽量少，但是必须使用穷举法来得出$\theta$为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。"></a>L0正则项的原理是使$\theta$的非零项的数量尽量少，但是必须使用穷举法来得出$\theta$为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。</h5></li></ul><h4 id="十二、弹性网"><a href="#十二、弹性网" class="headerlink" title="十二、弹性网"></a>十二、弹性网</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826220552522.png" alt="image-20200826220552522"></p><ul><li><h5 id="机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。"><a href="#机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。" class="headerlink" title="机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。"></a>机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。</h5></li><li><h5 id="弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。"><a href="#弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。" class="headerlink" title="弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。"></a>弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。</h5></li><li><h5 id="通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。"><a href="#通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。" class="headerlink" title="通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。"></a>通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。</h5></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 多项式回归 </tag>
            
            <tag> Pipeline </tag>
            
            <tag> 过拟合与欠拟合 </tag>
            
            <tag> 学习曲线 </tag>
            
            <tag> 交叉验证 </tag>
            
            <tag> 偏差与方差 </tag>
            
            <tag> 模型正则化 </tag>
            
            <tag> 岭回归、LASSO回归 </tag>
            
            <tag> 弹性网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记10</title>
      <link href="/2020/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010/"/>
      <url>/2020/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010/</url>
      
        <content type="html"><![CDATA[<h4 id="PCA-主成分分析（Principal-Component-Analysis）"><a href="#PCA-主成分分析（Principal-Component-Analysis）" class="headerlink" title="PCA-主成分分析（Principal Component Analysis）"></a>PCA-主成分分析（Principal Component Analysis）</h4><a id="more"></a><h4 id="一、PCA综述"><a href="#一、PCA综述" class="headerlink" title="一、PCA综述"></a>一、PCA综述</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810220126595.png" alt="image-20200810220126595"></p><h4 id="二、详解PCA"><a href="#二、详解PCA" class="headerlink" title="二、详解PCA"></a>二、详解PCA</h4><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810223917827.png" alt="image-20200810223917827"></p><ul><li><h4 id="我们首先想到的降维方法是舍弃特征1或舍弃特征2，这样样本点就会映射到单一坐标轴上，达到了降维的效果。如下图："><a href="#我们首先想到的降维方法是舍弃特征1或舍弃特征2，这样样本点就会映射到单一坐标轴上，达到了降维的效果。如下图：" class="headerlink" title="我们首先想到的降维方法是舍弃特征1或舍弃特征2，这样样本点就会映射到单一坐标轴上，达到了降维的效果。如下图："></a>我们首先想到的降维方法是舍弃特征1或舍弃特征2，这样样本点就会映射到单一坐标轴上，达到了降维的效果。如下图：</h4></li><li><h5 id="左图为舍弃特征1；右图为舍弃特征2："><a href="#左图为舍弃特征1；右图为舍弃特征2：" class="headerlink" title="左图为舍弃特征1；右图为舍弃特征2："></a>左图为舍弃特征1；右图为舍弃特征2：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810224220339.png" alt="image-20200810224220339"></p></li><li><h4 id="显然右图的降维效果比左图好，因为样本点直接距离较大，拥有更高的可区分度"><a href="#显然右图的降维效果比左图好，因为样本点直接距离较大，拥有更高的可区分度" class="headerlink" title="显然右图的降维效果比左图好，因为样本点直接距离较大，拥有更高的可区分度"></a>显然右图的降维效果比左图好，因为样本点直接距离较大，拥有更高的可区分度</h4></li></ul></li><li><h4 id="但是，这是最好的方案吗？显然不是。我们想象一下-能够找到下图这样的一条斜线w-将数据降维到w上-映射到w上-之后-能最好的保留原来的分布特征-且这些点分布在了一个轴上-斜线w-后点和点之间的距离也比之前的两种方案更加的大-此时的区分度也更加明显"><a href="#但是，这是最好的方案吗？显然不是。我们想象一下-能够找到下图这样的一条斜线w-将数据降维到w上-映射到w上-之后-能最好的保留原来的分布特征-且这些点分布在了一个轴上-斜线w-后点和点之间的距离也比之前的两种方案更加的大-此时的区分度也更加明显" class="headerlink" title="但是，这是最好的方案吗？显然不是。我们想象一下,能够找到下图这样的一条斜线w,将数据降维到w上(映射到w上)之后,能最好的保留原来的分布特征,且这些点分布在了一个轴上(斜线w)后点和点之间的距离也比之前的两种方案更加的大,此时的区分度也更加明显"></a>但是，这是最好的方案吗？显然不是。我们想象一下,能够找到下图这样的一条斜线w,将数据降维到w上(映射到w上)之后,能最好的保留原来的分布特征,且这些点分布在了一个轴上(斜线w)后点和点之间的距离也比之前的两种方案更加的大,此时的区分度也更加明显</h4><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810224703164.png" alt="image-20200810224703164"></p></li><li><h4 id="接下来我们要思考以下两个问题了："><a href="#接下来我们要思考以下两个问题了：" class="headerlink" title="接下来我们要思考以下两个问题了："></a>接下来我们要思考以下两个问题了：</h4><ul><li><h5 id="如何找到让这个样本降维后间距最大的轴"><a href="#如何找到让这个样本降维后间距最大的轴" class="headerlink" title="如何找到让这个样本降维后间距最大的轴?"></a>如何找到让这个样本降维后间距最大的轴?</h5></li><li><h5 id="如何定义样本间距"><a href="#如何定义样本间距" class="headerlink" title="如何定义样本间距?"></a>如何定义样本间距?</h5></li></ul></li><li><h4 id="在统计学中-有一个直接的指标可以表示样本间的间距-离散程度-那就是方差-Variance"><a href="#在统计学中-有一个直接的指标可以表示样本间的间距-离散程度-那就是方差-Variance" class="headerlink" title="在统计学中,有一个直接的指标可以表示样本间的间距(离散程度),那就是方差(Variance)"></a>在统计学中,有一个直接的指标可以表示样本间的间距(离散程度),那就是方差(Variance)</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810225109737.png" alt="image-20200810225109737"></p></li><li><h4 id="上述的问题就变为了：找到一个轴-使得样本空间的所有点映射到这个轴之后-方差最大"><a href="#上述的问题就变为了：找到一个轴-使得样本空间的所有点映射到这个轴之后-方差最大" class="headerlink" title="上述的问题就变为了：找到一个轴,使得样本空间的所有点映射到这个轴之后,方差最大"></a>上述的问题就变为了：找到一个轴,使得样本空间的所有点映射到这个轴之后,方差最大</h4></li><li><h4 id="接下来就是求解这个轴："><a href="#接下来就是求解这个轴：" class="headerlink" title="接下来就是求解这个轴："></a>接下来就是求解这个轴：</h4><ul><li><h5 id="首先，将样例的均值归为0（demean），即将所有样本减去这批样本的均值-所有的样本点都会移动，因此可以看作样本点不动，移动坐标轴-，之后就变为了下图所示"><a href="#首先，将样例的均值归为0（demean），即将所有样本减去这批样本的均值-所有的样本点都会移动，因此可以看作样本点不动，移动坐标轴-，之后就变为了下图所示" class="headerlink" title="首先，将样例的均值归为0（demean），即将所有样本减去这批样本的均值(所有的样本点都会移动，因此可以看作样本点不动，移动坐标轴)，之后就变为了下图所示"></a>首先，将样例的均值归为0（demean），即将所有样本减去这批样本的均值(所有的样本点都会移动，因此可以看作样本点不动，移动坐标轴)，之后就变为了下图所示</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810225647955.png" alt="image-20200810225647955"></p><h5 id="此时的样本的均值就为0，方差就变成了"><a href="#此时的样本的均值就为0，方差就变成了" class="headerlink" title="此时的样本的均值就为0，方差就变成了"></a>此时的样本的均值就为0，方差就变成了</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810230237650.png" alt="image-20200810230237650"></p></li><li><h5 id="我们想要求w轴的方向-w1-w2-使得-Var-Xproject-最大，Xproject-是映射到w轴之后的X的坐标："><a href="#我们想要求w轴的方向-w1-w2-使得-Var-Xproject-最大，Xproject-是映射到w轴之后的X的坐标：" class="headerlink" title="我们想要求w轴的方向(w1,w2),使得 Var(Xproject) 最大，Xproject 是映射到w轴之后的X的坐标："></a>我们想要求w轴的方向(w1,w2),使得 <em>Var(X<strong>project</strong>)</em> 最大，<em>X</em>project 是映射到w轴之后的X的坐标：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810230906480.png" alt="image-20200810230906480"></p><h5 id="注意：因为X是矩阵，X-i-是X中的一个向量，所以在公式中不能写成括号的平方，而是模的平方"><a href="#注意：因为X是矩阵，X-i-是X中的一个向量，所以在公式中不能写成括号的平方，而是模的平方" class="headerlink" title="注意：因为X是矩阵，X(i)是X中的一个向量，所以在公式中不能写成括号的平方，而是模的平方"></a>注意：因为X是矩阵，X(i)是X中的一个向量，所以在公式中不能写成括号的平方，而是模的平方</h5><h5 id="接下来求解-X-i-project-，即如何用-X-i-表示（下图点积可以参考《机器学习中的数学》13页-1-4-4）"><a href="#接下来求解-X-i-project-，即如何用-X-i-表示（下图点积可以参考《机器学习中的数学》13页-1-4-4）" class="headerlink" title="接下来求解$X^{(i)}_{project}$，即如何用$X^{(i)}$表示（下图点积可以参考《机器学习中的数学》13页 1.4.4）"></a>接下来求解$X^{(i)}_{project}$，即如何用$X^{(i)}$表示（下图点积可以参考《机器学习中的数学》13页 1.4.4）</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810233526467.png" alt="image-20200810233526467"></p><h5 id="注意：因为我们要将样本点映射到w方向上，所以将w定义为单位向量，其实最终求得是样本点向量在w方向上的分量，即蓝色线条，它对应的就是-vec-X-i-project"><a href="#注意：因为我们要将样本点映射到w方向上，所以将w定义为单位向量，其实最终求得是样本点向量在w方向上的分量，即蓝色线条，它对应的就是-vec-X-i-project" class="headerlink" title="注意：因为我们要将样本点映射到w方向上，所以将w定义为单位向量，其实最终求得是样本点向量在w方向上的分量，即蓝色线条，它对应的就是$\vec{X^{(i)}_{project}}$"></a>注意：因为我们要将样本点映射到w方向上，所以将w定义为单位向量，其实最终求得是样本点向量在w方向上的分量，即蓝色线条，它对应的就是$\vec{X^{(i)}_{project}}$</h5></li></ul></li><li><h4 id="最终："><a href="#最终：" class="headerlink" title="最终："></a>最终：</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810234231684.png" alt="image-20200810234231684"></p></li></ul></li></ul><h4 id="三、使用梯度上升法解决PCA问题"><a href="#三、使用梯度上升法解决PCA问题" class="headerlink" title="三、使用梯度上升法解决PCA问题"></a>三、使用梯度上升法解决PCA问题</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810235951325.png" alt="image-20200810235951325"></p><h4 id="接下来进行向量化，让其运行效率更高"><a href="#接下来进行向量化，让其运行效率更高" class="headerlink" title="接下来进行向量化，让其运行效率更高"></a>接下来进行向量化，让其运行效率更高</h4><ul><li><h5 id="在向量化时要想清楚谁是矩阵，谁是向量，向量是行还是列向量"><a href="#在向量化时要想清楚谁是矩阵，谁是向量，向量是行还是列向量" class="headerlink" title="在向量化时要想清楚谁是矩阵，谁是向量，向量是行还是列向量"></a>在向量化时要想清楚谁是矩阵，谁是向量，向量是行还是列向量</h5></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200811000106484.png" alt="image-20200811000106484"></p><h4 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200811000436388.png" alt="image-20200811000436388"></p><h4 id="四、求数据的前n个主成分（即新坐标系的每一个轴对应的方向）"><a href="#四、求数据的前n个主成分（即新坐标系的每一个轴对应的方向）" class="headerlink" title="四、求数据的前n个主成分（即新坐标系的每一个轴对应的方向）"></a>四、求数据的前n个主成分（即新坐标系的每一个轴对应的方向）</h4><ul><li><h5 id="PCA的本质是将数据从一个坐标系转移到另一个坐标系"><a href="#PCA的本质是将数据从一个坐标系转移到另一个坐标系" class="headerlink" title="PCA的本质是将数据从一个坐标系转移到另一个坐标系"></a>PCA的本质是将数据从一个坐标系转移到另一个坐标系</h5></li><li><h5 id="下面是求解过程："><a href="#下面是求解过程：" class="headerlink" title="下面是求解过程："></a>下面是求解过程：</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200814010029327.png" alt="image-20200814010029327"></p><h5 id="先将-X-i-project-乘以单位向量-vec-w-变成-vec-X-i-project-，这个就是数据X在第一主成分上的分量，我们用X减去这一分量，即-vec-X-i-vec-X-i-project-，这是向量的减法，最终得出的是图中绿色的向量。"><a href="#先将-X-i-project-乘以单位向量-vec-w-变成-vec-X-i-project-，这个就是数据X在第一主成分上的分量，我们用X减去这一分量，即-vec-X-i-vec-X-i-project-，这是向量的减法，最终得出的是图中绿色的向量。" class="headerlink" title="先将$|{X^{(i)}_{project}}|$乘以单位向量$\vec{w}$变成$\vec{X^{(i)}_{project}}$，这个就是数据X在第一主成分上的分量，我们用X减去这一分量，即$\vec{X^{(i)}} - \vec{X^{(i)}_{project}}$，这是向量的减法，最终得出的是图中绿色的向量。"></a>先将$|{X^{(i)}_{project}}|$乘以单位向量$\vec{w}$变成$\vec{X^{(i)}_{project}}$，这个就是数据X在第一主成分上的分量，我们用X减去这一分量，即$\vec{X^{(i)}} - \vec{X^{(i)}_{project}}$，这是向量的减法，最终得出的是图中绿色的向量。</h5></li><li><h5 id="然后在新的数据上求第一主成分（即原数据X的第二主成分），以此类推。"><a href="#然后在新的数据上求第一主成分（即原数据X的第二主成分），以此类推。" class="headerlink" title="然后在新的数据上求第一主成分（即原数据X的第二主成分），以此类推。"></a>然后在新的数据上求第一主成分（即原数据X的第二主成分），以此类推。</h5></li></ul></li></ul><h4 id="五、高维数据向低维数据映射"><a href="#五、高维数据向低维数据映射" class="headerlink" title="五、高维数据向低维数据映射"></a>五、高维数据向低维数据映射</h4><h5 id="PCA的本质是将数据从一个坐标系转换到另一个坐标系上，即原来是n维坐标系，转换后也是n维坐标系。但是我们要将高维向低维映射，所以要从转换后的n维坐标系中取前k个坐标轴形成一个k维的坐标系。然后将X中的每一行样本都乘以W中每一个维度的坐标轴。这样就从n维降到了k维："><a href="#PCA的本质是将数据从一个坐标系转换到另一个坐标系上，即原来是n维坐标系，转换后也是n维坐标系。但是我们要将高维向低维映射，所以要从转换后的n维坐标系中取前k个坐标轴形成一个k维的坐标系。然后将X中的每一行样本都乘以W中每一个维度的坐标轴。这样就从n维降到了k维：" class="headerlink" title="PCA的本质是将数据从一个坐标系转换到另一个坐标系上，即原来是n维坐标系，转换后也是n维坐标系。但是我们要将高维向低维映射，所以要从转换后的n维坐标系中取前k个坐标轴形成一个k维的坐标系。然后将X中的每一行样本都乘以W中每一个维度的坐标轴。这样就从n维降到了k维："></a>PCA的本质是将数据从一个坐标系转换到另一个坐标系上，即原来是n维坐标系，转换后也是n维坐标系。但是我们要将高维向低维映射，所以要从转换后的n维坐标系中取前k个坐标轴形成一个k维的坐标系。然后将X中的每一行样本都乘以W中每一个维度的坐标轴。这样就从n维降到了k维：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200814224611023.png" alt="image-20200814224611023"></p><h5 id="也可以从k维恢复到n维，但是恢复回来的-X-m-不等于原来的X："><a href="#也可以从k维恢复到n维，但是恢复回来的-X-m-不等于原来的X：" class="headerlink" title="也可以从k维恢复到n维，但是恢复回来的$X_m$不等于原来的X："></a>也可以从k维恢复到n维，但是恢复回来的$X_m$不等于原来的X：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200814224911159.png" alt="image-20200814224911159"></p><h4 id="六、PCA在人脸识别中的应用"><a href="#六、PCA在人脸识别中的应用" class="headerlink" title="六、PCA在人脸识别中的应用"></a>六、PCA在人脸识别中的应用</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200815183722277.png" alt="image-20200815183722277"></p><h5 id="对于-W-k-来说，本身每一行代表一个方向。那么换一种思路：如果每一行看作一个样本，那么第一行表示最能反映原数据X的样本特征，以此类推。"><a href="#对于-W-k-来说，本身每一行代表一个方向。那么换一种思路：如果每一行看作一个样本，那么第一行表示最能反映原数据X的样本特征，以此类推。" class="headerlink" title="对于$W_k$来说，本身每一行代表一个方向。那么换一种思路：如果每一行看作一个样本，那么第一行表示最能反映原数据X的样本特征，以此类推。"></a>对于$W_k$来说，本身每一行代表一个方向。那么换一种思路：如果每一行看作一个样本，那么第一行表示最能反映原数据X的样本特征，以此类推。</h5><h5 id="在人脸识别中-X的每一行对应一个人脸数据，W的每一行也是一个人脸数据，这个就被称为特征脸，所以每一个特征脸其实对应一个主成分"><a href="#在人脸识别中-X的每一行对应一个人脸数据，W的每一行也是一个人脸数据，这个就被称为特征脸，所以每一个特征脸其实对应一个主成分" class="headerlink" title="在人脸识别中,X的每一行对应一个人脸数据，W的每一行也是一个人脸数据，这个就被称为特征脸，所以每一个特征脸其实对应一个主成分"></a>在人脸识别中,X的每一行对应一个人脸数据，W的每一行也是一个人脸数据，这个就被称为特征脸，所以每一个特征脸其实对应一个主成分</h5>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> PCA </tag>
            
            <tag> 非监督学习 </tag>
            
            <tag> 数据降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记9</title>
      <link href="/2020/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09/"/>
      <url>/2020/08/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09/</url>
      
        <content type="html"><![CDATA[<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><a id="more"></a><h4 id="一，什么是梯度下降法"><a href="#一，什么是梯度下降法" class="headerlink" title="一，什么是梯度下降法"></a>一，什么是梯度下降法</h4><h4 id="1、综述"><a href="#1、综述" class="headerlink" title="1、综述"></a>1、综述</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200807161901759.png" alt="image-20200807161901759"></p><h4 id="2、什么是梯度下降法"><a href="#2、什么是梯度下降法" class="headerlink" title="2、什么是梯度下降法"></a>2、什么是梯度下降法</h4><h4 id="2-1、什么是方向导数"><a href="#2-1、什么是方向导数" class="headerlink" title="2.1、什么是方向导数"></a>2.1、什么是方向导数</h4><ul><li><h5 id="2-1-1、什么是全微分"><a href="#2-1-1、什么是全微分" class="headerlink" title="2.1.1、什么是全微分"></a>2.1.1、什么是全微分</h5><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822224646922.png" alt="image-20200822224646922"></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822224817278.png" alt="image-20200822224817278"></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822224842975.png" alt="image-20200822224842975"></li></ul></li><li><h5 id="2-1-2、方向余弦与单位方向向量"><a href="#2-1-2、方向余弦与单位方向向量" class="headerlink" title="2.1.2、方向余弦与单位方向向量"></a>2.1.2、方向余弦与单位方向向量</h5><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230436729.png" alt="image-20200822230436729"></li></ul></li><li><h5 id="2-1-3、方向导数"><a href="#2-1-3、方向导数" class="headerlink" title="2.1.3、方向导数"></a>2.1.3、方向导数</h5><ul><li><h5 id="偏导数衡量了点在x轴和y轴移动时函数的变化，那么如果在其它方向移动呢？是否在任意方向上都有一个导数呢？答案是肯定的，那就是方向导数。"><a href="#偏导数衡量了点在x轴和y轴移动时函数的变化，那么如果在其它方向移动呢？是否在任意方向上都有一个导数呢？答案是肯定的，那就是方向导数。" class="headerlink" title="偏导数衡量了点在x轴和y轴移动时函数的变化，那么如果在其它方向移动呢？是否在任意方向上都有一个导数呢？答案是肯定的，那就是方向导数。"></a>偏导数衡量了点在x轴和y轴移动时函数的变化，那么如果在其它方向移动呢？是否在任意方向上都有一个导数呢？答案是肯定的，那就是方向导数。</h5></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230631927.png" alt="image-20200822230631927"></p></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230656288.png" alt="image-20200822230656288"></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230718652.png" alt="image-20200822230718652"></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230805318.png" alt="image-20200822230805318"></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230823590.png" alt="image-20200822230823590"></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822230845941.png" alt="image-20200822230845941"></li></ul></li></ul><h4 id="2-2、什么是梯度"><a href="#2-2、什么是梯度" class="headerlink" title="2.2、什么是梯度"></a>2.2、什么是梯度</h4><ul><li><h5 id="上图（方向导数的最值）的向量a被称为梯度，记为grad-f-x0-y0-，它是综合了函数f所有偏导数的向量"><a href="#上图（方向导数的最值）的向量a被称为梯度，记为grad-f-x0-y0-，它是综合了函数f所有偏导数的向量" class="headerlink" title="上图（方向导数的最值）的向量a被称为梯度，记为grad f(x0,y0)，它是综合了函数f所有偏导数的向量"></a>上图（方向导数的最值）的向量a被称为梯度，记为grad f(x0,y0)，它是综合了函数f所有偏导数的向量</h5><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822231644755.png" alt="image-20200822231644755"></li></ul></li><li><h5 id="还可以表示为："><a href="#还可以表示为：" class="headerlink" title="还可以表示为："></a>还可以表示为：</h5><ul><li><h5 id=""><a href="#" class="headerlink" title=""></a><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808012812302.png" alt="image-20200808012812302"></h5></li></ul></li></ul><h4 id="2-3、梯度的意义"><a href="#2-3、梯度的意义" class="headerlink" title="2.3、梯度的意义"></a>2.3、梯度的意义</h4><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200822233159506.png" alt="image-20200822233159506"></li></ul><h4 id="2-4、梯度下降"><a href="#2-4、梯度下降" class="headerlink" title="2.4、梯度下降"></a>2.4、梯度下降</h4><ul><li><h4 id="在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解效用函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。"><a href="#在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解效用函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。" class="headerlink" title="在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解效用函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。"></a>在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解效用函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。</h4></li><li><h4 id="简单地说，梯度下降就是沿着沿梯度下降的方向求解极小值时的自变量-也就是-theta-。"><a href="#简单地说，梯度下降就是沿着沿梯度下降的方向求解极小值时的自变量-也就是-theta-。" class="headerlink" title="简单地说，梯度下降就是沿着沿梯度下降的方向求解极小值时的自变量(也就是\theta)。"></a>简单地说，梯度下降就是沿着沿梯度下降的方向求解极小值时的自变量(也就是<script type="math/tex">\theta</script>)。</h4></li><li><h4 id="原理："><a href="#原理：" class="headerlink" title="原理："></a>原理：</h4><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808015214860.png" alt="image-20200808015214860"></p></li><li><h4 id="问题是这样做实在太慢，迭代过程及其耗时，所以人们在此基础上设计出更加快速的处理办法—增加学习率"><a href="#问题是这样做实在太慢，迭代过程及其耗时，所以人们在此基础上设计出更加快速的处理办法—增加学习率" class="headerlink" title="问题是这样做实在太慢，迭代过程及其耗时，所以人们在此基础上设计出更加快速的处理办法—增加学习率"></a>问题是这样做实在太慢，迭代过程及其耗时，所以人们在此基础上设计出更加快速的处理办法—增加学习率</h4></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808015518727.png" alt="image-20200808015518727"></p></li><li><h4 id="下图用η表示学习率"><a href="#下图用η表示学习率" class="headerlink" title="下图用η表示学习率"></a>下图用η表示学习率</h4><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808020121659.png" alt="image-20200808020121659"></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808020208050.png" alt="image-20200808020208050"></li></ul></li></ul></li><li><h4 id="局部最优解和全局最优解"><a href="#局部最优解和全局最优解" class="headerlink" title="局部最优解和全局最优解"></a>局部最优解和全局最优解</h4><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808020340354.png" alt="image-20200808020340354"></li></ul></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200808020443300.png" alt="image-20200808020443300"></p></li></ul><h4 id="二、通过线性回归来学习：批量梯度下降"><a href="#二、通过线性回归来学习：批量梯度下降" class="headerlink" title="二、通过线性回归来学习：批量梯度下降"></a>二、通过线性回归来学习：批量梯度下降</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809002443221.png" alt="image-20200809002443221"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809002831126.png" alt="image-20200809002831126"></p><h4 id="说明"><a href="#说明" class="headerlink" title="说明:"></a>说明:</h4><ul><li><h4 id="首先这里面theta是未知向量，x和y是已知向量；其次为了简化，在Xb矩阵里面加一列1，进而简化损失函数"><a href="#首先这里面theta是未知向量，x和y是已知向量；其次为了简化，在Xb矩阵里面加一列1，进而简化损失函数" class="headerlink" title="首先这里面theta是未知向量，x和y是已知向量；其次为了简化，在Xb矩阵里面加一列1，进而简化损失函数"></a>首先这里面theta是未知向量，x和y是已知向量；其次为了简化，在Xb矩阵里面加一列1，进而简化损失函数</h4></li><li><h4 id="我们希望最终求出的梯度与样本数量m无关，所以请看下图"><a href="#我们希望最终求出的梯度与样本数量m无关，所以请看下图" class="headerlink" title="我们希望最终求出的梯度与样本数量m无关，所以请看下图"></a>我们希望最终求出的梯度与样本数量m无关，所以请看下图</h4></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809003822687.png" alt="image-20200809003822687"></p><h4 id="三，线性回归中梯度下降的向量化表示"><a href="#三，线性回归中梯度下降的向量化表示" class="headerlink" title="三，线性回归中梯度下降的向量化表示"></a>三，线性回归中梯度下降的向量化表示</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809013423862.png" alt="image-20200809013423862"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809013459381.png" alt="image-20200809013459381"></p><h4 id="四、真实环境下使用梯度下降法的注意事项"><a href="#四、真实环境下使用梯度下降法的注意事项" class="headerlink" title="四、真实环境下使用梯度下降法的注意事项"></a>四、真实环境下使用梯度下降法的注意事项</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809015016880.png" alt="image-20200809015016880"></p><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><ul><li><h4 id="梯度下降法对于多样本数据集比正规方程解更省时间"><a href="#梯度下降法对于多样本数据集比正规方程解更省时间" class="headerlink" title="梯度下降法对于多样本数据集比正规方程解更省时间"></a>梯度下降法对于多样本数据集比正规方程解更省时间</h4></li><li><h4 id="关于使用sklearn中的归一化可以参看https-blog-csdn-net-u011734144-article-details-84066784和Jupyter"><a href="#关于使用sklearn中的归一化可以参看https-blog-csdn-net-u011734144-article-details-84066784和Jupyter" class="headerlink" title="关于使用sklearn中的归一化可以参看https://blog.csdn.net/u011734144/article/details/84066784和Jupyter"></a>关于使用sklearn中的归一化可以参看<a href="https://blog.csdn.net/u011734144/article/details/84066784和Jupyter" target="_blank" rel="noopener">https://blog.csdn.net/u011734144/article/details/84066784和Jupyter</a></h4></li></ul><h4 id="五、随机梯度下降法"><a href="#五、随机梯度下降法" class="headerlink" title="五、随机梯度下降法"></a>五、随机梯度下降法</h4><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809220204342.png" alt="image-20200809220204342"></p><ul><li><h4 id="因为批量梯度下降法会随着样本数量的增加导致运行效率低下，所以我们考虑每次迭代时只随机选取某一行数据进行梯度计算"><a href="#因为批量梯度下降法会随着样本数量的增加导致运行效率低下，所以我们考虑每次迭代时只随机选取某一行数据进行梯度计算" class="headerlink" title="因为批量梯度下降法会随着样本数量的增加导致运行效率低下，所以我们考虑每次迭代时只随机选取某一行数据进行梯度计算"></a>因为批量梯度下降法会随着样本数量的增加导致运行效率低下，所以我们考虑每次迭代时只随机选取某一行数据进行梯度计算</h4></li></ul></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809220604635.png" alt="image-20200809220604635"></p><ul><li><h4 id="由于每次迭代是随机选取的一行数据，所以损失函数的值未必是一定朝着梯度下降的方向进行搜索，因此，学习率就不能是一个固定值（固定值可能导致已经搜索到了最优参数，但是迭代未完成，这样会导致远离最优参数），而是要逐渐减小的"><a href="#由于每次迭代是随机选取的一行数据，所以损失函数的值未必是一定朝着梯度下降的方向进行搜索，因此，学习率就不能是一个固定值（固定值可能导致已经搜索到了最优参数，但是迭代未完成，这样会导致远离最优参数），而是要逐渐减小的" class="headerlink" title="由于每次迭代是随机选取的一行数据，所以损失函数的值未必是一定朝着梯度下降的方向进行搜索，因此，学习率就不能是一个固定值（固定值可能导致已经搜索到了最优参数，但是迭代未完成，这样会导致远离最优参数），而是要逐渐减小的"></a>由于每次迭代是随机选取的一行数据，所以损失函数的值未必是一定朝着梯度下降的方向进行搜索，因此，学习率就不能是一个固定值（固定值可能导致已经搜索到了最优参数，但是迭代未完成，这样会导致远离最优参数），而是要逐渐减小的</h4></li><li><h4 id="因此，学习率为上图所示，其中a和b是随机梯度下降法的超参数"><a href="#因此，学习率为上图所示，其中a和b是随机梯度下降法的超参数" class="headerlink" title="因此，学习率为上图所示，其中a和b是随机梯度下降法的超参数"></a>因此，学习率为上图所示，其中a和b是随机梯度下降法的超参数</h4></li><li><h4 id="其实，该学习率是采用了模拟退火的思想"><a href="#其实，该学习率是采用了模拟退火的思想" class="headerlink" title="其实，该学习率是采用了模拟退火的思想"></a>其实，该学习率是采用了模拟退火的思想</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200809230249384.png" alt="image-20200809230249384"></p></li><li><h4 id="在随机梯度下降中，迭代次数指的是对提供的样本要迭代几遍，设样本数为m，迭代次数为n-iters，则真正的迭代次数为m-n-iters"><a href="#在随机梯度下降中，迭代次数指的是对提供的样本要迭代几遍，设样本数为m，迭代次数为n-iters，则真正的迭代次数为m-n-iters" class="headerlink" title="在随机梯度下降中，迭代次数指的是对提供的样本要迭代几遍，设样本数为m，迭代次数为n_iters，则真正的迭代次数为m*n_iters"></a>在随机梯度下降中，迭代次数指的是对提供的样本要迭代几遍，设样本数为m，迭代次数为n_iters，则真正的迭代次数为m*n_iters</h4></li></ul></li></ul><h4 id="六、关于梯度的调试"><a href="#六、关于梯度的调试" class="headerlink" title="六、关于梯度的调试"></a>六、关于梯度的调试</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810003902485.png" alt="image-20200810003902485"></p><h4 id="我们为了验证我们计算的梯度是否正确可以采用如上方法，在目标点（红点）的正方向和负方向分别取两个点，这两个点距离红点的距离近乎为0，然后将这两个蓝点连线，则它的斜率近似等于红点切线处的斜率"><a href="#我们为了验证我们计算的梯度是否正确可以采用如上方法，在目标点（红点）的正方向和负方向分别取两个点，这两个点距离红点的距离近乎为0，然后将这两个蓝点连线，则它的斜率近似等于红点切线处的斜率" class="headerlink" title="我们为了验证我们计算的梯度是否正确可以采用如上方法，在目标点（红点）的正方向和负方向分别取两个点，这两个点距离红点的距离近乎为0，然后将这两个蓝点连线，则它的斜率近似等于红点切线处的斜率"></a>我们为了验证我们计算的梯度是否正确可以采用如上方法，在目标点（红点）的正方向和负方向分别取两个点，这两个点距离红点的距离近乎为0，然后将这两个蓝点连线，则它的斜率近似等于红点切线处的斜率</h4><h4 id="下面我们推广到高维数据集上："><a href="#下面我们推广到高维数据集上：" class="headerlink" title="下面我们推广到高维数据集上："></a>下面我们推广到高维数据集上：</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810004502460.png" alt="image-20200810004502460"></p><h4 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810011144727.png" alt="image-20200810011144727"></p><h4 id="小批量梯度下降法结合了批量梯度下降法的稳定（一定沿着梯度下降方向计算）以及随机梯度下降法的速度快的优点，每次迭代只计算k行样本"><a href="#小批量梯度下降法结合了批量梯度下降法的稳定（一定沿着梯度下降方向计算）以及随机梯度下降法的速度快的优点，每次迭代只计算k行样本" class="headerlink" title="小批量梯度下降法结合了批量梯度下降法的稳定（一定沿着梯度下降方向计算）以及随机梯度下降法的速度快的优点，每次迭代只计算k行样本"></a>小批量梯度下降法结合了批量梯度下降法的稳定（一定沿着梯度下降方向计算）以及随机梯度下降法的速度快的优点，每次迭代只计算k行样本</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810011434738.png" alt="image-20200810011434738"></p><h4 id="关于梯度上升法："><a href="#关于梯度上升法：" class="headerlink" title="关于梯度上升法："></a>关于梯度上升法：</h4><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200810011643033.png" alt="image-20200810011643033"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记8</title>
      <link href="/2020/07/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08/"/>
      <url>/2020/07/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08/</url>
      
        <content type="html"><![CDATA[<h3 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h3><a id="more"></a><h4 id="一、什么是多元线性回归"><a href="#一、什么是多元线性回归" class="headerlink" title="一、什么是多元线性回归"></a>一、什么是多元线性回归</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730235347025.png" alt="image-20200730235347025"></p><h4 id="二、多元线性回归的正规方程解"><a href="#二、多元线性回归的正规方程解" class="headerlink" title="二、多元线性回归的正规方程解"></a>二、多元线性回归的正规方程解</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731030050773.png" alt="image-20200731030050773"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731030338464.png" alt="image-20200731030338464"></p><h4 id="因为X是一个特征矩阵，所以X-i-本身就是行向量，因此-theta必须由行向量转置为列向量"><a href="#因为X是一个特征矩阵，所以X-i-本身就是行向量，因此-theta必须由行向量转置为列向量" class="headerlink" title="因为X是一个特征矩阵，所以X^{(i)}本身就是行向量，因此\theta必须由行向量转置为列向量"></a>因为X是一个特征矩阵，所以<script type="math/tex">X^{(i)}</script>本身就是行向量，因此<script type="math/tex">\theta</script>必须由行向量转置为列向量</h4><h4 id="接下来进一步推广到整个特征矩阵"><a href="#接下来进一步推广到整个特征矩阵" class="headerlink" title="接下来进一步推广到整个特征矩阵"></a>接下来进一步推广到整个特征矩阵</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731031136440.png" alt="image-20200731031136440"></p><h4 id="因为X-b是m-n的矩阵，-theta-是n行列向量，所以-hat-y-是一个m行的列向量"><a href="#因为X-b是m-n的矩阵，-theta-是n行列向量，所以-hat-y-是一个m行的列向量" class="headerlink" title="因为X_b是m*n的矩阵，\theta 是n行列向量，所以\hat{y}是一个m行的列向量"></a>因为<script type="math/tex">X_b</script>是m*n的矩阵，<script type="math/tex">\theta</script> 是n行列向量，所以<script type="math/tex">\hat{y}</script>是一个m行的列向量</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731032608917.png" alt="image-20200731032608917"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731032641791.png" alt="image-20200731032641791"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200731032712657.png" alt="image-20200731032712657"></p><h4 id="因为-theta只是X前面的系数，与量纲无关，所以不需要归一化处理"><a href="#因为-theta只是X前面的系数，与量纲无关，所以不需要归一化处理" class="headerlink" title="因为\theta只是X前面的系数，与量纲无关，所以不需要归一化处理"></a>因为<script type="math/tex">\theta</script>只是X前面的系数，与量纲无关，所以不需要归一化处理</h4><h4 id="三、线性回归总结"><a href="#三、线性回归总结" class="headerlink" title="三、线性回归总结"></a>三、线性回归总结</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200805000611219.png" alt="image-20200805000611219"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200805000625908.png" alt="image-20200805000625908"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200805000642638.png" alt="image-20200805000642638"></p><h4 id="关于可解释性，我们拿波士顿房价预测来说，训练得到的参数的正负表示该特征与房价呈正相关还是负相关；绝对值大小表示对房价的影响程度，越大则对房价影响越大，具体看Jupyter。"><a href="#关于可解释性，我们拿波士顿房价预测来说，训练得到的参数的正负表示该特征与房价呈正相关还是负相关；绝对值大小表示对房价的影响程度，越大则对房价影响越大，具体看Jupyter。" class="headerlink" title="关于可解释性，我们拿波士顿房价预测来说，训练得到的参数的正负表示该特征与房价呈正相关还是负相关；绝对值大小表示对房价的影响程度，越大则对房价影响越大，具体看Jupyter。"></a>关于可解释性，我们拿波士顿房价预测来说，训练得到的参数的正负表示该特征与房价呈正相关还是负相关；绝对值大小表示对房价的影响程度，越大则对房价影响越大，具体看Jupyter。</h4>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性回归 </tag>
            
            <tag> 多元线性回归 </tag>
            
            <tag> 模型可解释性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记7</title>
      <link href="/2020/07/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07/"/>
      <url>/2020/07/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07/</url>
      
        <content type="html"><![CDATA[<h3 id="回归算法的评价"><a href="#回归算法的评价" class="headerlink" title="回归算法的评价"></a>回归算法的评价</h3><a id="more"></a><h4 id="一、从简单线性回归引出横梁标准"><a href="#一、从简单线性回归引出横梁标准" class="headerlink" title="一、从简单线性回归引出横梁标准"></a>一、从简单线性回归引出横梁标准</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730220946800.png" alt="image-20200730220946800"></p><h5 id="衡量标准就是测试集中的真实值与测试集中的预测值的差的平方，但是该衡量标准与样本数量m有关。为了避免不同样本量的数据集带来的模型优劣争议，我们要将这个式子除以m，得到下图"><a href="#衡量标准就是测试集中的真实值与测试集中的预测值的差的平方，但是该衡量标准与样本数量m有关。为了避免不同样本量的数据集带来的模型优劣争议，我们要将这个式子除以m，得到下图" class="headerlink" title="衡量标准就是测试集中的真实值与测试集中的预测值的差的平方，但是该衡量标准与样本数量m有关。为了避免不同样本量的数据集带来的模型优劣争议，我们要将这个式子除以m，得到下图"></a>衡量标准就是测试集中的真实值与测试集中的预测值的差的平方，但是该衡量标准与样本数量m有关。为了避免不同样本量的数据集带来的模型优劣争议，我们要将这个式子除以m，得到下图</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730221356928.png" alt="image-20200730221356928"></p><h5 id="但是MSE会使得该数据集的量纲（单位）为原来的平方（比如米变成平方米），所以我们可以给MSE开根号使得它和原数据集的量纲相同，这就是RMSE"><a href="#但是MSE会使得该数据集的量纲（单位）为原来的平方（比如米变成平方米），所以我们可以给MSE开根号使得它和原数据集的量纲相同，这就是RMSE" class="headerlink" title="但是MSE会使得该数据集的量纲（单位）为原来的平方（比如米变成平方米），所以我们可以给MSE开根号使得它和原数据集的量纲相同，这就是RMSE"></a>但是MSE会使得该数据集的量纲（单位）为原来的平方（比如米变成平方米），所以我们可以给MSE开根号使得它和原数据集的量纲相同，这就是RMSE</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730221818237.png" alt="image-20200730221818237"></p><h5 id="同样我们还可以使用以下的评价算法，注意线性回归模型找最优参数时不能用绝对值（不是处处可导），但是评价时就可以用，它们是互不影响的。"><a href="#同样我们还可以使用以下的评价算法，注意线性回归模型找最优参数时不能用绝对值（不是处处可导），但是评价时就可以用，它们是互不影响的。" class="headerlink" title="同样我们还可以使用以下的评价算法，注意线性回归模型找最优参数时不能用绝对值（不是处处可导），但是评价时就可以用，它们是互不影响的。"></a>同样我们还可以使用以下的评价算法，注意线性回归模型找最优参数时不能用绝对值（不是处处可导），但是评价时就可以用，它们是互不影响的。</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730222124641.png" alt="image-20200730222124641"></p><h3 id="二、RMSE-VS-MAE"><a href="#二、RMSE-VS-MAE" class="headerlink" title="二、RMSE VS  MAE"></a>二、RMSE VS  MAE</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730230527231.png" alt="image-20200730230527231"></p><h4 id="RMSE中的平方操作相当于放大了max-y-true-y-predict-，所以开根号以后依然比MAE大，因此我们要想让RMSE或MSE更有意义，就要让差值尽量小。"><a href="#RMSE中的平方操作相当于放大了max-y-true-y-predict-，所以开根号以后依然比MAE大，因此我们要想让RMSE或MSE更有意义，就要让差值尽量小。" class="headerlink" title="RMSE中的平方操作相当于放大了max(y_{true}-y_{predict})，所以开根号以后依然比MAE大，因此我们要想让RMSE或MSE更有意义，就要让差值尽量小。"></a>RMSE中的平方操作相当于放大了<script type="math/tex">max(y_{true}-y_{predict})</script>，所以开根号以后依然比MAE大，因此我们要想让RMSE或MSE更有意义，就要让差值尽量小。</h4><h3 id="三、最好的衡量线性回归法的指标"><a href="#三、最好的衡量线性回归法的指标" class="headerlink" title="三、最好的衡量线性回归法的指标"></a>三、最好的衡量线性回归法的指标</h3><h4 id="从分类算法的评价算法我们可以得出它可以通过0到1中的一个数来衡量模型的好坏。但由于RMSE和MSE只能得出平均差值，所以导致训练处理的模型不具有通用性（比如房价和学生成绩）"><a href="#从分类算法的评价算法我们可以得出它可以通过0到1中的一个数来衡量模型的好坏。但由于RMSE和MSE只能得出平均差值，所以导致训练处理的模型不具有通用性（比如房价和学生成绩）" class="headerlink" title="从分类算法的评价算法我们可以得出它可以通过0到1中的一个数来衡量模型的好坏。但由于RMSE和MSE只能得出平均差值，所以导致训练处理的模型不具有通用性（比如房价和学生成绩）"></a>从分类算法的评价算法我们可以得出它可以通过0到1中的一个数来衡量模型的好坏。但由于RMSE和MSE只能得出平均差值，所以导致训练处理的模型不具有通用性（比如房价和学生成绩）</h4><h4 id="因此，我们要使用以下算法来评价模型"><a href="#因此，我们要使用以下算法来评价模型" class="headerlink" title="因此，我们要使用以下算法来评价模型"></a>因此，我们要使用以下算法来评价模型</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730232425399.png" alt="image-20200730232425399"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730232723872.png" alt="image-20200730232723872"></p><h4 id="R2是衡量了我们模型没有产生错误的相应指标"><a href="#R2是衡量了我们模型没有产生错误的相应指标" class="headerlink" title="R2是衡量了我们模型没有产生错误的相应指标"></a>R2是衡量了我们模型没有产生错误的相应指标</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730233257406.png" alt="image-20200730233257406"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200730233453803.png" alt="image-20200730233453803"></p><h4 id="注意，Var指方差"><a href="#注意，Var指方差" class="headerlink" title="注意，Var指方差"></a>注意，Var指方差</h4>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性回归 </tag>
            
            <tag> 回归算法评价 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记6</title>
      <link href="/2020/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06/"/>
      <url>/2020/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06/</url>
      
        <content type="html"><![CDATA[<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><a id="more"></a><h4 id="一、线性回归简述"><a href="#一、线性回归简述" class="headerlink" title="一、线性回归简述"></a>一、线性回归简述</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726225427820.png" alt="image-20200726225427820"></p><ul><li><h5 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726225808532.png" alt="image-20200726225808532"></p></li><li><h5 id="注意：分类算法的横纵坐标都是样本特征；而回归算法的横纵坐标分别为样本特征和样本标签"><a href="#注意：分类算法的横纵坐标都是样本特征；而回归算法的横纵坐标分别为样本特征和样本标签" class="headerlink" title="注意：分类算法的横纵坐标都是样本特征；而回归算法的横纵坐标分别为样本特征和样本标签"></a>注意：分类算法的横纵坐标都是样本特征；而回归算法的横纵坐标分别为样本特征和样本标签</h5></li><li><h5 id="样本特征只有一个称为简单线性回归"><a href="#样本特征只有一个称为简单线性回归" class="headerlink" title="样本特征只有一个称为简单线性回归"></a>样本特征只有一个称为简单线性回归</h5></li></ul></li></ul><h3 id="二、简单线性回归"><a href="#二、简单线性回归" class="headerlink" title="二、简单线性回归"></a>二、简单线性回归</h3><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726230627487.png" alt="image-20200726230627487"></p></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726230701216.png" alt="image-20200726230701216"></p><h4 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h4><ul><li><h6 id="因为我们要考虑所有样本的差距，所以不能有正有负（可能会抵消为0），因此每个样本的差距值应该-gt-0。"><a href="#因为我们要考虑所有样本的差距，所以不能有正有负（可能会抵消为0），因此每个样本的差距值应该-gt-0。" class="headerlink" title="因为我们要考虑所有样本的差距，所以不能有正有负（可能会抵消为0），因此每个样本的差距值应该&gt;=0。"></a>因为我们要考虑所有样本的差距，所以不能有正有负（可能会抵消为0），因此每个样本的差距值应该&gt;=0。</h6></li><li><h6 id="由于我们希望真实值和预测值差距尽量小，所以需要求极值，而绝对值函数不是连续处处可导，所以我们改差值的平方函数。"><a href="#由于我们希望真实值和预测值差距尽量小，所以需要求极值，而绝对值函数不是连续处处可导，所以我们改差值的平方函数。" class="headerlink" title="由于我们希望真实值和预测值差距尽量小，所以需要求极值，而绝对值函数不是连续处处可导，所以我们改差值的平方函数。"></a>由于我们希望真实值和预测值差距尽量小，所以需要求极值，而绝对值函数不是连续处处可导，所以我们改差值的平方函数。</h6></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726231427640.png" alt="image-20200726231427640"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726231641441.png" alt="image-20200726231641441"></p><h4 id="说明：-1"><a href="#说明：-1" class="headerlink" title="说明："></a>说明：</h4><ul><li>损失函数：度量模型没有拟合到的样本的程度</li><li><p>效用函数：度量模型拟合到的样本的程度</p></li><li><p>近乎所有的参数学习算法都是这样来求解最优参数（线性回归，多项式回归，逻辑回归，SVM，神经网络等等）</p></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726232703309.png" alt="image-20200726232703309"></p><h4 id="2-最小二乘法："><a href="#2-最小二乘法：" class="headerlink" title="2.最小二乘法："></a>2.最小二乘法：</h4><h5 id="推导过程（求解a和b）"><a href="#推导过程（求解a和b）" class="headerlink" title="推导过程（求解a和b）"></a>推导过程（求解a和b）</h5><p>首先求解参数b</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726234248085.png" alt="image-20200726234248085"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200726234339896.png" alt="image-20200726234339896"></p><p>然后求解参数a</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200727000111783.png" alt="image-20200727000111783"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200727000132271.png" alt="image-20200727000132271"></p><h3 id="说明：其实到这里已经将参数a求了出来，但是为了编程实现方便，我们要进一步简化这个式子，请看下图，注意-bar-x和-bar-y-都是常数，所以可以提到求和符号前面"><a href="#说明：其实到这里已经将参数a求了出来，但是为了编程实现方便，我们要进一步简化这个式子，请看下图，注意-bar-x和-bar-y-都是常数，所以可以提到求和符号前面" class="headerlink" title="说明：其实到这里已经将参数a求了出来，但是为了编程实现方便，我们要进一步简化这个式子，请看下图，注意$ \bar x和\bar y $都是常数，所以可以提到求和符号前面"></a>说明：其实到这里已经将参数a求了出来，但是为了编程实现方便，我们要进一步简化这个式子，请看下图，注意$ \bar x和\bar y $都是常数，所以可以提到求和符号前面</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200727001049264.png" alt="image-20200727001049264"></p><h3 id="三，向量化运算"><a href="#三，向量化运算" class="headerlink" title="三，向量化运算"></a>三，向量化运算</h3><h4 id="在代码编写过程中使用for循环会使运行效率大大降低，所以我们要把循环去掉。"><a href="#在代码编写过程中使用for循环会使运行效率大大降低，所以我们要把循环去掉。" class="headerlink" title="在代码编写过程中使用for循环会使运行效率大大降低，所以我们要把循环去掉。"></a>在代码编写过程中使用for循环会使运行效率大大降低，所以我们要把循环去掉。</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200729233852136.png" alt="image-20200729233852136"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200729233924794.png" alt="image-20200729233924794"></p><h4 id="所以分子为-vec-vec-x-bar-x-cdot-vec-vec-y-bar-y"><a href="#所以分子为-vec-vec-x-bar-x-cdot-vec-vec-y-bar-y" class="headerlink" title="所以分子为(\vec{\vec{x}-\bar x})\cdot(\vec{\vec{y}-\bar y} )"></a>所以分子为<script type="math/tex">(\vec{\vec{x}-\bar x})\cdot(\vec{\vec{y}-\bar y} )</script></h4><h4 id="分母为-vec-vec-x-bar-x-cdot-vec-vec-x-bar-x"><a href="#分母为-vec-vec-x-bar-x-cdot-vec-vec-x-bar-x" class="headerlink" title="分母为(\vec{\vec{x}-\bar x})\cdot(\vec{\vec{x}-\bar x})"></a>分母为<script type="math/tex">(\vec{\vec{x}-\bar x})\cdot(\vec{\vec{x}-\bar x})</script></h4><h4 id="注意，这种思想非常重要，请以后常用"><a href="#注意，这种思想非常重要，请以后常用" class="headerlink" title="注意，这种思想非常重要，请以后常用"></a>注意，这种思想非常重要，请以后常用</h4>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性回归 </tag>
            
            <tag> 简单线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记5</title>
      <link href="/2020/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05/"/>
      <url>/2020/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05/</url>
      
        <content type="html"><![CDATA[<h3 id="更多有关KNN的思考"><a href="#更多有关KNN的思考" class="headerlink" title="更多有关KNN的思考"></a>更多有关KNN的思考</h3><a id="more"></a><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724025657646.png" alt="image-20200724025657646"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724025727331.png" alt="image-20200724025727331"></p><h4 id="解释：可以把离绿球最近的三个蓝球的均值作为绿球的值，也可以将距离权重考虑进去求加权平均数。总之，这些都是回归问题。（sklearn其实已经封装好了KNN解决回归问题的库：KNeighborsRegressor）"><a href="#解释：可以把离绿球最近的三个蓝球的均值作为绿球的值，也可以将距离权重考虑进去求加权平均数。总之，这些都是回归问题。（sklearn其实已经封装好了KNN解决回归问题的库：KNeighborsRegressor）" class="headerlink" title="解释：可以把离绿球最近的三个蓝球的均值作为绿球的值，也可以将距离权重考虑进去求加权平均数。总之，这些都是回归问题。（sklearn其实已经封装好了KNN解决回归问题的库：KNeighborsRegressor）"></a>解释：可以把离绿球最近的三个蓝球的均值作为绿球的值，也可以将距离权重考虑进去求加权平均数。总之，这些都是回归问题。（sklearn其实已经封装好了KNN解决回归问题的库：KNeighborsRegressor）</h4><h3 id="它的缺点"><a href="#它的缺点" class="headerlink" title="它的缺点"></a>它的缺点</h3><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724030254962.png" alt="image-20200724030254962"></p></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724030519829.png" alt="image-20200724030519829"></p><ul><li><h4 id="解释：缺点2：拿上图红蓝绿球来举例，假如k-3，而其中两个是红色，一个是蓝色。按照KNN就会认定绿球为红色类，但事实上绿球的标签是蓝色类"><a href="#解释：缺点2：拿上图红蓝绿球来举例，假如k-3，而其中两个是红色，一个是蓝色。按照KNN就会认定绿球为红色类，但事实上绿球的标签是蓝色类" class="headerlink" title="解释：缺点2：拿上图红蓝绿球来举例，假如k=3，而其中两个是红色，一个是蓝色。按照KNN就会认定绿球为红色类，但事实上绿球的标签是蓝色类"></a>解释：缺点2：拿上图红蓝绿球来举例，假如k=3，而其中两个是红色，一个是蓝色。按照KNN就会认定绿球为红色类，但事实上绿球的标签是蓝色类</h4></li></ul></li><li><h4 id="缺点4："><a href="#缺点4：" class="headerlink" title="缺点4："></a>缺点4：<img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724031158354.png" alt="image-20200724031158354"></h4></li></ul><h3 id="机器学习流程回顾"><a href="#机器学习流程回顾" class="headerlink" title="机器学习流程回顾"></a>机器学习流程回顾</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724031341729.png" alt="image-20200724031341729"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Python </tag>
            
            <tag> KNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记4</title>
      <link href="/2020/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/"/>
      <url>/2020/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/</url>
      
        <content type="html"><![CDATA[<h3 id="数据归一化与标准化"><a href="#数据归一化与标准化" class="headerlink" title="数据归一化与标准化"></a>数据归一化与标准化</h3><a id="more"></a><h4 id="一，为什么要进行数据归一化或归一化"><a href="#一，为什么要进行数据归一化或归一化" class="headerlink" title="一，为什么要进行数据归一化或归一化"></a>一，为什么要进行数据归一化或归一化</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200723234524737.png" alt="image-20200723234524737"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200723234716174.png" alt="image-20200723234716174"></p><ul><li><h4 id="这张图表示样本之间的距离又会被肿瘤大小来主导"><a href="#这张图表示样本之间的距离又会被肿瘤大小来主导" class="headerlink" title="这张图表示样本之间的距离又会被肿瘤大小来主导"></a>这张图表示样本之间的距离又会被肿瘤大小来主导</h4></li><li><h4 id="因此我们需要统一进行数据归一化"><a href="#因此我们需要统一进行数据归一化" class="headerlink" title="因此我们需要统一进行数据归一化"></a>因此我们需要统一进行数据归一化</h4></li></ul><h3 id="二，什么是数据归一化或标准化"><a href="#二，什么是数据归一化或标准化" class="headerlink" title="二，什么是数据归一化或标准化"></a>二，什么是数据归一化或标准化</h3><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200723235207881.png" alt="image-20200723235207881"></p></li><li><h4 id="最值归一化（normalization）适用于分布有明显边界的情况。比如像工资就不能用最值归一化（工资无上限）"><a href="#最值归一化（normalization）适用于分布有明显边界的情况。比如像工资就不能用最值归一化（工资无上限）" class="headerlink" title="最值归一化（normalization）适用于分布有明显边界的情况。比如像工资就不能用最值归一化（工资无上限）"></a>最值归一化（normalization）适用于分布有明显边界的情况。比如像工资就不能用最值归一化（工资无上限）</h4></li></ul><h4 id="第二种是标准化"><a href="#第二种是标准化" class="headerlink" title="第二种是标准化"></a>第二种是标准化</h4><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724000028651.png" alt="image-20200724000028651"></p></li><li><h5 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h5><ul><li><h4 id="此处老师讲错，应该为标准化，而不是均值方差归一化"><a href="#此处老师讲错，应该为标准化，而不是均值方差归一化" class="headerlink" title="此处老师讲错，应该为标准化，而不是均值方差归一化"></a>此处老师讲错，应该为标准化，而不是均值方差归一化</h4></li><li><h4 id="s表示标准差。除非数据集有明确的边界，（初学情况）一般情况下都使用标准化"><a href="#s表示标准差。除非数据集有明确的边界，（初学情况）一般情况下都使用标准化" class="headerlink" title="s表示标准差。除非数据集有明确的边界，（初学情况）一般情况下都使用标准化"></a>s表示标准差。除非数据集有明确的边界，（初学情况）一般情况下都使用标准化</h4></li><li><h4 id="其实就是利用标准化处理将正态分布转为了标准正态分布"><a href="#其实就是利用标准化处理将正态分布转为了标准正态分布" class="headerlink" title="其实就是利用标准化处理将正态分布转为了标准正态分布"></a>其实就是利用标准化处理将正态分布转为了标准正态分布</h4></li><li><h4 id="深入了解请看https-www-jiqizhixin-com-articles-19070701-https-www-zhihu-com-question-56891433"><a href="#深入了解请看https-www-jiqizhixin-com-articles-19070701-https-www-zhihu-com-question-56891433" class="headerlink" title="深入了解请看https://www.jiqizhixin.com/articles/19070701 https://www.zhihu.com/question/56891433"></a>深入了解请看<a href="https://www.jiqizhixin.com/articles/19070701" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/19070701</a> <a href="https://www.zhihu.com/question/56891433" target="_blank" rel="noopener">https://www.zhihu.com/question/56891433</a></h4></li><li><h4 id="在线演示标准正态分布https-www-shuxuele-com-data-standard-normal-distribution-table-html"><a href="#在线演示标准正态分布https-www-shuxuele-com-data-standard-normal-distribution-table-html" class="headerlink" title="在线演示标准正态分布https://www.shuxuele.com/data/standard-normal-distribution-table.html"></a>在线演示标准正态分布<a href="https://www.shuxuele.com/data/standard-normal-distribution-table.html" target="_blank" rel="noopener">https://www.shuxuele.com/data/standard-normal-distribution-table.html</a></h4></li></ul></li></ul><h3 id="三，这两种的区别"><a href="#三，这两种的区别" class="headerlink" title="三，这两种的区别"></a>三，这两种的区别</h3><h5 id="最值归一化"><a href="#最值归一化" class="headerlink" title="最值归一化"></a>最值归一化</h5><ul><li>数据集必须得有边界，否则会出现偏差很悬殊的值</li></ul><h5 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724020154141.png" alt="image-20200724020154141"></p></li><li><h5 id="平均值是曲线的中心。这是曲线的最高点，因为大多数点都在平均值附近；"><a href="#平均值是曲线的中心。这是曲线的最高点，因为大多数点都在平均值附近；" class="headerlink" title="平均值是曲线的中心。这是曲线的最高点，因为大多数点都在平均值附近；"></a>平均值是曲线的中心。这是曲线的最高点，因为大多数点都在平均值附近；</h5></li><li><h5 id="标准化更符合统计学假设（自然界大多数符合正态分布）：对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。"><a href="#标准化更符合统计学假设（自然界大多数符合正态分布）：对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。" class="headerlink" title="标准化更符合统计学假设（自然界大多数符合正态分布）：对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。"></a>标准化更符合统计学假设（自然界大多数符合正态分布）：对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。</h5></li><li><h5 id="新的数据由于对方差进行了归一化，这时候每个维度的量纲其实已经等价了，每个维度都服从均值为0、方差1的正态分布，在计算距离的时候，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。"><a href="#新的数据由于对方差进行了归一化，这时候每个维度的量纲其实已经等价了，每个维度都服从均值为0、方差1的正态分布，在计算距离的时候，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。" class="headerlink" title="新的数据由于对方差进行了归一化，这时候每个维度的量纲其实已经等价了，每个维度都服从均值为0、方差1的正态分布，在计算距离的时候，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。"></a>新的数据由于对方差进行了归一化，这时候每个维度的量纲其实已经等价了，每个维度都服从均值为0、方差1的正态分布，在计算距离的时候，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。</h5></li></ul><h3 id="四，对测试数据集如何归一化或标准化"><a href="#四，对测试数据集如何归一化或标准化" class="headerlink" title="四，对测试数据集如何归一化或标准化"></a>四，对测试数据集如何归一化或标准化</h3><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724022251989.png" alt="image-20200724022251989"></p></li><li><h4 id="结论：因为一个模型是要投入生产生活中的，所以我们要让测试数据集也要使用训练数据集的均值和标准差"><a href="#结论：因为一个模型是要投入生产生活中的，所以我们要让测试数据集也要使用训练数据集的均值和标准差" class="headerlink" title="结论：因为一个模型是要投入生产生活中的，所以我们要让测试数据集也要使用训练数据集的均值和标准差"></a>结论：因为一个模型是要投入生产生活中的，所以我们要让测试数据集也要使用训练数据集的均值和标准差</h4></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200724022712834.png" alt="image-20200724022712834"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Python </tag>
            
            <tag> 数据预处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记3</title>
      <link href="/2020/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/"/>
      <url>/2020/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/</url>
      
        <content type="html"><![CDATA[<h3 id="KNN-K近邻算法（K-Nearest-Neighbors）"><a href="#KNN-K近邻算法（K-Nearest-Neighbors）" class="headerlink" title="KNN -K近邻算法（K-Nearest Neighbors）"></a>KNN -K近邻算法（K-Nearest Neighbors）</h3><a id="more"></a><h4 id="一：KNN—非常适合初学者入门的算法"><a href="#一：KNN—非常适合初学者入门的算法" class="headerlink" title="一：KNN—非常适合初学者入门的算法"></a>一：KNN—非常适合初学者入门的算法</h4><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707205845716.png" alt="image-20200707205845716"></li></ul><h5 id="1-什么是KNN"><a href="#1-什么是KNN" class="headerlink" title="1.什么是KNN"></a>1.什么是KNN</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707210428247.png" alt="image-20200707210428247"></p></li><li><ul><li>上图解释：红色是良性肿瘤，蓝色是恶性肿瘤，绿色是一个新的不确定的肿瘤。这里设K=3（后面会讲如何取K值），表示选取离绿色最近的三个肿瘤，然后这3个之间进行投票（以自己的标签投票），最终蓝色：红色==3:0.因此我们有很大的概率认为绿色也是恶性肿瘤。</li><li>因此，KNN表示在K个样本中，哪个样本越多，就表示目标样本有很大概率是同一类别。</li></ul></li></ul><h5 id="2-KNN的特性"><a href="#2-KNN的特性" class="headerlink" title="2.KNN的特性"></a>2.KNN的特性</h5><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707234150755.png" alt="image-20200707234150755"></li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707234227091.png" alt="image-20200707234227091"></li><li><ul><li>scikit_learn正是按这套流程进行的算法封装</li></ul></li></ul><h5 id="3-判断机器学习算法的性能"><a href="#3-判断机器学习算法的性能" class="headerlink" title="3.判断机器学习算法的性能"></a>3.判断机器学习算法的性能</h5><ul><li>考虑到真实环境下，将所有的原始数据都当成训练数据来训练模型是不恰当的，因此我们要进行训练和测试数据集切分(train test split)</li><li><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711013945748.png" alt="image-20200710224630969"></li></ul></li><li><p>注意：这里补充一下seed随机种子的直观理解：其实，设置seed（）里的数字就相当于设置了一个盛有随机数的“聚宝盆”，一个数字代表一个“聚宝盆”，当我们在seed（）的括号里设置相同的seed，“聚宝盆”就是一样的，那当然每次拿出的随机数就会相同（不要觉得就是从里面随机取数字，只要设置的seed相同取出地随机数就一样）。如果不设置seed，则每次会生成不同的随机数。（注：seed括号里的数值基本可以随便设置哦）<br>————————————————<br>版权声明：本文为CSDN博主「白糖炒栗子~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/weixin_41571493/article/details/80549833" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41571493/article/details/80549833</a></p><ul><li>np.random.seed(0)<br>np.random.rand(4,3)<br>Out[362]:<br>array([[0.5488135 , 0.71518937, 0.60276338],<pre><code>   [0.54488318, 0.4236548 , 0.64589411],   [0.43758721, 0.891773  , 0.96366276],   [0.38344152, 0.79172504, 0.52889492]])</code></pre>np.random.seed(0)<br>np.random.rand(4,3)<br>Out[364]:<br>array([[0.5488135 , 0.71518937, 0.60276338],<pre><code>   [0.54488318, 0.4236548 , 0.64589411],   [0.43758721, 0.891773  , 0.96366276],   [0.38344152, 0.79172504, 0.52889492]])</code></pre></li></ul></li><li><p>分类准确度：accuracy：详见jupyter</p></li><li><p>超参数</p><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200710224630969.png" alt="image-20200711013945748"></p></li><li><p>如何寻找好的超参数</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711014228711.png" alt="image-20200711014228711"></p><ul><li><p>KNN中其实除了K还有一个超参数，那就是距离的权重（它是距离的倒数）</p><ul><li><p>设红和蓝与绿色的距离分别为1,3,4</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711021206076.png" alt="image-20200711021206076"></p><ul><li><p>若有三个类别，而k也等于3，那么就有可能出现平票的情况</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711021519742.png" alt="image-20200711021519742"></p></li><li><p>补充：明可夫斯基距离</p><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200711023118851.png" alt="image-20200711023118851"><ul><li>当p=1时是曼哈顿距离</li><li>当p=2时是欧拉距离</li><li>当p&gt;2时是其他距离</li></ul></li><li>更多的距离定义（暂不做深入研究，只是了解）<ul><li></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200723233646364.png" alt="image-20200723233646364"></p><ul><li><h5 id="我们可以使用sklearn的网格搜索来进行超参数搜索，具体看jupyter上的‘06-Validation-and-Cross-Validation’中的回顾网格搜索"><a href="#我们可以使用sklearn的网格搜索来进行超参数搜索，具体看jupyter上的‘06-Validation-and-Cross-Validation’中的回顾网格搜索" class="headerlink" title="我们可以使用sklearn的网格搜索来进行超参数搜索，具体看jupyter上的‘06-Validation-and-Cross-Validation’中的回顾网格搜索"></a>我们可以使用sklearn的网格搜索来进行超参数搜索，具体看jupyter上的‘06-Validation-and-Cross-Validation’中的回顾网格搜索</h5></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Python </tag>
            
            <tag> KNN </tag>
            
            <tag> 超参数与网格搜索 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记2</title>
      <link href="/2020/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/"/>
      <url>/2020/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h3><h4 id="一：机器学习的数据"><a href="#一：机器学习的数据" class="headerlink" title="一：机器学习的数据"></a>一：机器学习的数据</h4><a id="more"></a><h5 id="1-以鸢尾花数据来举例"><a href="#1-以鸢尾花数据来举例" class="headerlink" title="1.以鸢尾花数据来举例"></a>1.以鸢尾花数据来举例</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706180226511.png" alt="image-20200706180226511"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706180341873_1.png" alt="image-20200706180341873_1"></p><ul><li>注意：种类是用0,1,2来表示的。</li></ul><h5 id="2-从该数据集中引出的基本概念"><a href="#2-从该数据集中引出的基本概念" class="headerlink" title="2.从该数据集中引出的基本概念"></a>2.从该数据集中引出的基本概念</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706181305828.png" alt="image-20200706181305828"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706221107817_1.png" alt="image-20200706221107817_1"></p><p>注意:</p><ul><li>在机器学习中，大写字母表示矩阵，小写字母表示向量</li><li>标记y是机器学习真正要学习（预测）的</li><li>一般来说，特征向量都是列向量，所以如果表示整个特征集，需要将每个特征向量转置为行向量（看上图理解）</li></ul><h5 id="3-为了可视化方便，取鸢尾花的前两个特征（长度，宽度）绘图"><a href="#3-为了可视化方便，取鸢尾花的前两个特征（长度，宽度）绘图" class="headerlink" title="3.为了可视化方便，取鸢尾花的前两个特征（长度，宽度）绘图"></a>3.为了可视化方便，取鸢尾花的前两个特征（长度，宽度）绘图</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706221859929.png" alt="image-20200706221859929"></p><p>解释：</p><ul><li>红蓝代表两种鸢尾花，一个点落入哪块儿区域就说明是哪种类型的鸢尾花。</li><li>玩具数据集的特征空间切分可能是直线，但是现实中的数据集一般不可能用直线正好切分</li><li>有时若在高维空间不好理解，可以类比为低维度空间理解，然后再迁移到高维空间 </li></ul><h5 id="4-特征不一定是具有语义的，比如下图（像素点指每一个小矩形）"><a href="#4-特征不一定是具有语义的，比如下图（像素点指每一个小矩形）" class="headerlink" title="4.特征不一定是具有语义的，比如下图（像素点指每一个小矩形）"></a>4.特征不一定是具有语义的，比如下图（像素点指每一个小矩形）</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706223051356.png" alt="image-20200706223051356"></p><h4 id="二：机器学习（监督学习）的基本任务"><a href="#二：机器学习（监督学习）的基本任务" class="headerlink" title="二：机器学习（监督学习）的基本任务"></a>二：机器学习（监督学习）的基本任务</h4><h5 id="1-机器学习（监督学习）的基本任务主要分为：分类和回归"><a href="#1-机器学习（监督学习）的基本任务主要分为：分类和回归" class="headerlink" title="1.机器学习（监督学习）的基本任务主要分为：分类和回归"></a>1.机器学习（监督学习）的基本任务主要分为：分类和回归</h5><h5 id="2-分类任务："><a href="#2-分类任务：" class="headerlink" title="2.分类任务："></a>2.分类任务：</h5><ul><li><p>二分类：如垃圾邮件判断</p></li><li><p>多分类：</p><ul><li>如数字识别，图像识别</li><li>其实很多复杂问题可以转换为多分类问题，比如下围棋，无人驾驶</li><li>一些算法只支持二分类，但可以转换为多分类</li><li>但是多分类可以转换为二分类</li><li>有一些算法天然支持多分类任务</li></ul></li><li><p>多标签分类：属于cv方向</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706224809022.png" alt="image-20200706224809022"></p></li></ul><h5 id="3-回归任务：结果是一个连续值，而不是一个类别"><a href="#3-回归任务：结果是一个连续值，而不是一个类别" class="headerlink" title="3.回归任务：结果是一个连续值，而不是一个类别"></a>3.回归任务：结果是一个连续值，而不是一个类别</h5><ul><li>如房屋价格，市场分析，学生成绩，股票价格</li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706225329441.png" alt="image-20200706225329441"></p><ul><li>一些情况下，回归任务可以转换为分类任务<ul><li>如要预测学生成绩，可以预测具体成绩（回归），也可以预测成绩评级（分类）</li></ul></li></ul><h4 id="三：机器学习的分类"><a href="#三：机器学习的分类" class="headerlink" title="三：机器学习的分类"></a>三：机器学习的分类</h4><ul><li><p>监督学习：给机器的训练数据有标签（y）</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706230605230.png" alt="image-20200706230605230"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706230631948.png" alt="image-20200706230631948"></p></li><li><p>非监督学习：给机器的训练数据没有任何标签</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706230857321.png" alt="image-20200706230857321"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706231214211.png" alt="image-20200706231214211"></p><ul><li>特征压缩指在尽量小的损失下，将高维度数据转换为低维度数据</li><li>降维处理的意义：方便可视化，因为人类无法理解四维以上的空间</li></ul><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706231654647.png" alt="image-20200706231654647"></p></li><li><p>半监督学习：一部分数据有标签，另一部分数据没有，更符合现实情况</p><ul><li>通常都是先使用无监督学习手段对数据做处理，之后用监督学习手段做模型的训练与预测</li></ul></li><li><p>强化学习</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706233156709_1.png" alt="image-20200706233156709_1"></p><ul><li>Agent（指算法）会根据环境（environment）来做出一些行为（action），然后Agent会收到一些反馈，有时是奖赏（reward），有时是惩罚。根据这些反馈Agent会改进自己的行为（action）模式，在一轮轮迭代中逐渐增强自己的智能</li></ul></li></ul><h4 id="四：机器学习的其他分类"><a href="#四：机器学习的其他分类" class="headerlink" title="四：机器学习的其他分类"></a>四：机器学习的其他分类</h4><ul><li><p>批量学习（离线学习）（Batch learning）</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706234715003_1.png" alt="image-20200706234715003_1"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706234926469.png" alt="image-20200706234926469"></p><ul><li>如股票预测，因为其每时每刻都在变化</li><li>而垃圾邮件分类就可以定时重新批量学习，因为变化频率低 </li></ul></li><li><p>在线学习（Online Learning）</p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707011631601.png" alt="image-20200707011631601"></p><ul><li>输入样例输入模型以后会输出预测值，同时输入样例里的真实值也会输出 ，最终真实值，预测值，它们之间的差异等构成新的学习资料传给算法进行算法修正。</li><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707012338369.png" alt="image-20200707012338369"></li></ul></li><li><p>批量学习（离线学习）</p></li><li><p>参数学习（parametric learning）</p><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707012614143.png" alt="image-20200707012614143"><ul><li>我们假设该模型的的函数是线性回归，然后想办法找到最优的a和b这两个参数</li><li>一旦学到了参数，就不再需要原有的数据集</li></ul></li></ul></li><li><p>非参数学习（nonparametric learning）</p><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707013028462.png" alt="image-20200707013028462"></li><li>详细可参考<a href="https://www.cnblogs.com/wjunneng/p/9126906.html" target="_blank" rel="noopener">https://www.cnblogs.com/wjunneng/p/9126906.html</a></li></ul></li></ul><h4 id="五：和机器学习相关的哲学思考"><a href="#五：和机器学习相关的哲学思考" class="headerlink" title="五：和机器学习相关的哲学思考"></a>五：和机器学习相关的哲学思考</h4><h5 id="1-问题引出："><a href="#1-问题引出：" class="headerlink" title="1.问题引出："></a>1.问题引出：</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707013704925_1.png" alt="image-20200707013704925_1"></p></li><li><ul><li>纵坐标表示算法准确度，横坐标表示数据规模（数据量），可以看到随着数据量的增大，4种算法的准确度都处于上升状态，最终的差距越来越小</li></ul></li></ul><h5 id="2-争论点："><a href="#2-争论点：" class="headerlink" title="2.争论点："></a>2.争论点：</h5><ul><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707014212423.png" alt="image-20200707014212423"></p></li><li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707014340413.png" alt="image-20200707014340413"></p></li></ul><h5 id="3-总结："><a href="#3-总结：" class="headerlink" title="3.总结："></a>3.总结：</h5><ul><li><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200707014718816_1.png" alt="image-20200707014718816_1"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Python </tag>
            
            <tag> 机器学习基础概念 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记1</title>
      <link href="/2020/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/"/>
      <url>/2020/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习笔记"><a href="#机器学习笔记" class="headerlink" title="机器学习笔记"></a>机器学习笔记</h1><a id="more"></a><h3 id="一：什么是机器学习"><a href="#一：什么是机器学习" class="headerlink" title="一：什么是机器学习"></a>一：什么是机器学习</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706165617797.png" alt="image-20200706165617797"></p><h4 id="首先先了解人类如何学习（经验学习）"><a href="#首先先了解人类如何学习（经验学习）" class="headerlink" title="首先先了解人类如何学习（经验学习）"></a>首先先了解人类如何学习（经验学习）</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706170401290.png" alt="image-20200706170401290"></p><h4 id="机器学习和人类学习是非常相似的"><a href="#机器学习和人类学习是非常相似的" class="headerlink" title="机器学习和人类学习是非常相似的"></a>机器学习和人类学习是非常相似的</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706170712331.png" alt="image-20200706170712331"></p><h3 id="二：机器学习应用"><a href="#二：机器学习应用" class="headerlink" title="二：机器学习应用"></a>二：机器学习应用</h3><h4 id="1-当前应用"><a href="#1-当前应用" class="headerlink" title="1.当前应用"></a>1.当前应用</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706171220946.png" alt="image-20200706171220946"></p><h4 id="2-未来应用"><a href="#2-未来应用" class="headerlink" title="2.未来应用"></a>2.未来应用</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706171619513.png" alt="image-20200706171619513"></p><h3 id="三：机器学习的定位"><a href="#三：机器学习的定位" class="headerlink" title="三：机器学习的定位"></a>三：机器学习的定位</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706172204133.png" alt="image-20200706172204133"></p><h4 id="不要孤立的看待机器学习，它和很多领域都有密切的联系"><a href="#不要孤立的看待机器学习，它和很多领域都有密切的联系" class="headerlink" title="不要孤立的看待机器学习，它和很多领域都有密切的联系"></a>不要孤立的看待机器学习，它和很多领域都有密切的联系</h4><h3 id="四：机器学习的框架"><a href="#四：机器学习的框架" class="headerlink" title="四：机器学习的框架"></a>四：机器学习的框架</h3><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706172446522.png" alt="image-20200706172446522"></p><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200706172809977.png" alt="image-20200706172809977"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Python </tag>
            
            <tag> 机器学习基础概念 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
