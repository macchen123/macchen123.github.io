<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"macchen123.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true,"position":"right"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="多项式回归">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记11">
<meta property="og:url" content="http://macchen123.github.io/2020/08/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011/index.html">
<meta property="og:site_name" content="MaCcHen的个人小站">
<meta property="og:description" content="多项式回归">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200815221650806.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816004625821.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816210229364.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816214627319.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816214657728.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823165520156.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823170347284.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823182409604.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823182632636.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823184540816.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823185218727.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823185258191.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823221542723.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823191041828.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823191117875.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823232503791.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826102708414.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826211643626.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826212135574.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826213339759.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826213439382.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826214412564.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826214730165.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215536491.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215703508.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215846131.png">
<meta property="og:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826220552522.png">
<meta property="article:published_time" content="2020-08-15T14:13:10.000Z">
<meta property="article:modified_time" content="2020-08-26T14:14:30.117Z">
<meta property="article:author" content="macchen">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="多项式回归">
<meta property="article:tag" content="Pipeline">
<meta property="article:tag" content="过拟合与欠拟合">
<meta property="article:tag" content="学习曲线">
<meta property="article:tag" content="交叉验证">
<meta property="article:tag" content="偏差与方差">
<meta property="article:tag" content="模型正则化">
<meta property="article:tag" content="岭回归、LASSO回归">
<meta property="article:tag" content="弹性网">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200815221650806.png">

<link rel="canonical" href="http://macchen123.github.io/2020/08/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>机器学习笔记11 | MaCcHen的个人小站</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?be46ae65323b06abe5d4115d59e20a64";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">MaCcHen的个人小站</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">谨言慎行，格物致知</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://macchen123.github.io/2020/08/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="macchen">
      <meta itemprop="description" content="学习强国">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MaCcHen的个人小站">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习笔记11
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-15 22:13:10" itemprop="dateCreated datePublished" datetime="2020-08-15T22:13:10+08:00">2020-08-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-26 22:14:30" itemprop="dateModified" datetime="2020-08-26T22:14:30+08:00">2020-08-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><a id="more"></a>
<h4 id="一、什么是多项式回归"><a href="#一、什么是多项式回归" class="headerlink" title="一、什么是多项式回归"></a>一、什么是多项式回归</h4><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200815221650806.png" alt="image-20200815221650806"></p>
<h5 id="以上图来说明-该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的-ax-2-，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归"><a href="#以上图来说明-该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的-ax-2-，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归" class="headerlink" title="以上图来说明:该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的$ax^2$，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归"></a>以上图来说明:该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的$ax^2$，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归</h5><h5 id="本质上相当于对原数据集进行升维来更好的拟合高维数据"><a href="#本质上相当于对原数据集进行升维来更好的拟合高维数据" class="headerlink" title="本质上相当于对原数据集进行升维来更好的拟合高维数据"></a>本质上相当于对原数据集进行升维来更好的拟合高维数据</h5></li>
</ul>
<h4 id="二，关于sklearn中的PolynomialFeatures"><a href="#二，关于sklearn中的PolynomialFeatures" class="headerlink" title="二，关于sklearn中的PolynomialFeatures"></a>二，关于sklearn中的PolynomialFeatures</h4><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816004625821.png" alt="image-20200816004625821"></p>
<h5 id="设degree-i，则它会生成所有小于等于i的多项式特征，构成新的特征集"><a href="#设degree-i，则它会生成所有小于等于i的多项式特征，构成新的特征集" class="headerlink" title="设degree=i，则它会生成所有小于等于i的多项式特征，构成新的特征集"></a>设degree=i，则它会生成所有小于等于i的多项式特征，构成新的特征集</h5></li>
</ul>
<h4 id="三、Pipeline"><a href="#三、Pipeline" class="headerlink" title="三、Pipeline"></a>三、Pipeline</h4><ul>
<li><h5 id="拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步"><a href="#拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步" class="headerlink" title="拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步"></a>拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步</h5></li>
<li><h5 id="具体看jupyter中的代码"><a href="#具体看jupyter中的代码" class="headerlink" title="具体看jupyter中的代码"></a>具体看jupyter中的代码</h5></li>
</ul>
<h4 id="四、过拟合和欠拟合，以及分离数据集"><a href="#四、过拟合和欠拟合，以及分离数据集" class="headerlink" title="四、过拟合和欠拟合，以及分离数据集"></a>四、过拟合和欠拟合，以及分离数据集</h4><h4 id="1-泛化能力"><a href="#1-泛化能力" class="headerlink" title="1.泛化能力"></a>1.泛化能力</h4><ul>
<li><h5 id="泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱"><a href="#泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱" class="headerlink" title="泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱"></a>泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱</h5></li>
</ul>
<h4 id="2-过拟合"><a href="#2-过拟合" class="headerlink" title="2.过拟合"></a>2.过拟合</h4><ul>
<li><h5 id="就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测-区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。"><a href="#就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测-区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。" class="headerlink" title="就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。"></a>就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测/区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。</h5></li>
</ul>
<h4 id="3-欠拟合"><a href="#3-欠拟合" class="headerlink" title="3.欠拟合"></a>3.欠拟合</h4><ul>
<li><h5 id="测试样本的特性没有学到-模型不能完整的表述数据关系-，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差"><a href="#测试样本的特性没有学到-模型不能完整的表述数据关系-，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差" class="headerlink" title="测试样本的特性没有学到(模型不能完整的表述数据关系)，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差"></a>测试样本的特性没有学到(模型不能完整的表述数据关系)，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差</h5></li>
</ul>
<h4 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h4><ul>
<li><h5 id="总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行"><a href="#总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行" class="headerlink" title="总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行"></a>总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行</h5></li>
<li><p><strong>那么如何衡量模型的泛化能力强弱呢？那就是采用分离数据集的方法来衡量。这就是分离数据集更深层的意义</strong></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816210229364.png" alt="image-20200816210229364"></p>
</li>
</ul>
<h4 id="五、学习曲线（可视化查看模型的欠拟合、过拟合）"><a href="#五、学习曲线（可视化查看模型的欠拟合、过拟合）" class="headerlink" title="五、学习曲线（可视化查看模型的欠拟合、过拟合）"></a>五、学习曲线（可视化查看模型的欠拟合、过拟合）</h4><ul>
<li><h5 id="定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力"><a href="#定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力" class="headerlink" title="定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力"></a>定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力</h5></li>
<li><h5 id="学习曲线的解释："><a href="#学习曲线的解释：" class="headerlink" title="学习曲线的解释："></a>学习曲线的解释：</h5><ul>
<li><h5 id="随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；"><a href="#随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；" class="headerlink" title="随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；"></a>随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；</h5></li>
<li><h5 id="随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失"><a href="#随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失" class="headerlink" title="随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失"></a>随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失</h5></li>
</ul>
</li>
<li><h5 id="欠拟合，最佳拟合，过拟合的对比"><a href="#欠拟合，最佳拟合，过拟合的对比" class="headerlink" title="欠拟合，最佳拟合，过拟合的对比"></a>欠拟合，最佳拟合，过拟合的对比</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816214627319.png" alt="image-20200816214627319"></p>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200816214657728.png" alt="image-20200816214657728"></p>
</li>
</ul>
<h4 id="六、验证数据集与交叉验证"><a href="#六、验证数据集与交叉验证" class="headerlink" title="六、验证数据集与交叉验证"></a>六、验证数据集与交叉验证</h4><h5 id="1、验证数据集："><a href="#1、验证数据集：" class="headerlink" title="1、验证数据集："></a>1、验证数据集：</h5><ul>
<li><h5 id="由于train-test-split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集"><a href="#由于train-test-split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集" class="headerlink" title="由于train_test_split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集"></a>由于train_test_split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823165520156.png" alt="image-20200823165520156"></p>
<h5 id="由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集"><a href="#由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集" class="headerlink" title="由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集"></a>由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集</h5></li>
</ul>
</li>
<li><h5 id="但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。"><a href="#但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。" class="headerlink" title="但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。"></a>但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。</h5></li>
</ul>
<h5 id="2、交叉验证："><a href="#2、交叉验证：" class="headerlink" title="2、交叉验证："></a>2、交叉验证：</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823170347284.png" alt="image-20200823170347284"></p>
<ul>
<li><h5 id="把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。"><a href="#把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。" class="headerlink" title="把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。"></a>把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。</h5></li>
<li><h5 id="使用交叉验证进行调参优化得到的score可能会比train-test-split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据"><a href="#使用交叉验证进行调参优化得到的score可能会比train-test-split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据" class="headerlink" title="使用交叉验证进行调参优化得到的score可能会比train_test_split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据"></a>使用交叉验证进行调参优化得到的score可能会比train_test_split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据</h5></li>
</ul>
</li>
<li><h5 id="交叉验证总结："><a href="#交叉验证总结：" class="headerlink" title="交叉验证总结："></a>交叉验证总结：</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823182409604.png" alt="image-20200823182409604"></p>
<h5 id="但是这样训练出的模型是可信赖的"><a href="#但是这样训练出的模型是可信赖的" class="headerlink" title="但是这样训练出的模型是可信赖的"></a>但是这样训练出的模型是可信赖的</h5></li>
<li><h5 id="假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据"><a href="#假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据" class="headerlink" title="假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据"></a>假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823182632636.png" alt="image-20200823182632636"></p>
</li>
</ul>
</li>
</ul>
<h4 id="七、偏方差权衡（Bias-Variance-Trade-off）"><a href="#七、偏方差权衡（Bias-Variance-Trade-off）" class="headerlink" title="七、偏方差权衡（Bias Variance Trade off）"></a>七、偏方差权衡（Bias Variance Trade off）</h4><h5 id="1、偏差与方差："><a href="#1、偏差与方差：" class="headerlink" title="1、偏差与方差："></a>1、偏差与方差：</h5><h5 id="可以参考："><a href="#可以参考：" class="headerlink" title="可以参考："></a>可以参考：</h5><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">作者：「已注销」</span><br><span class="line">链接：https://www.zhihu.com/question/20448464/answer/765401873</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br></pre></td></tr></table></figure>
<ul>
<li><h5 id="偏差："><a href="#偏差：" class="headerlink" title="偏差："></a>偏差：</h5><ul>
<li><h5 id="描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。"><a href="#描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。" class="headerlink" title="描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。"></a>描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。</h5></li>
</ul>
</li>
<li><h5 id="方差："><a href="#方差：" class="headerlink" title="方差："></a>方差：</h5><ul>
<li><h5 id="描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。"><a href="#描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。" class="headerlink" title="描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。"></a>描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。</h5></li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823184540816.png" alt="image-20200823184540816"></p>
<h5 id="2、模型误差："><a href="#2、模型误差：" class="headerlink" title="2、模型误差："></a>2、模型误差：</h5><ul>
<li><h5 id="模型误差-偏差（Bias）-方差（Variance）-不可避免的误差（如噪声）"><a href="#模型误差-偏差（Bias）-方差（Variance）-不可避免的误差（如噪声）" class="headerlink" title="模型误差 = 偏差（Bias）+ 方差（Variance）+ 不可避免的误差（如噪声）"></a>模型误差 = 偏差（Bias）+ 方差（Variance）+ 不可避免的误差（如噪声）</h5></li>
</ul>
<h5 id="3、产生偏差和方差的主要原因"><a href="#3、产生偏差和方差的主要原因" class="headerlink" title="3、产生偏差和方差的主要原因"></a>3、产生偏差和方差的主要原因</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823185218727.png" alt="image-20200823185218727"></p>
</li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823185258191.png" alt="image-20200823185258191"></p>
</li>
</ul>
<h5 id="4、模型与偏差和方差："><a href="#4、模型与偏差和方差：" class="headerlink" title="4、模型与偏差和方差："></a>4、模型与偏差和方差：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823221542723.png" alt="image-20200823221542723"></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">一、为什么knn的k越小，模型越复杂（复杂指的就是过拟合）？：</span><br><span class="line">假设有一个男女性别的数据集，k=4,。在训练数据集中的，有一个新样本s（s是男），通过knn得出 男：女=3:1，意思是有75%的概率认为s是男，25%的概率认为s是女。但是为了追求更高的准确度（极端一点），让k=1，这样s的属性就会由一个离它最近的样本决定（假设该训练数据集离s最近的一个样本是男），这样我们就有100%的概率认为s为男。然而在测试数据集中，若有一个新样本j（j是男），但是离j最最近的样本是女，所以这个模型预测就出现了错误。这就是为了追求测试数据集的准确度而不具有泛化性（普遍性）导致了过拟合</span><br><span class="line"></span><br><span class="line">二、为什么knn是高方差算法？：</span><br><span class="line">可以参考：https://www.cnblogs.com/solong1989/p/9603818.html</span><br><span class="line"></span><br><span class="line">三、过拟合和方差：</span><br><span class="line">https://blog.csdn.net/liweibin1994/article/details/76859743</span><br><span class="line">假设对训练数据集过拟合，那么测试数据集（或验证数据集）上的样本点有的预测准确，有的预测不准确，这样造成预测结果之前离散程度很大，所以过拟合就是高方差</span><br></pre></td></tr></table></figure>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823191041828.png" alt="image-20200823191041828"></p>
<h5 id="5、偏差与方差的关系："><a href="#5、偏差与方差的关系：" class="headerlink" title="5、偏差与方差的关系："></a>5、偏差与方差的关系：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823191117875.png" alt="image-20200823191117875"></p>
<h5 id="6、机器学习-算法层面-的主要挑战"><a href="#6、机器学习-算法层面-的主要挑战" class="headerlink" title="6、机器学习(算法层面)的主要挑战"></a>6、机器学习(算法层面)的主要挑战</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200823232503791.png" alt="image-20200823232503791"></p>
<h4 id="八、模型正则化（Regularization）"><a href="#八、模型正则化（Regularization）" class="headerlink" title="八、模型正则化（Regularization）"></a>八、模型正则化（Regularization）</h4><ul>
<li><h5 id="模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。"><a href="#模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。" class="headerlink" title="模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。"></a>模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826102708414.png" alt="image-20200826102708414"></p>
<h5 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h5><ul>
<li><h5 id="要想使损失函数尽可能小，就必须让-theta-尽可能小"><a href="#要想使损失函数尽可能小，就必须让-theta-尽可能小" class="headerlink" title="要想使损失函数尽可能小，就必须让$\theta$尽可能小"></a>要想使损失函数尽可能小，就必须让$\theta$尽可能小</h5></li>
<li><h5 id="损失函数后面的正则化项求和是从1到n的，这意味着不需要将-theta-0-加入，因为-theta-0-是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）"><a href="#损失函数后面的正则化项求和是从1到n的，这意味着不需要将-theta-0-加入，因为-theta-0-是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）" class="headerlink" title="损失函数后面的正则化项求和是从1到n的，这意味着不需要将$\theta_0$加入，因为$\theta_0$是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）"></a>损失函数后面的正则化项求和是从1到n的，这意味着不需要将$\theta_0$加入，因为$\theta_0$是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）</h5></li>
<li><h5 id="第二项的1-2是为了求导好约分，加不加都行"><a href="#第二项的1-2是为了求导好约分，加不加都行" class="headerlink" title="第二项的1/2是为了求导好约分，加不加都行"></a>第二项的1/2是为了求导好约分，加不加都行</h5></li>
<li><h5 id="alpha-是一个超参数，它指的是-theta-的优化程度占整个损失函数的多少，很显然，-alpha-越大越好，但是在实际中我们要找一个能平衡经验误差项-MSE-和正则化项的-alpha-。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。"><a href="#alpha-是一个超参数，它指的是-theta-的优化程度占整个损失函数的多少，很显然，-alpha-越大越好，但是在实际中我们要找一个能平衡经验误差项-MSE-和正则化项的-alpha-。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。" class="headerlink" title="$\alpha$是一个超参数，它指的是$\theta$的优化程度占整个损失函数的多少，很显然，$\alpha$越大越好，但是在实际中我们要找一个能平衡经验误差项(MSE)和正则化项的$\alpha$。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。"></a>$\alpha$是一个超参数，它指的是$\theta$的优化程度占整个损失函数的多少，很显然，$\alpha$越大越好，但是在实际中我们要找一个能平衡经验误差项(MSE)和正则化项的$\alpha$。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。</h5></li>
</ul>
</li>
</ul>
<h4 id="九、岭回归（Ridge-Regression）"><a href="#九、岭回归（Ridge-Regression）" class="headerlink" title="九、岭回归（Ridge Regression）"></a>九、岭回归（Ridge Regression）</h4><ul>
<li><h5 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826211643626.png" alt="image-20200826211643626"></p>
</li>
</ul>
<h4 id="十、LASSO回归（LASSO-Regression）"><a href="#十、LASSO回归（LASSO-Regression）" class="headerlink" title="十、LASSO回归（LASSO Regression）"></a>十、LASSO回归（LASSO Regression）</h4><ul>
<li><h5 id="比较Ridge和LASSO：为什么随着-alpha-的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线"><a href="#比较Ridge和LASSO：为什么随着-alpha-的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线" class="headerlink" title="比较Ridge和LASSO：为什么随着$\alpha$的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线"></a>比较Ridge和LASSO：为什么随着$\alpha$的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826212135574.png" alt="image-20200826212135574"></p>
<h5 id="下面我们从梯度下降的角度来解释："><a href="#下面我们从梯度下降的角度来解释：" class="headerlink" title="下面我们从梯度下降的角度来解释："></a>下面我们从梯度下降的角度来解释：</h5><ul>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826213339759.png" alt="image-20200826213339759"></p>
</li>
<li><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826213439382.png" alt="image-20200826213439382"></p>
<h5 id="因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些-theta-为零。这也说明了为什么叫岭回归，因为它就像下山一样。"><a href="#因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些-theta-为零。这也说明了为什么叫岭回归，因为它就像下山一样。" class="headerlink" title="因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些$\theta$为零。这也说明了为什么叫岭回归，因为它就像下山一样。"></a>因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些$\theta$为零。这也说明了为什么叫岭回归，因为它就像下山一样。</h5></li>
</ul>
</li>
<li><h5 id="LASSO回归总结"><a href="#LASSO回归总结" class="headerlink" title="LASSO回归总结"></a>LASSO回归总结</h5><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826214412564.png" alt="image-20200826214412564"></p>
<h5 id="但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。"><a href="#但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。" class="headerlink" title="但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。"></a>但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。</h5></li>
</ul>
<h4 id="十一、L1，L2正则"><a href="#十一、L1，L2正则" class="headerlink" title="十一、L1，L2正则"></a>十一、L1，L2正则</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826214730165.png" alt="image-20200826214730165"></p>
<ul>
<li><h5 id="其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。"><a href="#其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。" class="headerlink" title="其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。"></a>其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。</h5></li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215536491.png" alt="image-20200826215536491"></p>
<ul>
<li><h5 id="基于上面的思想，我们把明可夫斯基距离进行一下如下变换："><a href="#基于上面的思想，我们把明可夫斯基距离进行一下如下变换：" class="headerlink" title="基于上面的思想，我们把明可夫斯基距离进行一下如下变换："></a>基于上面的思想，我们把明可夫斯基距离进行一下如下变换：</h5></li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215703508.png" alt="image-20200826215703508"></p>
<ul>
<li><h5 id="关于图右边开不开根号都不会影响到结果"><a href="#关于图右边开不开根号都不会影响到结果" class="headerlink" title="关于图右边开不开根号都不会影响到结果"></a>关于图右边开不开根号都不会影响到结果</h5></li>
</ul>
<p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826215846131.png" alt="image-20200826215846131"></p>
<ul>
<li><h5 id="L0正则项的原理是使-theta-的非零项的数量尽量少，但是必须使用穷举法来得出-theta-为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。"><a href="#L0正则项的原理是使-theta-的非零项的数量尽量少，但是必须使用穷举法来得出-theta-为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。" class="headerlink" title="L0正则项的原理是使$\theta$的非零项的数量尽量少，但是必须使用穷举法来得出$\theta$为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。"></a>L0正则项的原理是使$\theta$的非零项的数量尽量少，但是必须使用穷举法来得出$\theta$为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。</h5></li>
</ul>
<h4 id="十二、弹性网"><a href="#十二、弹性网" class="headerlink" title="十二、弹性网"></a>十二、弹性网</h4><p><img src="https://gitee.com/macljc/myimg/raw/master/imgs/image-20200826220552522.png" alt="image-20200826220552522"></p>
<ul>
<li><h5 id="机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。"><a href="#机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。" class="headerlink" title="机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。"></a>机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。</h5></li>
<li><h5 id="弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。"><a href="#弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。" class="headerlink" title="弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。"></a>弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。</h5></li>
<li><h5 id="通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。"><a href="#通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。" class="headerlink" title="通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。"></a>通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。</h5></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92/" rel="tag"># 多项式回归</a>
              <a href="/tags/Pipeline/" rel="tag"># Pipeline</a>
              <a href="/tags/%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88/" rel="tag"># 过拟合与欠拟合</a>
              <a href="/tags/%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF/" rel="tag"># 学习曲线</a>
              <a href="/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/" rel="tag"># 交叉验证</a>
              <a href="/tags/%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/" rel="tag"># 偏差与方差</a>
              <a href="/tags/%E6%A8%A1%E5%9E%8B%E6%AD%A3%E5%88%99%E5%8C%96/" rel="tag"># 模型正则化</a>
              <a href="/tags/%E5%B2%AD%E5%9B%9E%E5%BD%92%E3%80%81LASSO%E5%9B%9E%E5%BD%92/" rel="tag"># 岭回归、LASSO回归</a>
              <a href="/tags/%E5%BC%B9%E6%80%A7%E7%BD%91/" rel="tag"># 弹性网</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010/" rel="prev" title="机器学习笔记10">
      <i class="fa fa-chevron-left"></i> 机器学习笔记10
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/08/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012/" rel="next" title="机器学习笔记12">
      机器学习笔记12 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
      
      <!-- Insert clustrmaps.com -->
      <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=2_M6gOO_uk_5ftA8MDj1GtyvCyzGqPStnw1k5TNBneU&cl=ffffff&w=a"></script>


      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#多项式回归"><span class="nav-number">1.</span> <span class="nav-text">多项式回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#一、什么是多项式回归"><span class="nav-number">1.1.</span> <span class="nav-text">一、什么是多项式回归</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#以上图来说明-该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的-ax-2-，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归"><span class="nav-number">1.1.1.</span> <span class="nav-text">以上图来说明:该数据只有一个特征x，但是我们要将一些多项式当作是另一些特征，比如图上的$ax^2$，对于式子本身来说依然是具有线性关系的，但是从x的角度来看就是非线性方程。这样相比普通的线性回归能更好的拟合非线性数据。这就是多项式回归</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#本质上相当于对原数据集进行升维来更好的拟合高维数据"><span class="nav-number">1.1.2.</span> <span class="nav-text">本质上相当于对原数据集进行升维来更好的拟合高维数据</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二，关于sklearn中的PolynomialFeatures"><span class="nav-number">1.2.</span> <span class="nav-text">二，关于sklearn中的PolynomialFeatures</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#设degree-i，则它会生成所有小于等于i的多项式特征，构成新的特征集"><span class="nav-number">1.2.1.</span> <span class="nav-text">设degree&#x3D;i，则它会生成所有小于等于i的多项式特征，构成新的特征集</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三、Pipeline"><span class="nav-number">1.3.</span> <span class="nav-text">三、Pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步"><span class="nav-number">1.3.1.</span> <span class="nav-text">拿多项式回归来说，共有3步：1、生成多项式特征；2、数据归一化；3、调用线性回归。而Pipeline就可以帮助我们将这3步合在一起、使得我们每次调用时不需要重复这3步</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#具体看jupyter中的代码"><span class="nav-number">1.3.2.</span> <span class="nav-text">具体看jupyter中的代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#四、过拟合和欠拟合，以及分离数据集"><span class="nav-number">1.4.</span> <span class="nav-text">四、过拟合和欠拟合，以及分离数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-泛化能力"><span class="nav-number">1.5.</span> <span class="nav-text">1.泛化能力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱"><span class="nav-number">1.5.1.</span> <span class="nav-text">泛化能力是指我们由训练数据得出了一条拟合曲线，而这个曲线在面对测试数据时它的表现好坏就是泛化能力的强弱</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-过拟合"><span class="nav-number">1.6.</span> <span class="nav-text">2.过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测-区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。"><span class="nav-number">1.6.1.</span> <span class="nav-text">就是太过贴近于训练数据的特征了，在训练集上表现非常优秀，近乎完美的预测&#x2F;区分了所有的数据，但是在新的测试集上却表现平平，泛化能力差，拿到新样本后没有办法去准确的判断。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-欠拟合"><span class="nav-number">1.7.</span> <span class="nav-text">3.欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#测试样本的特性没有学到-模型不能完整的表述数据关系-，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差"><span class="nav-number">1.7.1.</span> <span class="nav-text">测试样本的特性没有学到(模型不能完整的表述数据关系)，或者是模型过于简单无法拟合或区分样本。就是在测试数据上都表现的很差</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-总结"><span class="nav-number">1.8.</span> <span class="nav-text">4.总结</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行"><span class="nav-number">1.8.1.</span> <span class="nav-text">总之，我们训练出来的模型是要进行预测的，所以我们并不是衡量它在训练数据上表现有多好，而是在测试数据上要很好才行</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#五、学习曲线（可视化查看模型的欠拟合、过拟合）"><span class="nav-number">1.9.</span> <span class="nav-text">五、学习曲线（可视化查看模型的欠拟合、过拟合）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力"><span class="nav-number">1.9.1.</span> <span class="nav-text">定义：随着训练样本的逐渐增多，算法训练出的模型的表现能力</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#学习曲线的解释："><span class="nav-number">1.9.2.</span> <span class="nav-text">学习曲线的解释：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；"><span class="nav-number">1.9.3.</span> <span class="nav-text">随着样本数的增加，训练模型的误差越来越大，然后趋于稳定，这是因为样本数越多，模型越难拟合；</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失"><span class="nav-number">1.9.4.</span> <span class="nav-text">随着样本数的增加，测试模型的误差越来越小，然后趋于稳定，同时之所以测试模型的误差比训练模型高，是因为将训练模型应用到测试模型上，泛化能力会有一定的损失</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#欠拟合，最佳拟合，过拟合的对比"><span class="nav-number">1.9.5.</span> <span class="nav-text">欠拟合，最佳拟合，过拟合的对比</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#六、验证数据集与交叉验证"><span class="nav-number">1.10.</span> <span class="nav-text">六、验证数据集与交叉验证</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1、验证数据集："><span class="nav-number">1.10.1.</span> <span class="nav-text">1、验证数据集：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#由于train-test-split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集"><span class="nav-number">1.10.2.</span> <span class="nav-text">由于train_test_split可能导致对特定的测试数据集过拟合，所以我们为了解决这一问题在中间又加入了验证数据集</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集"><span class="nav-number">1.10.3.</span> <span class="nav-text">由训练数据训练模型，使用验证数据验证模型的好坏，测试数据作为衡量模型最终性能的数据集</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。"><span class="nav-number">1.10.4.</span> <span class="nav-text">但是这样又出现了一个问题：由于验证数据集只有一份且是随机选取的数据构成的，那么有可能选取到某些极端数据样本进而影响到模型整体的性能评价。所以出现了交叉验证。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2、交叉验证："><span class="nav-number">1.10.5.</span> <span class="nav-text">2、交叉验证：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。"><span class="nav-number">1.10.6.</span> <span class="nav-text">把训练数据分成k份（上图为3份），以3份为例，当BC作为训练数据训练模型时，则A作为验证数据。依次类推，这样将生成3个模型，最终将这3个模型的性能指标的均值作为结果。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#使用交叉验证进行调参优化得到的score可能会比train-test-split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据"><span class="nav-number">1.10.7.</span> <span class="nav-text">使用交叉验证进行调参优化得到的score可能会比train_test_split低一些，这是因为交叉验证进行了均值处理，所以不会偏袒（过拟合）其中任意一份数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#交叉验证总结："><span class="nav-number">1.10.8.</span> <span class="nav-text">交叉验证总结：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#但是这样训练出的模型是可信赖的"><span class="nav-number">1.10.9.</span> <span class="nav-text">但是这样训练出的模型是可信赖的</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据"><span class="nav-number">1.10.10.</span> <span class="nav-text">假设训练数据集共有m个样本：每次将m-1份数据作为训练数据，将1份数据作为验证数据</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#七、偏方差权衡（Bias-Variance-Trade-off）"><span class="nav-number">1.11.</span> <span class="nav-text">七、偏方差权衡（Bias Variance Trade off）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1、偏差与方差："><span class="nav-number">1.11.1.</span> <span class="nav-text">1、偏差与方差：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#可以参考："><span class="nav-number">1.11.2.</span> <span class="nav-text">可以参考：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#偏差："><span class="nav-number">1.11.3.</span> <span class="nav-text">偏差：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。"><span class="nav-number">1.11.4.</span> <span class="nav-text">描述的是预测值与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#方差："><span class="nav-number">1.11.5.</span> <span class="nav-text">方差：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。"><span class="nav-number">1.11.6.</span> <span class="nav-text">描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2、模型误差："><span class="nav-number">1.11.7.</span> <span class="nav-text">2、模型误差：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#模型误差-偏差（Bias）-方差（Variance）-不可避免的误差（如噪声）"><span class="nav-number">1.11.8.</span> <span class="nav-text">模型误差 &#x3D; 偏差（Bias）+ 方差（Variance）+ 不可避免的误差（如噪声）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3、产生偏差和方差的主要原因"><span class="nav-number">1.11.9.</span> <span class="nav-text">3、产生偏差和方差的主要原因</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4、模型与偏差和方差："><span class="nav-number">1.11.10.</span> <span class="nav-text">4、模型与偏差和方差：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5、偏差与方差的关系："><span class="nav-number">1.11.11.</span> <span class="nav-text">5、偏差与方差的关系：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6、机器学习-算法层面-的主要挑战"><span class="nav-number">1.11.12.</span> <span class="nav-text">6、机器学习(算法层面)的主要挑战</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#八、模型正则化（Regularization）"><span class="nav-number">1.12.</span> <span class="nav-text">八、模型正则化（Regularization）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。"><span class="nav-number">1.12.1.</span> <span class="nav-text">模型正则化是解决过拟合的一种很好的手段，通常过拟合的模型中的多项式系数都非常大，导致模型非常复杂，正则化的目的就是要限制系数的大小，使模型泛化能力变强。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#说明："><span class="nav-number">1.12.2.</span> <span class="nav-text">说明：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#要想使损失函数尽可能小，就必须让-theta-尽可能小"><span class="nav-number">1.12.3.</span> <span class="nav-text">要想使损失函数尽可能小，就必须让$\theta$尽可能小</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#损失函数后面的正则化项求和是从1到n的，这意味着不需要将-theta-0-加入，因为-theta-0-是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）"><span class="nav-number">1.12.4.</span> <span class="nav-text">损失函数后面的正则化项求和是从1到n的，这意味着不需要将$\theta_0$加入，因为$\theta_0$是截距，它只决定模型拟合曲线的高低，不决定曲线每一部分的陡峭程度（导数，斜率）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#第二项的1-2是为了求导好约分，加不加都行"><span class="nav-number">1.12.5.</span> <span class="nav-text">第二项的1&#x2F;2是为了求导好约分，加不加都行</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#alpha-是一个超参数，它指的是-theta-的优化程度占整个损失函数的多少，很显然，-alpha-越大越好，但是在实际中我们要找一个能平衡经验误差项-MSE-和正则化项的-alpha-。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。"><span class="nav-number">1.12.6.</span> <span class="nav-text">$\alpha$是一个超参数，它指的是$\theta$的优化程度占整个损失函数的多少，很显然，$\alpha$越大越好，但是在实际中我们要找一个能平衡经验误差项(MSE)和正则化项的$\alpha$。深入探讨可以看微信公众号《机器学习实验室》的机器学习专栏中的Lasso回归部分。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#九、岭回归（Ridge-Regression）"><span class="nav-number">1.13.</span> <span class="nav-text">九、岭回归（Ridge Regression）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#损失函数："><span class="nav-number">1.13.1.</span> <span class="nav-text">损失函数：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#十、LASSO回归（LASSO-Regression）"><span class="nav-number">1.14.</span> <span class="nav-text">十、LASSO回归（LASSO Regression）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#比较Ridge和LASSO：为什么随着-alpha-的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线"><span class="nav-number">1.14.1.</span> <span class="nav-text">比较Ridge和LASSO：为什么随着$\alpha$的增大，Ridge几乎还是曲线，但是LASSO就几乎变成了直线</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#下面我们从梯度下降的角度来解释："><span class="nav-number">1.14.2.</span> <span class="nav-text">下面我们从梯度下降的角度来解释：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些-theta-为零。这也说明了为什么叫岭回归，因为它就像下山一样。"><span class="nav-number">1.14.3.</span> <span class="nav-text">因为LASS回归的正则化项为绝对值，不能求导，所以我们采用分段函数来求梯度，这样就不能向Ridge那样曲线梯度下降，而只能像图上那样到达零点，从图中还可以看出LASSO回归中的一些$\theta$为零。这也说明了为什么叫岭回归，因为它就像下山一样。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#LASSO回归总结"><span class="nav-number">1.14.4.</span> <span class="nav-text">LASSO回归总结</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。"><span class="nav-number">1.14.5.</span> <span class="nav-text">但是LASSO作为特征选择效果未必好，有可能会丢失一些重要的特征。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#十一、L1，L2正则"><span class="nav-number">1.15.</span> <span class="nav-text">十一、L1，L2正则</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。"><span class="nav-number">1.15.1.</span> <span class="nav-text">其实在机器学习领域对于不同的应用可能会发明不同的名词来衡量不同的标准，如上图：岭回归和LASSO回归是衡量正则化程度的；MSE和MAE是衡量回归算法好坏的；欧拉和曼哈顿距离是衡量两点直接距离的。但是它们背后的数学原理却是非常相似的，只不过是它们被应用在不同领域，产生了不同的效果，进而生成了不同的新名词。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#基于上面的思想，我们把明可夫斯基距离进行一下如下变换："><span class="nav-number">1.15.2.</span> <span class="nav-text">基于上面的思想，我们把明可夫斯基距离进行一下如下变换：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#关于图右边开不开根号都不会影响到结果"><span class="nav-number">1.15.3.</span> <span class="nav-text">关于图右边开不开根号都不会影响到结果</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#L0正则项的原理是使-theta-的非零项的数量尽量少，但是必须使用穷举法来得出-theta-为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。"><span class="nav-number">1.15.4.</span> <span class="nav-text">L0正则项的原理是使$\theta$的非零项的数量尽量少，但是必须使用穷举法来得出$\theta$为0和不为0的数量，这在现实中是很困难的。所以通常用L1来取代。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#十二、弹性网"><span class="nav-number">1.16.</span> <span class="nav-text">十二、弹性网</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。"><span class="nav-number">1.16.1.</span> <span class="nav-text">机器学习的算法中有一种组合思想，意思是把不同算法的优点结合在一起生成一个新模型。例子有小批量梯度下降法和这个弹性网。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。"><span class="nav-number">1.16.2.</span> <span class="nav-text">弹性网结合了L1正则和L2正则的优点，其中r是一个超参数，表示两种正则项的比例。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。"><span class="nav-number">1.16.3.</span> <span class="nav-text">通常优先采用岭回归，因为准确。但是它的计算量很大，特别是特征非常多的情况下。如果算力不够，我们要优先选择弹性网。它结合了岭回归的准确也结合了LASSO的特征选择的特性。</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="macchen"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">macchen</p>
  <div class="site-description" itemprop="description">学习强国</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">macchen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">12k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">11 分钟</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>










      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  <!-- 引入custom，就这一句，控制烟花特效 -->
  
  
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
    <script src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
    <script src="/js/cursor/explosion.min.js"></script>
  


<!-- 樱花特效 -->
  
      <script async src="/js/cursor/cherry.js"></script>
  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":250,"height":500},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body>
</html>
